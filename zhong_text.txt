DECISION MAKING FOR DISEASE TREATMENT: OPERATIONS
RESEARCH AND DATA ANALYTIC MODELING

A DISSERTATION
SUBMITTED TO THE DEPARTMENT OF MANAGEMENT SCIENCE AND
ENGINEERING
AND THE COMMITTEE ON GRADUATE STUDIES
OF STANFORD UNIVERSITY
IN PARTIAL FULFILLMENT OF THE REQUIREMENTS
FOR THE DEGREE OF
DOCTOR OF PHILOSOPHY

Huaiyang Zhong
August 2020

© 2020 by Huaiyang Zhong. All Rights Reserved.
Re-distributed by Stanford University under license with the author.

This dissertation is online at: http://purl.stanford.edu/nf470sd9819

ii

I certify that I have read this dissertation and that, in my opinion, it is fully adequate
in scope and quality as a dissertation for the degree of Doctor of Philosophy.
Eran Bendavid

I certify that I have read this dissertation and that, in my opinion, it is fully adequate
in scope and quality as a dissertation for the degree of Doctor of Philosophy.
Margaret Brandeau, Primary Adviser

I certify that I have read this dissertation and that, in my opinion, it is fully adequate
in scope and quality as a dissertation for the degree of Doctor of Philosophy.
Joshua Salomon

Approved for the Stanford University Committee on Graduate Studies.
Stacey F. Bent, Vice Provost for Graduate Education

This signature page was generated electronically upon submission of this dissertation in
electronic format. An original signed hard copy of the signature page is on file in
University Archives.

iii

iv

Acknowledgments
I would like to thank my advisor, Prof. Margaret Brandeau, for continued support and guidance
throughout my PhD study. More than an academic advisor, she also o↵ered much life advice. Her
attitude towards research and life will have lifelong impacts on me and I am grateful for it.
Besides my advisor, I would like to thank my thesis reading committee members, Prof. Eran
Bendavid and Prof. Joshua Salomon, for working with me on di↵erent interesting projects as well
as for their insightful comments and encouragement throughout my time at Stanford.
Moreover, I would like to thank Prof. Nick Bambos and Prof. Doug Owens for serving on my
PhD oral exam committee.
I want to thank the students in our research group for stimulating discussions and insightful
suggestions during our weekly research meetings.
Furthermore, I would like to thank two friends, Xiaocheng Li and Zhuozhi Chen. With Xiaocheng, I shared many sleepless nights discussing projects and writing papers. More importantly,
we continued to support and motivate each other through good and bad times. In addition, I
spent time with Zhuozhi having meals, hiking during the weekends, and engaging in many other
interesting activities. He was always willing to listen and o↵ered much advice as an outsider.
Last but not least, I would like to thank my parents, Weichun Zhong and Xiangmei Rui, for
their unconditional love and support.

v

Contents
iv
Acknowledgments

v

1 Introduction

1

2 Quantile Markov Decision Processes

3

2.1

2.2
2.3

2.4

2.5

2.6

Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

3

2.1.1

Main Contributions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

4

2.1.2

Other Related Literature . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

6

2.1.3

Illustration of the QMDP Output . . . . . . . . . . . . . . . . . . . . . . . . .

8

Markov Decision Process . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

9

2.2.1

Assumptions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

10

Quantile Markov Decision Process . . . . . . . . . . . . . . . . . . . . . . . . . . . .

11

2.3.1

Quantile Objective and Assumptions . . . . . . . . . . . . . . . . . . . . . . .

11

2.3.2

Value Function and Dynamic Programming . . . . . . . . . . . . . . . . . . .

12

2.3.3

Optimal Value and Optimal Policy . . . . . . . . . . . . . . . . . . . . . . . .

14

2.3.4

QMDP and Other Risk Measures . . . . . . . . . . . . . . . . . . . . . . . . .

16

Algorithms and Computational Aspects . . . . . . . . . . . . . . . . . . . . . . . . .

18

2.4.1

Algorithm for Solving the Optimization Problem OPT . . . . . . . . . . . . .

18

2.4.2

Algorithm for Solving QMDP . . . . . . . . . . . . . . . . . . . . . . . . . . .

20

2.4.3

Complexity Analysis and Approximation

. . . . . . . . . . . . . . . . . . . .

20

Extensions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

22

2.5.1

Conditional Value at Risk . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

22

2.5.2

Dynamic Risk Measures . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

24

2.5.3

Infinite-Horizon QMDP . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

25

Empirical Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

27

vi

2.7

2.6.1

Synthetic Experiment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

27

2.6.2

Case Study: HIV Treatment Initiation

. . . . . . . . . . . . . . . . . . . . .

32

Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

34

3 Health Outcomes and Cost-E↵ectiveness of Treating Depression in People With
HIV in Sub-Saharan Africa

37

3.1

Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

37

3.2

Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

38

3.3

Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

46

3.3.1

Base Case Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

46

3.3.2

Sensitivity Analyses . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

49

3.3.3

Supplemental Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

49

Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

52

3.4

4 Metamodeling for Policy Simulations with Multivariate Outcomes

55

4.1

Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

55

4.2

Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

56

4.2.1

Metamodeling Framework . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

56

4.2.2

Base Learners . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

58

4.2.3

Performance Improvement Scheme . . . . . . . . . . . . . . . . . . . . . . . .

60

4.2.4

Evaluation Metrics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

61

4.2.5
4.3

4.4

Case Study: HCV Screening and Treatment . . . . . . . . . . . . . . . . . . .

62

Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

63

4.3.1

Model Accuracy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

63

4.3.2

Computational Efficiency . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

65

4.3.3

Model Interpretability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

66

Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

70

5 Conclusions

72

A Supplement for Chapter 2

74

A.1 Relaxation of Assumption 1 (c) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

74

A.2 Proof of Lemmas and Theorems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

75

A.2.1 Proof of Lemma 2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

75

A.2.2 Proof of Theorems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

79

A.3 Synthetic Example: Computation Time for QMDP vs. QBDP . . . . . . . . . . . . .

84

A.4 HIV Treatment Example: Model Parameters . . . . . . . . . . . . . . . . . . . . . .

85

vii

A.5 HIV Treatment Example: Solution Using QBDP . . . . . . . . . . . . . . . . . . . .
Bibliography

87
88

viii

List of Tables
3.1

Data and Sources: Depression . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

39

3.2

Data and Sources: Utilities and Costs . . . . . . . . . . . . . . . . . . . . . . . . . .

40

3.3

Data and Sources for HIV Dynamics and Natural History: Demographics . . . . . .

41

3.4

Data and Sources for HIV Dynamics and Natural History: HIV Transmission Parameters . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

42

3.5

Data and Sources for HIV Dynamics and Natural History: HIV Disease Parameters

43

3.6

Data and Sources for HIV Dynamics and Natural History: HIV Retention Parameters 44

3.7

Base Case Results: Costs, Life Years, Quality-Adjusted Life Years, and Incremental
Cost-E↵ectiveness Ratio . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

3.8

54

Base Case Results: Relative Percentage Changes (vs. Status Quo) in Care Cascade
Parameters and 95% Uncertainty Bounds . . . . . . . . . . . . . . . . . . . . . . . .

54

4.1

Summary of explored hyperparameters (default parameters are bolded) . . . . . . .

61

4.2

List of Input Variables for the Metamodels . . . . . . . . . . . . . . . . . . . . . . .

64

4.3

2

Summary of model performance (averageaR ) on test data set for hepatitis C virus
model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

65

4.4

Variable importance for RF . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

67

4.5

Variable importance for GPR . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

67

A.1 Model parameters for HIV treatment example . . . . . . . . . . . . . . . . . . . . . .

86

ix

List of Figures
2.1

Comparison of MDP and QMDP value functions. Each plot is obtained from a di↵erent
initialization of the model parameters. The red lines are the optimal quantile rewards computed via QMDP. The gray dashed lines are the cumulative density function for simulations
with the execution of the optimal MDP policy. . . . . . . . . . . . . . . . . . . . . . . .

2.2

Illustration of backward dynamic program for computing vt from vt+1 .

8

Here pki =

P (St+1 = sk |St = s, a = ak ). . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

2.3

QMDP optimal value function and optimal policy for two-period gambling game. . .

2.4

Step-by-step execution of Algorithm 1 with n = 3 sample g(·, ·) functions. Numbers inside

and along the blocks represent the values and breakpoints of the input functions g(i, ·). The

15
17

shaded regions reflect the progress of the algorithm. In the end, the output is f. . . . . . .

20

2.5

Illustration of the simple QMDP model. . . . . . . . . . . . . . . . . . . . . . . . . .

27

2.6

Synthetic example: CPU time of QMDP and MDP. Base model parameters: time horizon
T = 10, state space size n = 20, max reward Rmax = 10. For each experiment, we changed
a single parameter and monitored the running time. The dark red solid lines indicate the
CPU time for execution of the QMDP algorithm and dashed gray lines indicate the CPU
time for execution of the MDP algorithm. . . . . . . . . . . . . . . . . . . . . . . . . . .

2.7

28

Synthetic example: QMDP value function comparison with MDP and QBDP. Each plot
is obtained from a di↵erent initialization of model parameters. The gray dashed lines are
the cumulative density function for simulations with the execution of the optimal MDP
policy. The red lines are the optimal quantile rewards computed via QMDP. The remaining
lines are the cumulative density functions obtained by simulating the optimal policies from
QBDP with di↵erent preset values of ⌧ . . . . . . . . . . . . . . . . . . . . . . . . . . . .

x

29

2.8

Synthetic example: QMDP value function comparison with MDP and utility function-based
MDP. Each plot is obtained from a di↵erent initialization of the model parameters. The
gray dashed lines are the cumulative density function for simulations with the execution
of the optimal MDP policy. The red lines are the optimal quantile rewards computed via
QMDP. The remaining lines are the cumulative density functions obtained by simulating
the optimal policies from the utility function-based approach with di↵erent preset values of . 29

2.9

Synthetic example: Optimal QMDP actions at di↵erent states (s) and di↵erent time
periods (t) with di↵erent ⌧ values. . . . . . . . . . . . . . . . . . . . . . . . . . . . .

31

2.10 HIV treatment example: Optimal actions for QMDP with ⌧ = 0.20, 0.50, and 0.80. .

35

2.11 HIV treatment example: Optimal QMDP reward and cumulative density function of
MDP reward. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3.1

Schematic representation of HIV natural history model and antidepressant treatment
scheme . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

3.2

47

Calibration result: model-projected ART coverage among HIV-infected individuals
compared with UNAIDS estimates from 2005 to 2015 . . . . . . . . . . . . . . . . . .

3.5

47

Calibration result: model-projected 2011 HIV prevalence in each age group compared
with Demographic Health Survey (DHS) data from 2011 . . . . . . . . . . . . . . . .

3.4

45

Calibration result: model-projected HIV prevalence compared with UNAIDS estimates from 2005 to 2015 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

3.3

35

48

Calibration result: model-projected population compared with World Bank estimated population for 2005 to 2015 . . . . . . . . . . . . . . . . . . . . . . . . . . . .

48

3.6

Cost-e↵ectiveness acceptability curve . . . . . . . . . . . . . . . . . . . . . . . . . . .

50

3.7

Estimates of the incremental cost ($ millions) of the ADT Provision strategy compared to the status quo for the 10-year modeled time horizon for the entire Uganda
population . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

3.8

51

Estimates of the future incremental cost ($ millions) of the ADT Provision strategy
compared to the status quo at the end of the 10-year modeled time horizon for the
entire Uganda population . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

51

4.1

Average training and prediction times for the five base models

66

4.2

Partial dependence plots from RF (random forest) and GPR (Gaussian process re-

. . . . . . . . . . . .

gression) for predicting the number of hepatitis C virus (HCV) cases identified in
one year by risk-based testing.

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

xi

68

4.3

Prediction of the number of hepatitis c virus (HCV) cases identified in one year
by risk-based testing. LIME (local interpretable model agnostic) models from RF
(random forest) and GPR (Gaussian process regression) for one test data point when
limiting the local linear regression variables to variables found by variable selection.

69

A.1 Synthetic example: CPU time of QMDP and QBDP (one run with ⌧ = 0.6). . . . . .

85

A.2 Synthetic example: CPU time of QMDP and QBDP (100 runs with di↵erent ⌧ values
and 10, 000 simulations of each policy). . . . . . . . . . . . . . . . . . . . . . . . . . .

85

A.3 HIV treatment example: Optimal actions for QBDP (for ⌧ = 0.20, 0.35, and 0.50)
and MDP. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

xii

87

Chapter 1

Introduction
This dissertation focuses on developing and applying various methodologies including operations
research and data analytics to solve important problems in healthcare decision making. Healthcare
decision making is interesting and challenging because of the probabilistic nature of many healthcare
decision problems and because of the range of decision makers involved (e.g., individual patients,
clinicians, and policy makers). Healthcare decision making occurs at two levels: clinical decision
making at the individual level and policy decision making at the population level. In clinical decision
making, practitioners aim to determine which patient needs what and when. The decision can be
a simple diagnosis such as predicting arthritis from MRI images or a series of decisions throughout
the treatment duration such as HIV treatment management. In policy decision making, policy
makers aim to assess decisions undertaken to achieve specific population-level healthcare goals
[109]. Problems in this field range from disease modeling to policy implementation.
In Chapter 2, to extend the boundary of current methodologies in clinical decision making, I
develop a theoretical sequential decision making framework, a quantile Markov decision process
(QMDP), based on the traditional Markov decision process (MDP). The QMDP model optimizes
a specific quantile of the cumulative reward instead of its expectation. I provide analytical results
characterizing the optimal QMDP value function and present a dynamic programming-based algorithm to solve for the optimal policy. The algorithm also extends to the MDP problem with a
conditional value-at-risk (CVaR) objective. Using the QMDP framework, patients’ risk attitudes
can be incorporated into the decision making process, thereby enabling patient-centered care. I apply the QMDP framework to an HIV treatment initiation problem, where patients aim to balance
the potential benefits and risks of the treatment.
In Chapter 3, to inform public health policy regarding treatment for HIV-infected individuals
with clinical depression, I develop a microsimulation model of HIV disease and care in Uganda which

1

CHAPTER 1. INTRODUCTION

2

captures individuals’ depression status and the relationship between depression and HIV behaviors.
I consider a strategy of screening for depression and providing antidepressant therapy with fluoxetine
at initiation of antiretroviral therapy or re-initiation (if a patient has dropped out). I use the model
to estimate the e↵ectiveness and cost-e↵ectiveness of such strategies. I show that screening for and
treating depression among people living with HIV in sub-Saharan Africa with fluoxetine would be
e↵ective in improving HIV treatment outcomes and would be highly cost-e↵ective.
In Chapter 4, with the aim of improving policy implementation, I examine the problem of
simplifying complex healthcare decision models using metamodeling. Many healthcare decision
models, particularly those involving simulation of patient outcomes, are highly complex and may
be difficult to use for practical decision making. A metamodel is a simplified version of a more
complex model which approximates the relationships between model inputs and outputs, and thus
can be used as a surrogate for the more complex model. I develop a framework for metamodeling
of simulation models with multivariate outcomes. I apply the methodology to simplify a complex
simulation model that evaluates strategies for hepatitis C virus (HCV) screening and treatment in
correctional settings.
Chapter 5 concludes with discussion of promising areas for further research.

Chapter 2

Quantile Markov Decision
Processes
2.1

Introduction

The problem of sequential decision making has been widely studied in the fields of operations
research, management science, artificial intelligence, and stochastic control. Markov decision processes (MDPs) are one important framework for addressing such problems. In the traditional MDP
setting, an agent sequentially performs actions based on information about the current state and
then obtains rewards based on the action and state. The goal of an MDP is to maximize the
expected cumulative reward over a defined horizon which may be finite or infinite.
In many applications, however, a decision maker may be interested in optimizing a specific
quantile of the cumulative reward instead of its expectation. For example, a physician may want
to determine the optimal drug regime for a risk-averse patient with the objective of maximizing
the 0.10 quantile of the cumulative reward; this is the cumulative improvement in health that is
expected to occur with at least 90% probability for the patient [22]. A company such as Amazon
that provides cloud computing services might want their cloud service to be optimized at the 0.01
quantile [41], meaning that the company strives to provide service that satisfies 99% of its customers.
In the finance industry, the quantile measure, sometimes referred as value at risk (VaR), has been
used as a measure of capital adequacy [47]. For example, the 1996 Market Risk Amendment to
the Basel Accord officially used VaR for determining the market risk capital requirement [19]. The
advantage of a quantile objective lies in its focus on the distribution of rewards. The cumulative
reward usually cannot be characterized by the expectation alone; distributions of cumulative reward

3

CHAPTER 2. QUANTILE MARKOV DECISION PROCESSES

4

with the same expectation may di↵er greatly in their lower or upper quantiles, especially when they
are skewed.
In this chapter, we study the problem of optimizing quantiles of cumulative rewards of a Markov
decision process, which we refer to as a quantile Markov decision process (QMDP). Our QMDP
formulation considers a quantile objective for an underlying MDP with finite states and actions,
and with either a finite or infinite horizon. We show that the key to solving the optimal policy
for a QMDP problem is proper augmentation of the state space. The augmented state acts as a
performance measure of the past cumulative reward and thus “Markovizes” the optimal decisions.
This enables us to develop an efficient dynamic programming procedure that finds the optimal
QMDP value function for all states and quantiles in one pass. In the execution of the optimal
policy, the augmented state guides the strategy in subsequent periods to be aggressive, neutral,
or conservative. We also demonstrate how the same idea extends to the conditional value at risk
(CVaR) objective.

2.1.1

Main Contributions

In this section we describe the contribution of our work in three areas: model formulation (as a
risk-sensitive MDP), solution methodology (the design of a dynamic programming algorithm to
handle a non-Markovian objective), and practical application.
Risk-sensitive MDP. There are two types of uncertainty associated with an MDP: inherent
uncertainty, which is the variability of cumulative cost/reward caused by the stochasticity of the
MDP itself, and model uncertainty, which is the uncertainty caused by unavoidable model ambiguity
or parameter estimation errors [42, 138]. The QMDP model aims to explicitly characterize the
inherent uncertainty of an MDP by looking at the quantiles and the distribution of the cumulative
reward.
The study of risk-sensitive MDPs dates back to Howard and Matheson [69] who proposed the
use of an exponential utility function to capture risk attitude. The authors developed a policy
iteration algorithm that relies on the structure of the exponential function to solve for the optimal
policy. Subsequently, Piunovskiy [114], Tamar et al. [127], and Mannor and Tsitsiklis [86] imposed
a variance constraint, and Altman [7] and Ermon et al. [49] added a probabilistic constraint on
the MDP cumulative reward. The variance or probability constraint characterizes the variation of
the cumulative reward and is introduced to control the internal risk of MDP. Di Castro et al. [45]
derived policy gradient algorithms for variance-related risk criteria and Arlotto et al. [8] identified
types of MDP problems where the mean of the cumulative reward dominates its variance. Chow
[34] noted that the optimal policy for such models is usually very sensitive to the choice of the risk
parameter value and to misspecification of the underlying probability distribution.

CHAPTER 2. QUANTILE MARKOV DECISION PROCESSES

5

Another stream of research on risk-sensitive MDPs employs risk measures that account for the
variation of the cumulative reward. Ruszczynski [118] and Jiang and Powell [74] proposed nested
risk measures for a risk-averse MDP problem. The nested objective inductively summarizes the
cost-to-go reward at each time step into a deterministic value; thus the problem can be solved
by a dynamic programming procedure similar to that for a traditional MDP. One shortcoming of
the nested risk measure is that there is no clear relation between the cumulative reward and the
optimal nested objective function value. Moreover, the nested risk measure involves a user-specified
parameter. We will further elaborate on the di↵erence between QMDP and nested risk measure
models through an example in Section 2.3.4.
The risk-sensitive MDP models described in this section solve the optimal policy for only one
risk parameter at a time. Consequently, they require prior knowledge to specify the risk parameter;
if, after obtaining the optimal policy for a given parameter value, the reward is not satisfactory,
one has to solve the model again with another parameter value, essentially using a trial-and-error
procedure. In contrast, QMDP outputs the optimally achievable quantile of the cumulative reward
for all quantiles (the risk parameter in the QMDP model) in a single run of dynamic programming.
Dynamic programming for a non-Markovian objective. The main difficulty in solving risksensitive MDP models is the design of an efficient dynamic programming algorithm. Nested risk
measure models [118, 74] compose a sequence of one-step risk measures. Since the optimal action
in each period depends only on the current state, these models avoid the inconvenience of dealing
with non-Markovian structures. Certain model structures can be utilized for algorithm design,
such as the dual-based dynamic programming approach for the multi-stage stochastic programming
problem [121]. For MDP with the CVaR objective, a number of studies [15, 142, 35, 36] have each
solved the problem under slightly di↵erent settings. A common technique employed in these papers
is augmentation of the state space and execution of a dynamic programming algorithm in the
augmented state space; however, the augmentation methods are restricted to the CVaR objective
and cannot be generalized to handle the quantile objective.
QMDP deals with a non-Markovian objective where the optimal policy may depend on the
entire past history. Our solution algorithm provides a state-augmentation method to handle the
non-Markovian objective and complements the literature on dynamic programming algorithms. The
augmented state for the quantile objective acts like a “sufficient statistic” for the past history. The
dynamic programming algorithm is executed over the augmented state space with an optimization
subroutine. Compared to the augmented state in the CVaR MDP, our augmented variable conveys
a tangible meaning – quantile – whereas the augmented state in Bauerle and Ott [15] and Yu et al.
[142] is only a nominal variable to facilitate the solving of the optimization problem. The QMDP
cost-to-go function at each time period is a function of the current state and the augmented state

CHAPTER 2. QUANTILE MARKOV DECISION PROCESSES

6

(quantile), and represents the optimal value function of a QMDP subproblem for the remaining
periods (given the current state and for all quantiles), in contrast to the nested risk measure formulation [118, 74] where the cost-to-go function is simply a deterministic value. This special property
of the augmented state enables us to solve QMDP for the optimal value function and the optimal
policy for all quantiles in one pass of dynamic programming. The other formulations can only solve
one risk parameter at a time, and the CVaR MDP algorithms proposed by Bauerle and Ott [15]
and Yu et al. [142] could possibly require solving dynamic programming procedures infinitely many
times to obtain the optimal value function and policy for a single percentile parameter.
The dynamic programming results from QMDP also provide insights for understanding a dynamic quantile risk measure and give a non-constructive explanation for the time-inconsistency [30]
of the quantile risk measure. A quantile objective specifies a family of risk measures. The execution
of the QMDP solution procedure entails a dynamic change of the risk measure within the family.
This makes the conventional definition of dynamic risk measure unsuitable for the quantile objective. In Section 2.5.2, we discuss this issue in detail and develop a new notion of time-consistent
risk measure.
Practical Relevance of QMDP. MDP models have been widely applied to many real-world
problems including, for example, financial derivative pricing [130], service system planning [120],
and chronic disease treatment [122, 87]. However, these applications do not consider the fact
that many decisions are inherently risk-sensitive. For instance, both physicians and patients are
concerned about the risk associated with di↵erent medical treatment decisions. Practitioners have
applied the quantile objective in a variety of applications, but in a descriptive manner [19, 10, 22].
Our work contributes to the adoption of quantile criteria in sequential decision making. In prior
work there has been no clear solution to decode the full distribution of cumulative reward. With a
single pass of the QMDP solution algorithm, we can obtain the optimal rewards for all quantiles.
Comparing these rewards to the quantiles of the cumulative reward under the traditional optimal
MDP policy can help assess the need for adoption of a risk-sensitive framework. In this sense,
QMDP is not a substitute for but a complement to MDP models in real-world applications.

2.1.2

Other Related Literature

To the best of our knowledge, this chapter is the first to address the MDP problem with a quantile
objective in a generic setting. Several studies have examined restricted versions of the problem. Filar
et al. [53] studied the quantile objective for the limiting average reward of an infinite-horizon MDP,
determining whether there exists a policy that achieves a specified value of the long-run limiting
average reward at a specified probability level. Ummels and Baier [131] developed an algorithm to
compute the quantile cumulative rewards for a given policy in polynomial time. The algorithm is

CHAPTER 2. QUANTILE MARKOV DECISION PROCESSES

7

descriptive rather than prescriptive in terms of understanding the uncertainty associated with the
Markov chain (MDP with a fixed policy). Gilbert et al. [59] addressed the quantile MDP problem
for the special case of deterministic rewards and preference-based MDP.
CVaR, also known as average value at risk (AVaR) or expected shortfall, has been explored in
the context of risk-sensitive MDPs. CVaR is defined as the expectation of the cost/reward in the
worst q% of cases. From the perspective of chance constrained optimization, the CVaR criterion
can be viewed as a convex relaxation of the quantile criterion and thus can be optimized more
conveniently [97]. Bauerle and Ott [15] utilized a variational representation of the CVaR criteria
and derived an analytical framework for solving MDP with a CVaR objective. The variational
form expresses the optimal value of CVaR MDP as an optimization of a univariate function on
the real line. The function value at each real number must be computed by executing a dynamic
programming procedure in the same way as for a traditional MDP. Yu et al. [142] studied MDP
under a more general class of risk measures that have similar variational form. The algorithms in
Bauerle and Ott [15] and Yu et al. [142] optimize a function via grid search and employ a dynamic
programming subroutine for the underlying MDP and thus o↵er no complexity guarantee and only
solve for a single percentile each time. In contrast, QMDP solves for all the quantiles in a single
pass of dynamic programming.
Recent work has explored the interaction of the CVaR objective with MDP. Carpin et al. [28]
studied the CVaR objective for the total cost/reward of transient MDPs. Chow and Ghavamzadeh
[35] considered MDP with an expectation objective but a CVaR constraint. Chow et al. [36]
considered the CVaR objective for the cumulative reward, which is close to the quantile objective
in this chapter, but only considered infinite-horizon discounted MDPs. In this chapter, we show
that the derivation of our QMDP model naturally extends to CVaR MDP, and we provide an exact
algorithm for solving the CVaR MDP problem (including for the case of a finite horizon and an
undiscounted setting not considered by Chow et al. [36]).
Finally, in the area of reinforcement learning, Bellemare et al. [16] proposed this distributional
perspective for RL and derived a method that outputs the distribution, rather than just the expectation, of the optimal cumulative reward. The optimal policy was defined as maximizing the
expectation of the cumulative reward. Their method provides additional insights regarding the
distribution of the optimal reward. Subsequent studies [40, 141] examined di↵erent ways to parameterize and learn the distributional value function. These studies, though still adopting expectation
as the optimality criterion, shed light on the importance of the distributional information in a
sequential decision making context.

CHAPTER 2. QUANTILE MARKOV DECISION PROCESSES

8

Figure 2.1: Comparison of MDP and QMDP value functions. Each plot is obtained from a di↵erent
initialization of the model parameters. The red lines are the optimal quantile rewards computed via QMDP.
The gray dashed lines are the cumulative density function for simulations with the execution of the optimal
MDP policy.

2.1.3

Illustration of the QMDP Output

The method presented in this chapter computes the QMDP optimal value function and optimal
policy for all quantiles with a single pass of dynamic programming. Figure 2.1 shows the QMDP
optimal value function for three di↵erent underlying MDPs that share the same state and action
space but have di↵erent reward functions and transition probabilities (this example is worked out
in Section 2.6.1). Each point on the red solid curve indicates the optimally achievable quantile level
for the cumulative reward. The gray dashed curve shows the empirical cumulative density function
(CDF) of the cumulative reward under the traditional MDP optimal policy that maximizes the
expected reward.
The QMDP optimal value function captures the distributional information of the cumulative
reward. For nested risk measure or utility function formulations the value function could be a
complicated non-linear transformation of the cumulative reward; if we want to know the distribution
of the cumulative reward, simulation of each policy is necessary (in the same way that the empirical
CDF is obtained for the traditional MDP). Additionally, risk-sensitive MDP models usually involve
a trade-o↵ procedure between risk and reward, which entails solving models for multiple parameters.
For QMDP the optimal value function can be computed for all parameters at once.
The QMDP model output provides a risk assessment for the underlying MDP problem. The
three MDP problems in Figure 2.1 have di↵erent patterns of inherent risk. For the example in the
middle panel, if a quantile reward is desired, there is an opportunity for significant improvement

CHAPTER 2. QUANTILE MARKOV DECISION PROCESSES

9

for quantiles below the median. In this case, a risk-sensitive MDP model might be desirable for a
risk-averse decision maker. For the example in the left panel, the only significant di↵erence occurs
at the lowest quantiles. In this case the optimal policy under the traditional MDP, although not
necessarily achieving quantile optimality, is quite stable and robust. In this way, QMDP can be
used to determine whether a risk-sensitive MDP model is desirable and what kind of improvement
one would expect if adopting a risk-sensitive MDP.
The remainder of this chapter is organized as follows. We lay out the traditional MDP problem
formulation and assumptions in Section 2.2 and present the QMDP problem formulation and dynamic programming solution in Section 2.3. We describe the algorithm for solving QMDP as well
as its computational aspects in Section 2.4. We discuss extensions of the model in Section 2.5. We
present empirical results on a synthetic example as well as on an HIV treatment initiation problem
in Section 2.6. We conclude with discussion in Section 2.7.

2.2

Markov Decision Process

A Markov decision process (MDP) consists of two parts [21]: an underlying discrete-time dynamic
system, and a reward function that is additive over time. A dynamic system defines the evolution
of the state over time:
St+1 = ft (St , at , wt ), t = 0, 1, ..., T

1,

(2.1)

where St denotes the state variable at time t from state space S, at denotes the actions/decisions
at time t and wt is a random variable that captures the stochasticity in the system. The reward
function at time t, denoted by rt (St , at , wt ), accumulates over time. The cumulative reward is
rT (ST ) +

T
X1

rt (St , at , wt ),

t=0

where rT (ST ) is the terminal reward at the end of the process. The random variable wt 2 W

determines the transition in the state space and the state St+1 follows a distribution Pt (·|St , at )
that is possibly dependent on the state St and the action at . We consider the class of policies
that consist of a sequence of functions ⇡ = {µ0 , ..., µT

1 } where µt maps historical information

Ht = (S0 , a0 , ..., St 1 , at 1 , St ) to an admissible action at 2 At ⇢ A. Here we use At and A

to denote the admissible action set. The policy ⇡ together with the function ft determines the
dynamics of the process. Given an initial state S0 and a policy ⇡, we have the following expected
total reward:

"

E ⇡ rT (ST ) +

T
X1
t=0

#

rt (St , at , wt ) .

CHAPTER 2. QUANTILE MARKOV DECISION PROCESSES

10

The objective of an MDP is to choose an optimal policy in the set ⇧ of all admissible policies
that maximizes the expected total reward, i.e.
max E
⇡2⇧

⇡

"

rT (ST ) +

T
X1

#

rt (St , at , wt ) ,

t=0

where the expectation is taken with respect to (w0 , w1 , ..., wT

(2.2)

1 ). Without loss of generality, we

assume rT (ST ) = 0 for all ST .

2.2.1

Assumptions

We first discuss a few assumptions and clarify the scope of this chapter.
Assumption 1 (State and Action Space)
(a) The state space S and the action space A are finite.
(b) The random variable wt 2 W has a finite support, i.e. |W| < 1.
(c) The function ft is “weakly invertibile”: S ⇥ A ⇥ W ! S governs the dynamic system (2.1).

Specifically, there exists a function lt : S ⇥ A ⇥ S ! W such that for any s 2 S, a 2 A and
w 2 W,

lt (s, a, ft (s, a, w)) = w.

Part (a) is a classic assumption about the finiteness of the state and action spaces. In part (b),
we assume that the random variable wt has a finite support, i.e. wt is a discrete random variable
only taking finite possible values. This chapter concerns the quantiles of cumulative reward; if wt
has infinite support, it will result in the reward rt (St , at , wt ) and the cumulative reward having
infinite support. In fact, there is no general way to store the infinite support random variable or
to query its quantiles unless the distribution has some parameterized structure. Since we aim for a
generic treatment of the quantile MDP problem, the assumption of finite support is necessary. Also,
because a random variable can be always approximated by a finitely supported discrete random
variable at any granularity, we believe part (b) will not cause much practical limitation.
Part (c) is introduced for notational simplicity in our derivation. We show how to remove this
assumption in Appendix A.1. Part (c) states that the random variable wt can be fully recovered
with the knowledge of St , at , and St+1 , i.e. there exists a function lt s.t. wt = lt (St , at , St+1 ) . This
assumption means that there is no additional randomness other than that which governs the state
transitions. It follows that the reward rt will be a function of St , at , and St+1 . In practice, this

CHAPTER 2. QUANTILE MARKOV DECISION PROCESSES

11

assumption is well satisfied by most MDP applications. Additionally, we allow the dynamics ft (·)
in part (c) and the reward function rt (·) to be non-stationary and non-parametric.

2.3

Quantile Markov Decision Process

In this section, we formulate the QMDP problem and derive our main result – a dynamic programming procedure to solve QMDP. All proofs are provided in Appendix A.2.

2.3.1

Quantile Objective and Assumptions

The quantile of a random variable is defined as follows.
Definition 1 For ⌧ 2 (0, 1), the ⌧ -quantile of a random variable X is defined as
Q⌧ (X) inf{x | P (X  x)

⌧ }.

For ⌧ = 0, 1 we define Q0 (X) = inf{X} and Q1 (X) = sup{X}, respectively.1
The following properties are implied by the definition.
Lemma 1 For a given random variable X, Q⌧ (X) is a left continuous and non-decreasing function
of ⌧. Additionally,
P (X  Q⌧ (X))

⌧.

The goal of the QMDP is to maximize the ⌧ -quantile of the total reward:
max Q⇡⌧
⇡2⇧

"T 1
X

#

rt (St , at , wt ) .

t=0

Here the quantile is taken with respect to the random variables (w0 , w1 , ..., wT

(2.3)
1 ), and the super-

script ⇡ denotes the policy we choose. The above formulation is for the case of a fixed finite horizon,
i.e. T < 1. For the infinite-horizon case, the objective is
max Q⇡⌧
⇡2⇧

where

"1
X
t=0

t

#

rt (St , at , wt ) ,

(2.4)

2 (0, 1) is the discount factor.

1 Here we do not consider the e↵ect of 0-measure set. More precisely, the definition should be Q (X) = sup{D 2
0
R|P (X D) = 1} and Q1 (X) = inf{U 2 R|P (X  U ) = 1}.

CHAPTER 2. QUANTILE MARKOV DECISION PROCESSES

12

As in the derivation of MDP with expectation objective, we introduce a value function for the
quantile reward of the Markov decision process. Suppose that the process initiates in state s at
time t, and we adopt the policy ⇡t:T . The value function is
vt⇡t:T (s, ⌧ )Q⌧

"T 1
X

#

rk (Sk , ak , wk ) St = s .

k=t

Here ⇡t:T = (µt , ..., µT

1 ) denotes the policy and the action

ak = µk (Hk0 ) = µk (St , at , ..., Sk )
for k = t, ..., T

1. Since the process initiates at time t, the history Hk0 also begins with St . We

emphasize that the value function is a function of both the state s and the quantile of interest ⌧
and is indexed by time t. The value function also depends on the chosen policy ⇡t:T .

The objective of QMDP is to maximize the value vt⇡t:T (s, ⌧ ) by optimizing the policy ⇡t:T . Thus,
we define the optimal value function as
vt (s, ⌧ ) max vt⇡t:T (s, ⌧ ).
⇡t:T 2⇧

(2.5)

When t = 0, the value function v0 (s, ⌧ ) is equal to the optimal value in (2.3).

2.3.2

Value Function and Dynamic Programming

We construct a dynamic programming procedure and derive the optimal value function vt (s, ⌧ )
backward from t = T

1 to t = 0. The key step is to relate the value functions vt (s, ⌧ ) with

vt+1 (s, ⌧ ). Intuitively, vt+1 (s, ⌧ ) is obtained by optimizing ⇡(t+1):T while vt (s, ⌧ ) is obtained by
optimizing ⇡t:T . The di↵erence lies in the choice of ⇡t = µt (·). To connect them, we introduce an
intermediate value function by fixing the output action of µt (s) to be a:

Note that

vet (s, ⌧, a)

max

{⇡t:T 2⇧|µt (s)=a}

vt⇡t:T (s, ⌧ ).

vt (s, ⌧ ) = max vet (s, ⌧, a).
a

(2.6)

CHAPTER 2. QUANTILE MARKOV DECISION PROCESSES

13

We now establish the relationship between vet (s, ⌧, a) and the value function vt+1 (s0 , ⌧ 0 ). We have
vet (s, ⌧, a) =
=

max

vt⇡t:T (s, ⌧ )

max

X

{⇡t:T 2⇧|µt (s)=a}

{⇡t:T 2⇧|µt (s)=a}

Q⌧

0

s0 2S

1{St+1 = s |St = s, at = a}

"T 1
X

rk (Sk , ak , wk ) St = s, St+1 = s

k=t

0

#!

.

(2.7)

Here the second line is obtained by di↵erentiating possible values for the state St+1 . It is a summation of |S| random variables, each of which is associated with a specific state s0 . Analyzing each
term more carefully, we have,
0

1{St+1 = s |St = s, at = a}

"T 1
X

rk (Sk , ak , wk ) St = s, St+1 = s

k=t

0

0

= 1{St+1 = s |St = s, at = a}rt (St , at , wt )+1{St+1 = s |St = s, at = a}

"T 1
X

0

#

rk (Sk , ak , wk ) St = s, St+1 = s

k=t+1

The first term here is deterministic with the knowledge of St and St+1 under Assumption 1 (c).
The second term seems to be closely related to the value function vt+1 (s0 , ⌧ 0 ) in that the summation
begins from t + 1 and the conditional part includes the information of St+1 . The following theorem
formally establishes the relationship between vet (s, ⌧, a) and vt+1 (s0 , ⌧ 0 ).

Theorem 1 (Value Function Dynamic Programming) Let S = {s1 , ..., sn }. Solving the value
function defined in (2.5) is equivalent to solving the following optimization problem:
OP T (s, ⌧, a, vt+1 (·, ·)) max
q

min

i2{qi 6=1|i=1,2,...,n}

= max
q

subject to

n
X
i=1

min

[vt+1 (si , qi ) + rt (s, a, wt )] ,

i2{qi 6=1|i=1,2,...,n}

(2.8)

[vt+1 (si , qi ) + rt (s, a, lt (s, a, si ))] ,

pi qi  ⌧, qi 2 [0, 1], pi = P (St+1 = si |St = s, at = a).

Here wt = lt (s, a, st+1 ) = lt (s, a, si ) is from Assumption 1 (c). We use vt+1 (·, ·) to denote the value

function at t + 1 and to emphasize that it is a function of state and quantile. The decision variable
here is the vector q. Then,
vet (s, ⌧, a) = OP T (s, ⌧, a, vt+1 (·, ·)) .

0

#

.

CHAPTER 2. QUANTILE MARKOV DECISION PROCESSES

14

The optimization problem stated in the theorem comes from the following lemma which computes the quantile of a sum of random variables (as it appears in the right-hand side of (2.7)).
Recall that the expectation of a summation of random variables equals the summation of the expectations, and this linearity makes possible the backward dynamic programming in the traditional
MDP. Lemma 2 plays a similar role in that it relates the quantile of the summation of random
variables to the quantiles of each random variable. This result together with the optimization algorithm in the next section is of potential interest for other applications concerned with the quantiles
of random variables.
Lemma 2 Consider n discrete random variables Xi , i = 1, ..., n, (here and hereafter, by discrete
random variables, we mean that Xi take values on a finite set) and another n binary random
Pn
variables Yi 2 {0, 1} with i=1 Yi = 1. Then the quantile of the summation
Q⌧

n
X

Xi Yi

i=1

!

is given by the solution to the following optimization problem:
max

min

q

i2{qi 6=1|i=1,2,...,n}

subject to

n
X
i=1

Qqi (Xi )

(2.9)

pi qi  ⌧,

qi 2 [0, 1], pi = P (Yi = 1).
Here q = (q1 , ..., qn ) is the decision variable and Qqi (Xi ) is the qi -quantile of the conditional distribution Xi |Yi = 1.
The key idea for the proof of Theorem 1 is to introduce a random variable Xi such that its
quantile Q⌧ (Xi ) = Q⌧ (vt+1 (si , ⌧ ) + rt (s, a, h(s, a, si ))) for all ⌧ 2 [0, 1]. Then the right-hand side
of (2.8) is in the same form as (2.9) and Lemma 2 applies. By putting Theorem 1 together with

(2.6), we establish the relationship between vt (s, ⌧ ) and vt+1 (s0 , ⌧ 0 ) and build the foundation for a
backward dynamic program to compute optimal value functions. Importantly, the algorithm derives
the entire value function, i.e., the output we obtain at time t is the function vt (·, ·) rather than its
evaluation at some specific s and ⌧.

2.3.3

Optimal Value and Optimal Policy

In this section, we establish that the value functions obtained from the backward dynamic program
correspond to the optimal value for the QMDP and thus define the optimal policy. The procedure

CHAPTER 2. QUANTILE MARKOV DECISION PROCESSES

15

Figure 2.2: Illustration of backward dynamic program for computing vt from vt+1 . Here pki =
P (St+1 = sk |St = s, a = ak ).
for computing the value functions is illustrated in Figure 2.2. The optimization problem OPT takes
vt+1 as its argument and outputs ṽt (s, ⌧, a); then by taking maximum over the action a, we obtain
vt . Theorem 2 verifies that the value function v0 computed via backward dynamic programming is
equal to the optimal quantile value.
Theorem 2 (Optimal Value Function) Let vT (s, ⌧ ) = 0 for all s 2 S and ⌧ 2 [0, 1]. Iteratively,
we compute

vt (s, ⌧ ) = max OP T (s, ⌧, a, vt+1 (·, ·)) ,
a

for t = T

1, ..., 0. Then we have
v0 (s, ⌧ ) = max
⇡2⇧

Q⇡⌧

"T 1
X
k=0

#

rk (Sk , ak , wk ) .

Theorem 3 characterizes the optimal policy. Unlike the case of MDP, the optimal policy ⇡t for
QMDP is a function of the history ht = (S0 , a0 , ..., St ) instead of simply the current state St – but
all of the history ht is summarized in the quantile level ⌧t . In other words, ⌧t is a function (although
not explicit) of the history and plays a role like that of a “summary statistic.” Theorem 3 tells us
that the optimal policy ⇡t is a function of only the current state St and the “summary statistic”
⌧t . Intuitively, this augmented quantile level ⌧t reflects the historical performance of the MDP. A
higher quantile level will encourage a more aggressive policy in the remaining periods while a lower

CHAPTER 2. QUANTILE MARKOV DECISION PROCESSES

16

quantile level will encourage conservative moves. For example, if we start with ⌧0 = ⌧ = 0.5, which
means that our ultimate goal is to maximize the median cumulative reward over 0 to T , then at
Pt
some time t in between, if we have already achieved a relatively high reward, i.e., a large k=0 rk ,
the augmented quantile level ⌧t will decrease to some value smaller than 0.5 accordingly. This will
drive us to take relatively conservative moves in the future, and vice versa.
Theorem 3 (Optimal Policy) We augment the state St with a quantile ⌧t to assist in the execution of the optimal policy. At the initial state s0 and ⌧0 = ⌧, we define our initial policy function
as
⇡0 : µ0 (s0 , ⌧0 ) = arg max ve0 (s0 , ⌧0 , a).
a

At time t, we execute the output of µt and then the process reaches state St+1 . Let q⇤ be the
solution to the optimization problem OP T (St , µt (St , ⌧t ), ⌧t , vt+1 (·, ·)). Here vt+1 (·, ·) is computed
as in Theorem 2. The term ⌧t+1 is assigned as

⌧t+1 = qi⇤
for the specific i that satisfies St+1 = si . We define ⇡t+1 as
⇡t+1 : µt+1 (St+1 , ⌧t+1 ) = arg max vet+1 (St+1 , ⌧t+1 , a).
a

The policy ⇡ = (⇡0 , ..., ⇡T ) defined above is the optimal policy for the objective (2.3) and obtains
the optimal value v0 (s0 , ⌧0 ).

2.3.4

QMDP and Other Risk Measures

We compare QMDP to other risk measures using a simple example. Consider the following twoperiod gambling game: In time period one, a gambler participates in the game and receives or loses
$50 with equal probability; in time period two, the gambler has an option to participate in one of
two fair games with, respectively, an equal chance of winning or losing $20 or $100. In the MDP
framework, the two options are equivalent because both output a zero expected return.
Using MDP with a nested risk measure [118, 74], the objective for this gambling game is
max ⇢✓ (r1 + ⇢✓ (r2,a )),
a

where r1 denotes the random reward in time period one, a denotes the action, r2,a denotes the
random reward in time period two, and ⇢✓ (·) is the risk measure to be specified by the decision
maker where the parameter ✓ reflects the decision maker’s risk attitude. Assuming the risk measure

CHAPTER 2. QUANTILE MARKOV DECISION PROCESSES

(a) Optimal value function

17

(b) Optimal policy

Figure 2.3: QMDP optimal value function and optimal policy for two-period gambling game.
⇢✓ (·) is monotonically non-decreasing, the optimal action a is determined by
max ⇢✓ (r2,a ).
a

The optimal action for this model does not take into account the reward history. This allows for
a dynamic programming algorithm that solves for the optimal decision (see [118] and [74]), but it
fails to capture the subsequent risk attitude of the gambler. If r1 = $50, the gambler might prefer
to adopt the more conservative option in the second time period to guarantee winning at least $30
by the end. On the other hand, if r1 =

$50, the gambler might prefer to participate in the risky

game ($100) to compensate for the loss; by taking the $20 game in time period two, the gambler
is doomed to lose money, whereas by taking the $100 game, there is a chance of winning money at
the end. To capture this type of risk attitude, the risk measure ⇢✓ (·) at time period two should be
dependent on the outcome of r1 , which cannot be covered by a nested MDP formulation.
In the QMDP model, the objective function is
max Q⌧ (r1 + r2,a )
a

where ⌧ is the quantile level specified by the gambler. Figure 2.3 shows the optimal QMDP value
function, calculated as in Section 2.3.2, and the optimal policy, obtained through forward execution
following Theorem 3. The optimal action in time period two is a↵ected by both the risk parameter
⌧ and the outcome of time period one. For example, if ⌧ = 0.4, which is a slightly conservative
attitude, then the gambler will participate in the less risky ($20) game if r1 = $50 but will participate
in the risky ($100) game if r1 =

$50. If ⌧ = 0.6, which is a slightly aggressive attitude, then the

gambler will participate the less risky game if r1 = $50 just as in the case of a conservative

CHAPTER 2. QUANTILE MARKOV DECISION PROCESSES

18

attitude; however, if there were no time period one but only time period two, then the gambler
would participate in the riskier game for the 50% chance of winning $100. In QMDP, the risk
attitude that governs the optimal action of each time period can change dynamically according to
the outcomes in the past time periods. QMDP can search for the optimal policy in a more general
class of (non-Markovian) policies than the nested risk measure models.
Using a utility function-based MDP formulation, the objective function is
max u✓ (r1 + r2,a ),
a

where u(·) is a utility function and subscript ✓ denotes risk attitude. In such a model there is no
transparent connection between the risk attitude (the choice of ✓) and the outcome r1 + r2,a , and
there is no clear characterization of the outcome or the objective function value under di↵erent
choices of ✓ unless we repeatedly solve the problem with di↵erent specifications of ✓. The QMDP
model provides a more explicit visualization of the return by characterizing the risk of the cumulative
reward in a distributional manner.

2.4

Algorithms and Computational Aspects

In this section, we present our algorithm for solving QMDP and discuss its computational aspects.
As mentioned earlier, the key for computing the value function is to solve the optimization problem
OPT. Thus, we first provide an efficient algorithm for solving OPT and then analyze its complexity.

2.4.1

Algorithm for Solving the Optimization Problem OPT

We formulate OP T (s, ⌧, a, vt+1 (·, ·)) in a more general way as follows:

OP T max
q

subject to

min

i2{qi 6=1|i=1,2,...,n}
n
X
i=1

qi 2 [0, 1],

pi qi  ⌧,
for

n
X

g(i, qi ),

(2.10)

pi = 1,

i=1

i = 1, ..., n.

Here ⌧ and the pi ’s are known parameters. The decision variable is q = (q1 , ..., qn ). We introduce a
function g : {1, ..., n}⇥[0, 1] ! R. We assume that g(i, ·) is a left continuous and piecewise constant
function with finite breakpoints for all i. The variable i refers to the state in the QMDP settings.
We will show later that these assumptions are satisfied for value functions of QMDP with finite

CHAPTER 2. QUANTILE MARKOV DECISION PROCESSES

19

state space and discrete rewards. Therefore, we can represent and encode each function g(i, ·) with
a set of breakpoint-value pairs

n⇣

(1)

(1)

bi , v i

⌘

⇣
⌘o
(n ) (n )
, ..., bi i , vi i

where ni is the number of pairs. Then we have

g(i, x) =

h
i
(1) (2)
for x 2 bi , bi ,
⇣
i
(k) (k+1)
for x 2 bi , bi
and k = 2, ...ni .

8 (1)
< vi ,

: v (k) ,
i

(1)

Here we define bi

(n +1)

= 0 and bi i

= 1 for all i.

Algorithm 1 Algorithm for the Optimization Problem (2.10)
1: Input: {pi , g(i, ·) | i = 1, .., n}
2: Initialize k1 = ... = kn = 1, ⌧tmp = 0.
(1)

3: Let ui = g(i, 0) = vi
4: Let f (0) = u.
5: while S 6= ; do
6:
7:
8:
9:
10:

, i 2 S = {1, ..., n} and u = mini2S ui .

S0 = argmini2S ui
⌧new = ⌧tmp
for i 2 S0 do
if ki = ni then
⇣
⌧new = ⌧new + pi 1

(k )

bi i

⌘

S = S\{i}
else
⇣
⌘
(k +1)
(k )
⌧new = ⌧new + pi bi i
bi i
14:
ki = ki + 1
(k )
15:
Update ui = vi i
16:
if ⌧tmp = 0 then
17:
Let f (⌧ ) = u for ⌧ 2 [⌧tmp , ⌧new ]
18:
else
19:
Let f (⌧ ) = u for ⌧ 2 (⌧tmp , ⌧new ]
20:
Update ⌧tmp = ⌧new
21:
Update u = mini2S ui
22: Return f (·)
11:
12:
13:

Algorithm 1 solves OPT. The idea is quite straightforward: we start with qi = 0 for all i and
Pn
gradually increase the qi that has the smallest value of g(i, qi ) until the constraint i=1 pi qi  ⌧
is violated. The g(i, qi ) that has smallest value is the bottleneck for the objective function value.

By increasing the corresponding qi , we keep improving the objective function value. The output of

CHAPTER 2. QUANTILE MARKOV DECISION PROCESSES

20

Figure 2.4: Step-by-step execution of Algorithm 1 with n = 3 sample g(·, ·) functions. Numbers inside
and along the blocks represent the values and breakpoints of the input functions g(i, ·). The shaded regions
reflect the progress of the algorithm. In the end, the output is f.

the algorithm f (·) restores the optimal values of OPT as a function of ⌧ 2 [0, 1].

We illustrate the algorithm with an example of n = 3 in Figure 2.4. In this example, we have

three functions g(i, ·) represented by three gray rectangles with corresponding transition probabilities denoted by pi . We want to determine f (·), which is indicated by the red rectangle for each step.
Following Algorithm 1, we initialize input k1 = k2 = k3 = 1, ⌧tmp = 0 and u1 = 10, u2 = 8, u3 = 10.
We then find (Step 1) u = mini2S={1,2,3} ui = 8, and thus f (0) = 8. To find the upper bound b(2)
for the value of 8, we execute the “while” loop. The only g(i, ·) that has value of 8 is g(2, ·) so

we assign S0 = 2 ⇣
and ⌧new = ⌧tmp
⌘ = 0. Since k2 = 1 and n2 = 2 in this example, we can update
(k +1)

⌧new = ⌧new + pi b2 2

(k )

b2 2

= 0 + 0.5(0.4

0) = 0.2. Thus, in Step 1 we have f (⌧ ) = 8

for ⌧ 2 [0, 0.2]. The algorithm keeps updating f (·) until the set S becomes empty. In the end
(rightmost panel of Figure 2.4) we have fully specified f (·), and thus we have found the optimal
values of OPT as a function of ⌧ 2 [0, 1].

2.4.2

Algorithm for Solving QMDP

In this subsection, we summarize the previous results and provide the algorithm for solving QMDP
as Algorithm 2. It is obtained by putting together Algorithm 1 with Theorems 2 and 3. One
advantage of this dynamic programming algorithm is that the optimal value functions and the
optimal policies at all states and quantiles are computed in a single pass. Indeed, this single-pass
property is necessary for the quantile objective, because the optimal value and action at time t
could depend on the value function at time t + 1 for all the quantiles.

2.4.3

Complexity Analysis and Approximation

The computational cost of our algorithm for solving QMDP (Algorithm 2) is mostly concentrated in
computing the value functions. It is easy to show that all the value functions are piecewise constant.
This is because when the input functions of OPT are piecewise constant, the OPT procedure will

CHAPTER 2. QUANTILE MARKOV DECISION PROCESSES

21

Algorithm 2 Algorithm for Solving QMDP
1: Input: Transition probabilities P(St , a, St+1 ), reward function rt (st , at , wt ), time horizon T .
2: Computing Value Functions:
3: Initialize Let S = {s1 , ..., sn }, vT (si , ⌧ ) = 0 for all i = 1, ..., n and ⌧ 2 [0, 1].
4: for t = T
1, ...., 0 do
5:
for i = 1, ..., n do
6:
for a 2 At do
7:
ptmp (sj ) = P(St = si , a, St+1 = sj ) for j = 1, ..., n
8:
vtmp (sj , ⌧ ) = vt+1 (St+1 = sj , ⌧ ) + rt for j = 1, ..., n and ⌧ 2 [0, 1]

ve(si , ⌧, a) = OP T (ptmp (·), vtmp (·, ·))
vt (si , ⌧ ) = maxa ve(si , ⌧, a)
11: Output: {vt (s, ⌧ ), v
et (s, ⌧, a)}Tt=01 for all s 2 S, a 2 A and ⌧ 2 [0, 1]
12: Execution:
13: Initialize S0 = s and our goal is to maximize ⌧ quantile. Let R = 0 and ⌧0 = ⌧.
14: for t = 0, ..., T
1 do
15:
Take action at = arg maxa vet (St , ⌧t , a)
16:
Transit from St to St+1 = sj for some j 2 {1, ..., n}
17:
R = R + rt (st , at , st+1 )
18:
Let q⇤ be the optimizer of OP T (St , at , vt+1 (·, ·))
19:
Update ⌧t+1 = q⇤j
9:

10:

20: Output: Cumulative reward R

output a piecewise constant function as well. Also, it can be readily seen that the complexity of
Algorithm 1 is linear in the number of breakpoints for its output functions. Based on these facts,
we have the following proposition.
When the rewards are integer and bounded, |rt |  R for all t, then the complexity of Algorithm

1 for computing value functions for QMDP is O (AST · max(RT, S)) . Here T is the length of the
time horizon, and A = |A| and S = |S| are the sizes of the action and state space, respectively.

The proof of this proposition is straightforward: When the reward is integer and bounded by

R, the cumulative reward is bounded by RT. Thus any value function has at most RT breakpoints,
which means that each call of OPT will induce at most O(RT ) complexity. Additionally, each call
of OPT will have a read and write complexity of O(S). Therefore each iteration has O(max(RT, S))
complexity. Since there are AST iterations in total, the overall complexity is O (AST · max(RT, S)) .

Though the algorithms work for both integer and non-integer cases, the analysis is more com-

plicated when the rewards are non-integer because we have no simple way to bound the number of
breakpoints for the value functions. The value function can become “exponentially” complicated
as the backward dynamic programming proceeds, so that the cost to restore the value function
will also grow exponentially. Nemirovski and Shapiro [97] pointed out that the computation of
the distribution of the sum of independent random variables is already NP-hard. To prevent this

CHAPTER 2. QUANTILE MARKOV DECISION PROCESSES

22

explosion, one can either truncate the rewards to integers or create approximations of the value
functions. For the truncation approach, if we still want to preserve computational precision, we
can scale up the rewards before truncation. For the approximation approach, we would restore the
value function at N uniform breakpoints. The choice of N is up to the user and can be as large
as, for example, 10, 000, which means that we restore the value function only for all the quantile
values with an interval of 0.0001.
From the above analysis, we observe that the bottleneck for the complexity of our algorithm
lies in the complexity of the value function. In traditional MDP, the value function is a function of
the state s and time stamp t. In QMDP, for each given s and t, we need to compute and retain the
optimal values for all the quantiles in order to derive the value function for time t

1. Therefore,

there is not much room for improvement on this complexity upper bound in a generic setting.

2.5

Extensions

In this section, we discuss several extensions of the QMDP model. We extend the model to solving
CVaR MDP (Section 2.5.1) and present a time-consistency result for the quantile risk measure
(Section 2.5.2). We establish the optimal value and policy for the infinite-horizon case (Section
2.5.3).

2.5.1

Conditional Value at Risk

In this section, we show how the dynamic programming idea in QMDP extends to the CVaR
objective. We follow the characterization of [117] for a definition of CVaR.
Definition 2 For ⌧ 2 (0, 1), the conditional value at risk (CVaR) at level ⌧ is defined as
CVaR⌧ (X)Q⌧ (X) +

1
1

⌧

E [X

+

Q⌧ (X)] .

We consider an alternative objective, that of maximizing the CVaR of the cumulative reward.
max
⇡2⇧

CVaR⇡⌧

"T 1
X
t=0

#

rt (St , at , wt ) .

(2.11)

[15] and [142] solved the CVaR MDP problem via an augmented variable but their approach was
computationally intensive. Our results using the QMDP model reveal the key step in the dynamic
programming for CVaR MDP as an optimization problem that is similar to our OPT problem.

CHAPTER 2. QUANTILE MARKOV DECISION PROCESSES

23

Chow and Ghavamzadeh [35] considered MDP with a CVaR constraint, Carpin et al. [28] developed
approximate algorithms for CVaR MDP under a total cost formulation, and Chow et al. [36] solved
CVaR MDP for the case of an infinite horizon and discounted reward. The method presented here
complements this line of literature, and the core part of our dynamic programming procedure shares
the same spirit as the CVaR decomposition (proposed in [113] and later exploited by Chow and
Ghavamzadeh [35]).
As for the quantile objective, we define the value function
u⇡t t:T (s, ⌧ )CVaR⇡⌧

"T 1
X

#

rk (Sk , ak , wk ) St = s .

k=t

Here ⇡t:T = (µt , ..., µT

1 ) denotes the policy and the action

ak = µk (Hk0 ) = µk (St , at , ..., Sk )
for k = t, ..., T

1.

Theorem 4 (CVaR Value Function) Let S = {s1 , ..., sn } and uT (s, ⌧ ) = u0T (s, ⌧ ) = 0 for all
s 2 S and ⌧ 2 (0, 1). Then,

ũ0t (s, ⌧, a) = OP T (s, ⌧, a, u0t+1 (·, ·))

and q⇤ = (q1⇤ , ..., qn⇤ ) as the optimal solution to the OPT problem (dependent on s and ⌧ ).
ũt (s, ⌧, a) =
=

1
1

n
X

⌧ i=1
1

1

n
X

⌧ i=1

pi (1

qi ) [ut+1 (si , qi⇤ ) + rt (s, a, wt )]

pi (1

qi ) [ut+1 (si , qi⇤ ) + rt (s, a, lt (s, a, si ))]

Here wt = lt (s, a, st+1 ) = lt (s, a, si ) is from Assumption 1 (c). By taking maximum over the action
a,
ut (s, ⌧ ) = max u
et (s, ⌧, a).
a

⇤

Denote the optimal action as a (dependent on s and ⌧ ).

In this way, we have

u0t (s, ⌧ ) = u
e0t (s, ⌧, a⇤ ).
u0 (s, ⌧ ) = max CVaR⇡⌧
⇡2⇧

"T 1
X
t=0

#

rt (St , at , wt ) .

CHAPTER 2. QUANTILE MARKOV DECISION PROCESSES

24

Theorem 4 presents a dynamic programming formulation for the CVaR MDP problem. The key
observation is that the CVaR definition involves the quantile, and the results developed Section
2.4 provide useful tools for quantile-related computations. The functions ut and u0t in Theorem
4 represent the optimal CVaR cost-to-go value function and the corresponding quantiles of the
cumulative reward. In contrast to QMDP, the formulation here takes the maximum of the CVaR
function and updates the corresponding quantile function according to the optimal action. This
result provides a finite-horizon solution that complements the infinite-horizon solution in [36].

2.5.2

Dynamic Risk Measures

The quantile objective, as a dynamic risk measure, has been criticized for its time-inconsistency
[30, 70]. In fact, the quantile objective specifies a family of risk measures (functions) parameterized
by the quantile level ⌧ and thus the conventional notion of time-consistency no longer fits. In
Theorem 5, we present a time-consistency result for the quantile risk measure. The result is implied
by the dynamic programming results developed in the previous sections.
Theorem 5 Given two real-value Markov chains with a finite state space, {Xt }Tt=01 and {Yt }Tt=01
and a function r : R ! R, if

Q⌧

T
X1
t=0

r(Xt ) Fk

!

Q⌧

T
X1
t=0

r(Yt ) Fk

!

(2.12)

holds for k = 1, ..., T 1 and all ⌧ 2 (0, 1), where Fk denotes the -algebra generated by {(Xt , Yt )}kt=1 ,
and if X0 and Y0 have an identical distribution, then we have
Q⌧

T
X1
t=0

r(Xt )

!

Q⌧

T
X1
t=0

!

r(Yt ) .

{Xt }Tt=0 and {Yt }Tt=0 can be interpreted as two Markov chains specified by an MDP with two

fixed policies. Condition (2.12) in the above theorem can be viewed as parallel to the dynamic risk

measure in [30] and [70], but it is a stronger condition because here the inequality is required to
hold for all ⌧ 2 [0, 1]. We can see that this is entailed in the dynamic programming procedure for

solving QMDP, where the quantile value at time t depends on the quantile values at time t + 1
for all ⌧ 2 (0, 1) in general. This explains why the quantile objective, as a risk measure, is not

time-consistent in the conventional sense, where we only require that inequality (2.12) holds for a

CHAPTER 2. QUANTILE MARKOV DECISION PROCESSES

25

fixed ⌧ . It emphasizes that the optimization of a dynamic quantile risk measure requires changing
the risk measure parameter (the quantile level ⌧t in Theorem 3) itself over time. With the stronger
condition (2.12), the quantile objective can also be viewed as a time-consistent risk measure.
The changing of the risk measure parameter can be seen in the example in Section 2.3.4. We
know that the risk parameter ⌧t in the QMDP model changes over time according to both ⌧t 1
and the reward outcome rt 1 . In the example, if ⌧0 = ⌧ = 0.4 and r1 = 50, then ⌧1 = 0.3. A
risk-averse decision maker will become more risk-averse if a good return is achieved in the first time
period under the quantile objective. The dynamic changing of risk parameter cannot be captured
by conventional time-consistent risk measures such as the nested risk measure that require the same
risk parameter over the entire time horizon. In this way, the quantile objective enriches the family
of time-consistent risk measures.

2.5.3

Infinite-Horizon QMDP

For the infinite-horizon QMDP, the objective is
max Q⇡⌧
⇡2⇧

"1
X

t

#

rt (St , at , wt ) .

t=0

2 (0, 1) is the discount factor.

Here rt = r(St , at , wt ) is stationary and

The policy ⇡ =

{µt }1
t=0 consists of a sequence of decision functions and µt maps the historical information ht =
(S0 , a0 , ..., St 1 , at 1 , St ) to an admissible action at 2 At ⇢ A. The value function is
v(s, ⌧ ) max Q⇡⌧
⇡2⇧

"1
X

t

#

rt (St , at , wt ) S0 = s .

t=0

(2.13)

Similar to the infinite-horizon MDP, we propose a value iteration procedure to compute the
QMDP value function. The result is formally stated as Theorem 6. We use k to denote the
iteration number here to distinguish it from the index notation t in Theorem 2 which is the time
stamp for backward dynamic programming.
Theorem 6 (Infinite-Horizon Optimal Value Function) Consider the following value iteration procedure:
v (0) (s, ⌧ ) = 0,
ve(k+1) (s, ⌧, a) = OP T (s, ⌧, a, v (k) (·, ·)),
v (k+1) (s, ⌧ ) = max ve(k+1) (s, ⌧, a).
a

CHAPTER 2. QUANTILE MARKOV DECISION PROCESSES

26

Then we have
lim v (k) (s, ⌧ ) = v(s, ⌧ ),

k!1

for any s 2 S and ⌧ 2 [0, 1]. Furthermore, since the function v(s, ⌧ ) is a monotonic function for ⌧ ,
the convergence is uniform.

The key to the proof of the theorem is to show that the OPT procedure, as an operator, features the
same contractive mapping property as the Bellman operator in a traditional MDP. The contraction
rate is simply the discount factor . Based on the optimal value function, we have the following
result characterizing the optimal policy.
Theorem 7 (Infinite-Horizon Optimal Policy) Let v(·, ·) be the optimal value function as in
Theorem 6 and

ve(s, ⌧, a)OP T (s, ⌧, a, v(·, ·)).

We augment the state St with a quantile ⌧t to assist the execution of the optimal policy. At the
initial state s0 and ⌧0 = ⌧, we define our initial policy function as
µ0 (s0 , ⌧0 ) = arg max ve(s0 , ⌧0 , a).
a

At time stamp t, we execute ⇡t and then arrive at state St+1 . Let q⇤ be the solution to the optimization problem OP T (St , µt (St , ⌧t ), ⌧t , v(·, ·)). The term ⌧t+1 is defined as
⌧t+1 = qi⇤ ,
for the specific i such that St+1 = si , and µt+1 is defined as
µt+1 (St+1 , ⌧t+1 ) = arg max ve(St+1 , ⌧t+1 , a).
a

The policy ⇡ = {µt }1
t=0 is the optimal policy for the objective (2.13) and obtains the optimal value
v(s0 , ⌧0 ).

The value iteration procedure is similar to the backward dynamic programming procedure for
the finite-horizon case. This is because we can always interpret the finite-horizon reward as an
approximation of the infinite-horizon reward by truncating the reward after time T . It is worth
noting that CVaR does not break the value iteration aspect of the infinite-horizon case. Therefore
the results in the previous subsection for the infinite-horizon counterpart of CVaR MDP also hold,
and the value iteration procedure provides an alternative solution to the infinite-horizon CVaR
MDP problem [36].

CHAPTER 2. QUANTILE MARKOV DECISION PROCESSES

2.6

27

Empirical Results

We present two sets of empirical results, evaluating our model on both a synthetic example and a
problem of HIV treatment initiation.

2.6.1

Synthetic Experiment

Overview
We construct a synthetic example and perform simulations to illustrate the computational complexity of QMDP as a function of the state size, time horizon, and reward structure with comparison
to MDP and two risk-sensitive MDP models. We also demonstrate how the QMDP model can be
applied for risk assessment of an MDP.
Model Formulation
In this example, a player moves along a chain and receives rewards dependent on his location.
Figure 2.5 illustrates the model for this game. The arrows represent the possible movements. At
each time step (s)he takes the action to stay or to move. If the player chooses to move, (s)he will
move randomly to a neighboring state. The goal is to maximize expected cumulative reward over
the time horizon.

Figure 2.5: Illustration of the simple QMDP model.
We formulate the model in the language of MDP as follows:
• Time Horizon: We assume there are T decision periods.
• State: We denote the state by St , t = 0, .., T where St 2 S = {1, ..., n}.
• Action: At each time t, the player takes an action at 2 A = {Stay, Move}.
• Transition Probability :
– When at = Stay, the player will stay at his location with probability 1.
– When at = Move, the player will move randomly to one of its neighbors. When the
player starts from the ends of the chain, then (s)he moves to her/his single neighbor
with probability 1.

CHAPTER 2. QUANTILE MARKOV DECISION PROCESSES

28

• Rewards: When the player stays in state i at the beginning of a time period, (s)he receives
a reward Ri .

Results
We ran 105 simulation trials solving QMDP and MDP (on a laptop with a 2.8 GHz Intel Core i7).
In each simulation trial, the transition probabilities were randomly generated and the rewards were
randomly sampled integers no greater than Rmax . Figure 2.6 shows the average computation time
as a function of the time horizon T , the number of states n, and the maximum reward amount
Rmax . The CPU time for solving QMDP is quadratic in the time horizon T and linear in the state
size n, and grows linearly with maximum reward amount and then fluctuates after Rmax reaches a
certain level. This does not contradict the complexity analysis in Proposition 2.4.3; the quadratic
complexity in Rmax is an upper bound but is not necessarily tight for every trial. Figure 2.6 shows
that, as expected, MDP is more time efficient than QMDP since QMDP records the full distribution
of cumulative reward at each step of dynamic programming whereas MDP only records the mean
value.

Figure 2.6: Synthetic example: CPU time of QMDP and MDP. Base model parameters: time horizon T =
10, state space size n = 20, max reward Rmax = 10. For each experiment, we changed a single parameter
and monitored the running time. The dark red solid lines indicate the CPU time for execution of the
QMDP algorithm and dashed gray lines indicate the CPU time for execution of the MDP algorithm.

In addition, we implemented two other risk-sensitive MDP models. We applied the nested
composition of a one-step risk measure [74, 118], which aims to solve the following objective with

CHAPTER 2. QUANTILE MARKOV DECISION PROCESSES

29

Figure 2.7: Synthetic example: QMDP value function comparison with MDP and QBDP. Each plot is
obtained from a di↵erent initialization of model parameters. The gray dashed lines are the cumulative
density function for simulations with the execution of the optimal MDP policy. The red lines are the
optimal quantile rewards computed via QMDP. The remaining lines are the cumulative density functions
obtained by simulating the optimal policies from QBDP with di↵erent preset values of ⌧ .

Figure 2.8: Synthetic example: QMDP value function comparison with MDP and utility function-based
MDP. Each plot is obtained from a di↵erent initialization of the model parameters. The gray dashed lines
are the cumulative density function for simulations with the execution of the optimal MDP policy. The
red lines are the optimal quantile rewards computed via QMDP. The remaining lines are the cumulative
density functions obtained by simulating the optimal policies from the utility function-based approach with
di↵erent preset values of .

CHAPTER 2. QUANTILE MARKOV DECISION PROCESSES

30

⇢⌧ as a quantile operator for corresponding value at specified ⌧ :
max ⇢⌧ [r1 + ⇢⌧ (r2 + · · · + ⇢⌧ (rT ))].

(2.14)

⇡2⇧

We will refer to this as quantile-based dynamic programming (QBDP). We also implemented a
utility function-based approach [69, 34] which solves the MDP with exponential utility function
u(v) =
value of

v
|v| e

v

. The parameter

indicates the risk attitude of the decision maker: a negative

indicates that the decision maker is risk-seeking, whereas a positive

means the decision

maker is risk-averse.
Figures 2.7 and 2.8 compare the outcome of QMDP with MDP and with QBDP and the utility
function-based MDP, respectively. We generated three random simulation trials corresponding to
the three panels in each figure. To obtain the CDF for the cumulative reward under ⇡ ⇤ , we simulated
20, 000 instances and plotted the empirical histogram as the gray dashed line. We plotted the best
quantile reward obtained from QMDP as the red line. A point (q, r0 ) on the gray dashed line means
that the policy ⇡ ⇤ can achieve at least r0 cumulative reward with probability 1

q. A point (q, r1 )

on the red line means that the optimal q-quantile reward is r1 , i.e., there exists a policy that can
achieve at least r1 cumulative reward with probability 1

q. For the QBDP and utility function-

based approaches, we solved the problem by setting various values for the preset parameters (⌧ in
QBDP and

in the utility function approach) and then simulating 20, 000 instances to obtain the

CDF of cumulative reward associated with the corresponding policies.
The QMDP value function tells the optimally achievable quantile values for all quantiles. In
contrast, the optimal MDP value function only concerns the expectation, and the optimal QBDP
value function has no explicit connection to the cumulative reward. To interpret the corresponding
policy in a traditional MDP or QBDP, we need to run simulations and plot the histogram of the
cumulative reward. The computational cost of this simulation procedure may o↵set the computational advantage of such models. In Appendix A.3 we compare the computation time of QMDP
versus QDBP for the synthetic example. Moreover, most risk-sensitive MDP models, like QBDP,
only provide a glimpse of the inherent risk by solving the MDP problem with a single risk parameter. A procedure to trade o↵ the risk and reward is then needed to select a proper risk parameter.
QMDP simplifies the procedure by providing the information for all quantiles at once.
MDP Risk Assessment
In Figures 2.7 and 2.8, we observe that the red curve, by definition of the QMDP, is never below
the gray curve for any quantile. The gap between the curves indicates the space for improvement of
QMDP over MDP for any given quantile, and thus helps us understand the inherent risk associated
with the optimal MDP policy. Specifically,

CHAPTER 2. QUANTILE MARKOV DECISION PROCESSES

31

Figure 2.9: Synthetic example: Optimal QMDP actions at di↵erent states (s) and di↵erent time
periods (t) with di↵erent ⌧ values.
• For the example on the left in both figures, the only significant di↵erence occurs at the lowest
quantiles. In this case the policy ⇡ ⇤ , although not necessarily achieving quantile optimality,
is quite stable and robust.
• For the example in the middle in both figures, if a quantile reward is desired, there is an
opportunity for significant improvement for quantiles below the median. In this case, a risksensitive MDP model might be desirable for a risk-averse decision maker.
• For the example on the right in both figures, small di↵erences occur throughout, indicating
that if a quantile reward is desired, QMDP can achieve a somewhat better solution. In general,
if the gap between the two curves is not significant, the traditional MDP should be used since
it guarantees the optimal expected return in addition to achieving a near-optimal quantile
reward.
QMDP also provides risk interpretation of each state. Figure 2.9 shows the optimal QMDP
actions for an instance of the game shown in Figure 2.5. In this problem instance, T = 500, |S| = 8,
and the reward vector is (1, 10, 2, 0, 7, 9, 12, 18). We considered ⌧ = 0.2, 0.5, and 0.8. The color of

the point at location (t, s) denotes the optimal action when the state at time t is s. Note that the
optimal actions under all quantiles are obtained as the output of QMDP in one shot, as a byproduct
of the optimal value function. The highest reward, 18, is obtained in state 8 so, intuitively, the
optimal decision is to move until s = 8. However, state 2 also provides a good reward of 10, so
the player may want to stay in state 2 if there are not many remaining time periods because it
takes certain amount of time to traverse from state 2 to state 8 and less reward can be collected
during the process. The decision of whether to move from state 2 also depends on the player’s
risk attitude: a risk-averse player (⌧ = 0.2) would choose to stay while a risk-seeking player would

CHAPTER 2. QUANTILE MARKOV DECISION PROCESSES

32

choose to move. The optimal action plot in Figure 2.9 provides an understanding of this state-level
inherent risk and provides guidance for balancing the risk and reward for di↵erent risk attitudes
and time horizons.

2.6.2

Case Study: HIV Treatment Initiation

Background
An estimated 37 million people worldwide are living with HIV [140]. E↵ective antiretroviral therapy
(ART) reduces HIV-associated morbidity and mortality for treated individuals [128] and has transformed HIV into a chronic disease. However, there is debate around the optimal time to initiate
ART because of potential long-term side e↵ects such as increased cardiac risk [57]. Patients who
delay initiating ART may sacrifice immediate immunological benefits but avoid future side e↵ects.
Neogoescu et al. [96] constructed a sequential decision model to determine the ART initiation
time for individual patients that maximizes expected quality-adjusted life expectancy, taking into
account the potential for long-term side e↵ects of ART (increased cardiac risk). However, the MDP
model used in the analysis cannot capture patients’ risk attitudes, which may a↵ect their preferences
regarding treatment [56]. QMDP bridges the gap between the traditional MDP and the patient’s
risk attitude by allowing for di↵erent values of the quantile threshold in the QMDP objective to
incorporate the risk preferences.
Neogoescu et al. [96] and many other MDP healthcare applications have considered the impact
of parameter uncertainty on the optimal policy. Some of these e↵orts can be characterized as robust
MDP frameworks [100, 138, 144]. QMDP di↵ers in that it provides a unique perspective on the
inherent risk of the original MDP. The analysis explicitly reveals the uncertainty of the cumulative
reward and allows the analyst to determine whether a risk-sensitive MDP framework should be
used.
Model Formulation
The QMDP formulation of the optimal ART initiation time problem is straightforward and is similar
to the MDP formulation.
• Time Horizon: We assume the patient is assessed at each time period t 2 {0, 1, 2, · · · , T }.
• State: We characterize the state of the patient at time t as St = (ct , yt , dt ). The state is a
function of the patient’s CD4 cell count (ct ) (which is a measure of the current strength of

the patient’s immune system), age (yt ), and ART treatment duration (dt ). In addition, we
create an absorbing state for death, D. We divide the continuous CD4 cell counts into L bins,

CHAPTER 2. QUANTILE MARKOV DECISION PROCESSES

33

C = {C1 , C2 , · · · , CL }. For age, we have yt 2 [Y0 , YN ], where Y0 is the starting age and YN is
the terminal age of the patient. For treatment duration, dt = 0 indicates that the patient has
not yet started ART. Once the patient has started ART, dt increases by one unit after each
time step.
• Action: At each time t, the patient takes an action at 2 {W, Rx}, where W represents waiting
for another period and Rx means starting ART treatment immediately (and remaining on
ART for life).
• Transition Probability : The transition probability Pk (St , at , St+1 ) depends on the patient’s
current state St , the action at at time t, and the state St+1 . Two types of transitions can
occur: transition between di↵erent CD4 count levels and transition to the terminal (death)
state D.
• Rewards: Two types of rewards are accrued: an immediate reward and a terminal reward.
The immediate reward (RI ) is measured as the quality-adjusted life years (QALYs) the patient
experiences when transitioning from St to St+1 (St 2 C, St+1 2 {C, D}). We assume that if

death occurs (that is, the patient transitions to state D in period t + 1) its timing is uniformly
distributed from t to t + 1; in this case, we halve the immediate reward associated with state
St . The terminal reward (RE ) is the cumulative remaining lifetime QALYs for a patient who
passes the terminal age (YN ).
We instantiated the model for the case of HIV-infected women in the United States, aged 20 to
90 years old. We grouped CD4 count levels into 7 bins: 0-50, 50-100, 100-200, 200-300, 300-400,
400-500, >500 cells/mm3 . Lower CD4 counts indicate greater disease progression, with CD4 counts
at the lowest levels typically corresponding to full-blown AIDS. Each time period represents half
a year. Every six months a patient can choose to start ART immediately or delay for another six
months. To obtain the cumulative QALYs after the terminal age (RE ), we performed a cohort
simulation that utilized the same model parameters including transition probabilities and utilities
(quality-of-life multipliers). Values for all model parameters are provided in Appendix A.4.
Results
We considered QMDP models with three di↵erent quantile thresholds, ⌧ = 0.2, 0.5, and 0.8. As ⌧
increases, the patient becomes less risk-averse. Figure 2.10 shows the optimal actions as a function
of age and CD4 count. Similar to the findings from the MDP model [96], we find that patients
who are older or who have high CD4 counts tend to delay ART. In both cases, the reduction in
HIV-associated morbidity and mortality from initiating ART is outweighed by the induced cardiac

CHAPTER 2. QUANTILE MARKOV DECISION PROCESSES

34

risks. For older patients, the induced cardiac risks are substantial because of the higher baseline
cardiac risks at older ages. Patients with high CD4 counts are relatively healthy so the benefits
from starting ART are less than the induced cardiac risks.
In contrast to an MDP analysis, which maximizes expected cumulative reward and does not
consider risk preferences, Figure 2.10 shows that di↵erent risk attitudes of patients will lead to
di↵erent treatment preferences. Patients who are less risk-averse (i.e., patients with higher levels of
⌧ ) will tend to start ART sooner than patients who are more risk-averse. For example, a risk-averse
60-year-old woman with a CD4 count of 200 will choose to delay ART initiation, whereas a less
risk-averse woman with the same CD4 count would choose to start ART. This is because patients
with lower levels of risk aversion are more willing to accept elevated cardiac risks in order to gain
the immunological benefits of ART. By incorporating the patient’s risk attitude, QMDP allows for
a patient-centered care plan.
We can use the QMDP model to illuminate the inherent risk associated with MDP. We simulated
50,000 di↵erent reward trajectories based on the optimal policy obtained from MDP for a 20-year
old patient with CD4 level of 300-350 cells/mm3 . We then calculated the CDF for cumulative
rewards from the simulated trajectories and best achievable reward for each quantile from the
QMDP model. As shown in Figure 2.11, there is a greater opportunity for improvement at higher
quantiles, which suggests that one should consider using QMDP when patients are less risk-averse.
We note that instabilities exist in the computed QMDP policies for this example, especially
within regions where an action switch is made. This phenomenon is similar to a non-monotonic
policy achieved in an MDP or robust MDP where some of the sufficient conditions for a monotonic
policy are violated [144]. The instability may be caused by the structure of the rewards (immediate
and terminal) and/or the transition probabilities of the underlying simulation model. Further
research is needed to determine sufficient conditions for a monotonic optimal policy for QMDP.
In Appendix A.5, we solve the HIV treatment initiation problem using QBDP and compare the
results to QMDP.

2.7

Discussion

We have presented a novel quantile framework for Markov decision processes in which the objective
is to maximize the quantiles of the cumulative reward. We established several theoretical results
regarding quantiles of random variables which contribute to an efficient algorithm for solving QMDP.
The examples we presented show how solving QMDP with di↵erent values of ⌧ generates solutions
consistent with di↵erent levels of risk aversion.
The QMDP model can be applied to a variety of problems in areas such as healthcare, finance,

CHAPTER 2. QUANTILE MARKOV DECISION PROCESSES

35

Figure 2.10: HIV treatment example: Optimal actions for QMDP with ⌧ = 0.20, 0.50, and 0.80.

Figure 2.11: HIV treatment example: Optimal QMDP reward and cumulative density function of
MDP reward.
and service management where decision robustness and risk awareness play a key role. In this
chapter, we have restricted our attention to obtaining an exact solution for the QMDP problem.
The complexity of our algorithm is O (AST · max(RT, S)), where T is the length of the time horizon,

and A = |A| and S = |S| are the sizes of the action and state space, respectively. A promising area
for future research is to determine how the QMDP model can be applied to very large scale real-

world problems. In other risk-sensitive MDP settings, approximate dynamic programming (ADP)
methods (e.g., [74]) have been used to address the issue of exploding state space and inefficient
sampling in large-scale problems. Further research could investigate how to incorporate ADP
methods into the QMDP model.
In addition to its value in determining the optimal decisions associated with di↵erent levels
of risk aversion, QMDP provides a useful adjunct to MDP. Comparing the QMDP solution for

CHAPTER 2. QUANTILE MARKOV DECISION PROCESSES

36

di↵erent values of ⌧ to the CDF of the MDP reward reveals the improvement that could be gained
over the MDP solution if a quantile criterion were used. Depending on the decision maker’s risk
attitude, one might want to instead use QMDP or another risk-sensitive MDP model.

Chapter 3

Health Outcomes and
Cost-E↵ectiveness of Treating
Depression in People With HIV in
Sub-Saharan Africa
3.1

Introduction

Despite considerable e↵orts devoted to improving antiretroviral therapy (ART) coverage and viral
suppression in Sub-Saharan Africa (SSA) [128], HIV care falls short of UNAIDS 90-90-90 goals in
most SSA countries [103]. Two key targets for improving HIV care are reducing loss from care
and increasing adherence to ART [61, 103, 88]. Developing e↵ective and cost-e↵ective approaches
to attain these goals is central to many HIV control programs. In this chapter we assess the
outcomes and cost-e↵ectiveness of screening for, and treating, depression among people living with
HIV (PLHIV) to improve population health.
Increasing evidence points to a high prevalence of depression among PLHIV. A meta-analysis
found that 9-32% of PLHIV in SSA su↵er from depression [20]. Compared to uninfected individuals, PLHIV have nearly twice the risk of depression [5, 38, 124]. Depression jeopardizes
ART e↵ectiveness and viral suppression by decreasing adherence, and reducing patient engagement
[12, 27, 33, 60, 67, 134]. Treatment of depression has been found to improve ART adherence and
clinic attendance [123, 133]. Treatment approaches include pharmacological therapy and group and

37

CHAPTER 3. TREATING DEPRESSION IN PEOPLE WITH HIV

38

individual psychotherapy [66, 95].
We focus on a pharmacological intervention, fluoxetine, that is inexpensive and has good evidence of e↵ectiveness in reducing depressive symptoms [39]. Using data for Uganda, we assess the
impact on HIV care cascade endpoints, clinical indicators, and cost-e↵ectiveness of using fluoxetine
to treat depression among PLHIV.

3.2

Methods

We used a microsimulation model [17] that captures HIV transmission, HIV disease course, depression, and the benefits from treatment of both HIV and depression. We consider a strategy
of screening for depression and providing antidepressant therapy (ADT) with fluoxetine at ART
initiation. Key data and sources are shown in Tables 3.1 - 3.6.
Our model builds on a previous microsimulation model of HIV transmission and progression
among individuals in sub-Saharan Africa [17]. Our HIV disease progression model is similar but
is calibrated to data from Uganda from 2005-2015. The model population consists of individuals
aged 15 or older, and proceeds in monthly time steps. Here we describe the modifications we
made to the model. These relate to patient depression status and its relationship to adherence
with antiretroviral therapy (ART) and various retention parameters (i.e., pre-ART leakage, loss
to follow-up (LTFU), and first-line ART failure). We also modified the model to account for the
e↵ects of antidepressant therapy on patient depression status. The model was programmed in
Matlab 2015b (The MathWorks, Inc., Natick, Massachusetts).
Each individual is characterized by demographic features (age and gender), HIV-related features (HIV status, CD4 cell counts, viral load, and opportunistic infections), treatment features
(treatment regimen and adherence), depression status (with or without depression), and depression
treatment status. We track HIV care, including pre-ART loss from care (leakage), post-ART loss
to follow-up (LTFU), and ART failure. Uninfected individuals can become infected through sexual
contact with infected individuals.
Several studies have estimated the prevalence of clinical depression among HIV-infected and
uninfected individuals in Uganda and nearby regions [26, 111]. To estimate the chance of depression
for each type of individual in the modeled population, we used odds ratios found in the literature
[77, 81, 112] to estimate depression prevalence as a function of sex and CD4 count (if HIV infected).
Using the prevalence of ART adherence among the entire population [93] and the odds ratio between
clinical depression and ART adherence [27], we estimated the chance that a patient is adherent to
ART based on the patient’s depression status (with or without clinical depression). In the model,

CHAPTER 3. TREATING DEPRESSION IN PEOPLE WITH HIV

39

Table 3.1: Data and Sources: Depression

Parameter

Value

Range

Type of
Probability
Distribution
Used
in PSA

Sources

Depression
HIV Negative
17.0
(12.0, 23.0)
Beta
[111]
HIV positive
28.0
(23.0, 33.0)
Beta
[26]
Odds ratios for depression
HIV positive, status known
1.99
(1.32, 3.00)
Lognormal
[4, 38]
CD4 <50
2.34
(1.39, 3.93)
Lognormal
[77]
Female
1.85
(1.24, 2.44)
Lognormal
[77, 81]
Adherence prevalence (%)
80.0
(72.0, 88.0)
Beta
[93]
Odds ratios for HIV-infected individuals
with depression
Adherence
0.32
(0.11, 0.93)
Lognormal
[27]
No leakage
0.32
(0.11, 0.93)
Lognormal
Assumed
No LTFU
0.32
(0.11, 0.93)
Lognormal
Assumed
Odds ratio for viral failure due to
9.90
(3.20, 45.10)
Lognormal
[101, 136]
non-adherence
PHQ-9 diagnostic test
Sensitivity
0.92
(0.83, 1.00)
Beta
[3]
Specificity
0.81
(0.73, 0.89)
Beta
[3]
Treatment e↵ects
Probability of remission at month
0.70
(0.32, 0.79)
Beta
[99, 112]
6 of initial therapy
Probability of remission at month
0.40
(0.36, 0.50)
Beta
[64]
12 of continuation therapy
ART: antiretroviral therapy; LTFU: loss to followup; PHQ-9: Patient Health Questionnaire- 9;
PSA: probabilistic sensitivity analysis.

CHAPTER 3. TREATING DEPRESSION IN PEOPLE WITH HIV

40

Table 3.2: Data and Sources: Utilities and Costs

Parameter

Value

Range

Type of
Probability
Distribution
Used
in PSA

Sources

Utilities
Uninfected
1.00
–
Asymptomatic HIV
0.94
(0.76, 1.00)
Beta
Symptomatic HIV
0.81
(0.68, 0.95)
Beta
AIDS
0.70
(0.57, 0.86)
Beta
ART multiplier
1.15
(1.00, 1.35)
Lognormal
Depression multiplier
0.58
(0.48, 0.68)
Beta
Remission with antidepressant
0.78
(0.72, 0.83)
Beta
Costs ($)
During simulated time horizon
Inpatient day
15.63
(4.43, 47.53)
Gamma
Outpatient visit
3.30
(1.13, 9.27)
Gamma
HIV test
1.13
(0.57, 6.23)
Gamma
CD4 test
8.76
(5.43, 19.00)
Gamma
Viral load test
34.34
(25.41, 48.69)
Gamma
Antidepressant Administration
0.66
(0.33, 1.32)
Gamma
Monthly 1st line ART
10.92
(8.19, 14.38)
Gamma
Monthly 2nd line ART
39.03
(35.51, 43.00)
Gamma
Monthly fluoxetine cost
1.16
(0.48, 2.52)
Gamma
Monthly background healthcare
10.09
(7.57, 12.61)
Gamma
expenditure
End-of-simulation calculation
Yearly costs for HIV-infected
219.00 (164.25, 273.75)
Gamma
individuals on ART
Cost multiplier for HIV-infected
1.5
(1.25, 2.00)
Gamma
individuals not on ART
Discount factor
3%
–
ART: antiretroviral therapy; PSA: probabilistic sensitivity analysis.

[119]
[85, 129]
[85, 129]
[85, 129]
[85, 6, 76]
[32, 116]
[52]

[71, 55, 105]
[71, 55, 105]
[71, 94]
[71, 72, 78, 89]
[71, 72, 78]
Assumed
[71, 54]
[71, 54]
[62]
[14]

[126]
Assumed
[119]

CHAPTER 3. TREATING DEPRESSION IN PEOPLE WITH HIV

Table 3.3: Data and Sources for HIV Dynamics and Natural History: Demographics
Parameter
Age distribution
Male

Value

15-19
20-24
25-29
30-34
35-39
40-44
45-49
50-54
55-59
60-64
65+

22.0%
18.2%
14.8%
11.6%
8.6%
6.0%
6.0%
4.8%
3.2%
2.6%
4.6%

15-19
20-24
25-29
30-34
35-39
40-44
45-49
50-54
55-59
60-64
65+
Male circumcision probability
Life expectancy (years)
Male
Female
Partnership distribution
2005-2010
0 partners
1 partners
2 partners
3 partners
2011 onwards
0 partners
1 partners
2 partners
3 partners

21.4%
17.6%
14.3%
11.0%
8.0%
5.9%
5.1%
4.5%
3.7%
2.9%
5.4%
0.28

Sources
[46]

Female

[44]
[2]

53.54
56.36
[44]
28.2%
50.8%
18.0%
3.0%
13.6%
79.4%
6.5%
1.5%

41

CHAPTER 3. TREATING DEPRESSION IN PEOPLE WITH HIV

42

Table 3.4: Data and Sources for HIV Dynamics and Natural History: HIV Transmission Parameters
Parameters
Age-specific HIV prevalence (%)
Male
15-19
20-24
25-29
30-34
35-39
40-44
45-49
50-54
55-59
60-64
65 +
Female
15-19
20-24
25-29
30-34
35-39
40-44
45-49
50-54
55-59
60-64
65 +
Risk of infection per sex act,
by log viral load
<2.7
2.7-3.48
3.49-4
4.01-4.48
4.48
Late stage AIDS
Acute infection
Reduction in HIV risk for
circumcised men

Value

Source
[44]

0.30
2.40
5.90
8.10
9.20
9.30
6.90
6.90
5.80
5.80
5.80
2.60
6.30
8.70
12.10
9.90
8.40
8.20
5.40
4.90
5.00
5.00
[115, 135]
0.0001
0.0011
0.0012
0.0014
0.0023
0.0043
0.0082
0.5

[108]

CHAPTER 3. TREATING DEPRESSION IN PEOPLE WITH HIV

43

Table 3.5: Data and Sources for HIV Dynamics and Natural History: HIV Disease Parameters
Parameters
Initializing HIV parameters
Log viral load initial mean
for initial infection
Log viral load initial standard deviation
for initial infection
Log viral load for acute case
Log viral load for treatment
E↵ective months of VL decline due to ART
Log viral load drop per month
Min log viral load level
Max CD4 for chronic HIV
CD4 for uninfected
Monthly decrease in CD4 by VL
0-2.5
2.6-3.6
3.7-4.4
4.5 +
Decrease in CD4 during acute event
Monthly 1st line ART failure probability
Monthly mortality for the HIV infected
on ART by CD4 count
0-49
50-99
100-149
150-249
250 +
Monthly mortality for the HIV infected
not on ART by CD4 count
0-49
50-99
100-199
200-299
300-399
400-499
500 +
CD4 threshold for immediate ART
initiation
2005- 2009
2010-2014
2014 onwards
Monthly opportunistic infections
by CD4 count
0-49
50-99
100-199
200-299
300-399
400-499
500 +

Value

Source

4.02

[115]

0.85

[115]

6.00
2.60
6.00
1.70
3.00
650
830

[115]
[115]
[115]
[115]
[115]
[115]
[115]
[115]

0
4.4
5.5
6.6
58.5
0.0018

[115]
[110]
[92]

0.00559
0.00306
0.00228
0.00168
0.00159
[11]
0.0430
0.0198
0.0156
0.0121
0.0098
0.0083
0.0052
[107, 50]
200
350
500
[65]
0.0238
0.0119
0.0088
0.0059
0.0025
0.0006
0

CHAPTER 3. TREATING DEPRESSION IN PEOPLE WITH HIV

44

Table 3.6: Data and Sources for HIV Dynamics and Natural History: HIV Retention Parameters
Parameters
Monthly probability of pre-ART leakage
Monthly probability of loss to followup during ART by duration category
0-12 months
13-24 months
25-36 months
Probability of returning from an OI event in a given month

Value
0.0050

Source
Assumed
[18]

0.0033
0.0027
0.0012
0.5000

the depression status of an HIV-positive diagnosed individual can change without antidepressant
therapy only when the patients’ CD4 count falls below 50/µ L. Individuals can switch from being
ART adherent to not adherent when their depression status changes.
The base case assumes individuals with depression su↵er from depression until receiving ADT.
Depression prevalence is stratified by gender, HIV infection status, and CD4 count. Individuals
without depression may become depressed following either an HIV-positive test result or a decline
in CD4 cell count below 50 cells/µ L (Table 3.1). Depression lowers ART adherence rates, increases
leakage, and increases LTFU (Table 3.1). ART adherence changes when depression status changes
(Table 3.1).
Based on the published literature we estimated the monthly probability of pre-ART leakage,
LTFU, and first-line ART failure [18, 110] for each individual in the model. Using the odds ratio
between ART nonadherence and failure of first-line ART [101, 136], we estimated the chance of firstline ART failure as a function of whether a patient is adherent with ART. A study of depression and
ART adherence in Uganda [27] estimated a 0.32 odds ratio for ART adherence among patients with
clinical depression. We were unable to find estimates of the odds ratios for pre-ART leakage and
LTFU among such patients, so we assumed the same value of 0.32 for the odds ratio. In sensitivity
analysis we ranged these odds ratios widely (0.11, 0.93).
We assume a depression screening tool with characteristics similar to the Patient Health Questionnaire-9 (PHQ-9) [3, 134], a simple diagnostic questionnaire with estimated 91.6% sensitivity
and 81.3% specificity for depression in Uganda [3, 82]. We model ADT based on the cost and
e↵ectiveness of fluoxetine at a dosage of 40 mg/day. Our treatment algorithm (Figure 3.1) is based
on the clinical experience of long-term ADT that provide 6-month, 12-month, and long-term relapse
and remission rates with 42% overall long-term remission (Table 3.1).
We assumed that fluoxetine (generic Prozac) is given to everyone who is screened positive
for depression for 6 months (acute phase treatment) with a dosage of 40 mg/day (Figure 3.1).
Individuals who screen false positive on the PHQ-9 will receive 6 months of antidepressant therapy
before discontinuing. After 6 months of fluoxetine, we used an estimated remission rate of 70% of

CHAPTER 3. TREATING DEPRESSION IN PEOPLE WITH HIV

45

Figure 3.1: Schematic representation of HIV natural history model and antidepressant treatment
scheme
individuals who were depressed and started fluoxetine [99]. Individuals not in remission are not
given further antidepressant therapy since we assume them to be refractory to fluoxetine. Such
individuals may be switched to other antidepressants or other forms of interventions such as group
psychotherapy. Due to the lack of literature of the efficacy of such inventions among individuals who
are resistant to fluoxetine, we did not model the additional interventions. Individuals in remission
at the end of the acute phase continue the same treatment for another 12 months (continuation
therapy). Those remaining in remission at the end of 12 months of consolidation therapy are
given a lifetime of antidepressants, and we assume they remain in remission. Otherwise, patients
are provided five years of antidepressants. We assumed a relapse rate of 40% after 12 months of
continuation therapy [64]. In the general population, our algorithm yields a 42% chance of remission
with fluoxetine at the end of 10 years. For non-depressed individuals who receive treatment (false
positives from the PHQ-9) we include treatment costs but assume that the antidepressant does not
change health outcomes.
We used the model to assess costs and health outcomes measured in quality-adjusted life years
(QALYs), discounted at 3%, and cost-e↵ectiveness of providing ADT. We took a healthcare system
perspective and include all relevant costs and health e↵ects [119]. Outcomes measured include

CHAPTER 3. TREATING DEPRESSION IN PEOPLE WITH HIV

46

prevalence of depression, annual rates of leakage and LTFU, percent of patients who are adherent
to ART, percent of patients with virologic suppression, costs (2017 USD), life years, and QALYs.
We measured the costs of antidepressant medications, CD4 tests, viral load tests, ART (first-line
and second-line), HIV tests, HIV-related inpatient and outpatient care, and non-HIV healthcare
for each individual (Table 3.2). We stratified QALY weights based on HIV infection status and
disease. Quality-of-life values were not available for the specific population we considered (HIVinfected individuals in Uganda, with and without depression), so we used values from a di↵erent
population. We also measured life years experienced. In each HIV disease stage, the quality of
life for patients with untreated depression is 42% lower than those without depression, while for
patients on ART, quality of life is 15% higher than for comparable patients not on ART [32, 116].
We discounted costs and QALYs at 3% [119].
We performed one-way sensitivity analyses to examine critical assumptions, including parameters related to costs, fluoxetine e↵ectiveness, and change in quality of life from improved depression
status.
We performed probabilistic sensitivity analysis varying all model parameters (including all parameters that determine ART adherence, pre-ART leakage, LTFU, first-line ART failure, fluoxetine
costs and e↵ectiveness, and ART costs) (distributions in Tables 3.1 and 3.2). We estimated 95%
uncertainty bounds and the likelihood of cost-e↵ectiveness at varying willingness-to-pay thresholds.
We also performed a structural sensitivity analysis: we re-ran our analyses using a model of
depression that additionally includes natural remission from depression and a natural rate of developing depression.
We calibrated our model to UNAIDS estimates of HIV prevalence in Uganda from 2005-2015
among adults (15 years old) [102], age-specific HIV prevalence in 2011 from a nationally representative serosurvey [43], ART coverage from 2010-2015 [102], and population growth from 2005-2015
[13]. Our model estimates closely match the available data (Figures 3.2 - 3.5).

3.3

Results

3.3.1

Base Case Analysis

The ADT intervention yields an additional 0.02 QALYs per person compared to the status quo
(Table 3.7). The benefits of providing ADT accrue mainly from relief of depression leading to higher
quality of life, improvements in retention and adherence, and reduction in new HIV infections.
Compared to the status quo, providing ADT reduces depression prevalence among HIV-infected
individuals by 16.0% [95% uncertainty bounds 15.8%, 16.1%] from a baseline prevalence of 28%
(Table 3.8). All changes are percent change relative to the status quo.

CHAPTER 3. TREATING DEPRESSION IN PEOPLE WITH HIV

47

Figure 3.2: Calibration result: model-projected HIV prevalence compared with UNAIDS estimates
from 2005 to 2015

Figure 3.3: Calibration result: model-projected 2011 HIV prevalence in each age group compared
with Demographic Health Survey (DHS) data from 2011

CHAPTER 3. TREATING DEPRESSION IN PEOPLE WITH HIV

48

Figure 3.4: Calibration result: model-projected ART coverage among HIV-infected individuals
compared with UNAIDS estimates from 2005 to 2015

Figure 3.5: Calibration result: model-projected population compared with World Bank estimated
population for 2005 to 2015

CHAPTER 3. TREATING DEPRESSION IN PEOPLE WITH HIV

49

Relief of depression improves several HIV care outcomes (Table 3.8). Rates of LTFU after ART
initiation are reduced by 3.7% [3.4%, 4.1%]. ART adherence increases by a modest 1.0% [1.0%,
1.0%] and failure of first-line ART is reduced by 2.5% [2.3%, 2.8%]. Compared to the status quo,
an additional 1.0% [1.0%, 1.0%] of the HIV-infected population is virologically suppressed.
The intervention costs $15/QALY gained compared to the status quo and thus would be considered very cost-e↵ective by any criteria [84, 106]. Extrapolated to the population level, we estimate
that the intervention would cost $12.13 million and gain 790,000 QALYs over the analytic time
horizon (Table 3.7).

3.3.2

Sensitivity Analyses

At $2.52, the highest reported price (for Botswana) [105], the ICER is $22/QALY. At the lowest
reported e↵ectiveness (32% in remission at 6 months), depression prevalence is reduced by 7.3%
[7.2%, 7.4%] and the ICER is $42/QALY. If the long-term relapse rate is 50% (vs. 42% in the base
case), the ICER is $18/QALY. If ADT uptake is only 50%, the ICER is $36/QALY.
The improvement in quality of life from ADT contributes significantly to the health benefits.
If there is no quality-of-life improvement from treating individuals with depression, the ICER is
$292/QALY. If we remove the quality-of-life benefits from treating depression and increase antidepressant cost to the upper bound of $2.52, the ICER is $419/QALY.
We performed one-way sensitivity analyses on the parameters characterizing the relationship
between depression, ART adherence, and viral failure. In the base case, the OR for adherence among
those depressed is 0.32, and the OR for viral failure is 9.9 with ART non-adherence. Changing the
ORs to their bounds, 0.93 and 3.2, respectively, increases the ICER to $29/QALY.
In probabilistic sensitivity analysis, the ADT intervention was very cost-e↵ective for all model
parameter realizations when using the Uganda GDP per capita ($631) [14] as the willingness-to-pay
reference value (Figure 3.6).
In structural sensitivity analysis, using a model that incorporates natural development of and
remission from depression, the ICER was $28/QALY.

3.3.3

Supplemental Results

Although costs increase over the 10-year analytic horizon because of the additional costs of the
antidepressant (Figure 3.7), costs incurred by PLHIV not on ART beyond the end of the horizon
are reduced by $4.0 million (Figure 3.8). The savings result from the lower number of new infections
and the lower average annual costs of care for PLHIV on e↵ective ART compared with those not
on ART.

CHAPTER 3. TREATING DEPRESSION IN PEOPLE WITH HIV

50

Figure 3.6: Cost-e↵ectiveness acceptability curve
The improvement in adherence is relatively small because overall adherence rates are high at
baseline [93]. Our estimate of 80% adherence to ART in the entire HIV-infected population at
baseline, together with the 0.32 odds ratio of non-adherence among depressed individuals and
prevalence of depression, implies that ART adherence among non-depressed individuals is 83.9%.
Thus, the ceiling for the e↵ect of antidepressants on population adherence is modest, and our 1%
increase in population adherence may be modest at the population level but is substantial compared
to what is achievable.
We varied monthly fluoxetine cost from $0.48 to $2.52, based on WHO reported prices [62].
At $2.52 (reported for Botswana), the ICER is $22/QALY. The ICER exceeds Uganda’s GDP per
capita ($631) [14] only when the monthly fluoxetine cost rises to $239.16.
The base case assumed that 70% of depressed patients will be in remission after 6 months of
fluoxetine treatment. However, fluoxetine e↵ectiveness is highly variable. At the lowest reported
e↵ectiveness (32% in remission at 6 months), depression prevalence is reduced by 7.3% [7.2%, 7.4%]
and the ICER is $42/QALY. If the relapse rate is 50% after 12 months of continuation therapy

CHAPTER 3. TREATING DEPRESSION IN PEOPLE WITH HIV

51

Figure 3.7: Estimates of the incremental cost ($ millions) of the ADT Provision strategy compared
to the status quo for the 10-year modeled time horizon for the entire Uganda population

Figure 3.8: Estimates of the future incremental cost ($ millions) of the ADT Provision strategy
compared to the status quo at the end of the 10-year modeled time horizon for the entire Uganda
population

CHAPTER 3. TREATING DEPRESSION IN PEOPLE WITH HIV

52

beyond the initial 6 months of acute therapy (vs. 42% in the base case), the ICER is $18/QALY.
If ADT uptake is only 50% (the base case assumed 100% uptake among patients because we treat
ADT as an adjunct to current ART) the ICER is $36/QALY.
The improvement in quality of life from ADT contributes significantly to the health benefits.
If depression does not reduce the quality of life, and thus there is no quality-of-life improvement
from treating individuals with depression, the ICER is $292/QALY. This is because of indirect
benefits from the improved HIV care cascade. If we remove the quality-of-life benefits from treating
depression and increase antidepressant cost to the upper bound of $2.52, the ICER is $419/QALY.
If we remove the quality-of-life benefits from treating depression and lower ADT uptake to 50%,
the ICER is $946/QALY; if instead ADT improves quality of life by 1%, the ICER is $583/QALY.
We performed one-way sensitivity analyses on the parameters that characterize the relationship between depression, ART adherence, and viral failure of first-line treatment. In the base
case, the adherence OR for depression is 0.32 and the OR for viral failure is 9.9 with ART nonadherence. When we changed the ORs to the upper bound of 0.93 or the lower bound of 3.2,
respectively, the ICER was $21/QALY. When we changed both parameters simultaneously, the
ICER was $29/QALY.
We performed a structural sensitivity analysis using a model that incorporates natural remission
from depression and a natural rate of becoming depressed. We set the baseline monthly natural
remission rate at 5% [137]. To achieve a stable depression prevalence in the population without any
intervention, we set the monthly rate of becoming depressed to 1.94% for non-depressed PLHIV and
to 1.02% for non-depressed HIV-uninfected individuals. For this analysis the ICER is $28/QALY.

3.4

Discussion

Our analysis indicates that screening PLHIV for depression and providing fluoxetine or a similar
antidepressant is an e↵ective and highly cost-e↵ective strategy for improving HIV-related outcomes
in Uganda, and likely throughout other regions of SSA with a generalized HIV epidemic. Beyond
reducing depressive symptoms among PLHIV, ADT improves adherence and reduces virologic failure rates, thereby improving suppression rates and reducing HIV transmission. Such a strategy is
likely to be highly cost-e↵ective, especially where the prices of generic antidepressant medications
are low. At an estimated cost of $1.16 per month and an ICER of $15/QALY gained, provision
of ADT to PLHIV with depression falls well below commonly accepted thresholds, and is more
cost-e↵ective than interventions such as home care treatment [84].
Treatment of depression in PLHIV in SSA has been limited to date. International funding
agencies frequently focus on physical illnesses (such as HIV) rather than mental illness. Country

CHAPTER 3. TREATING DEPRESSION IN PEOPLE WITH HIV

53

ministries also provide few funds for mental health services. In 2010, Uganda devoted only 1% of
public health expenditures to mental health care [80]. Additionally, Uganda and other countries
in SSA have very few trained mental health providers [80]. Providing ADT can be accomplished
as part of routine HIV care and thus would be practical to implement in settings with few trained
mental health professionals.
Our analysis has several limitations. First, our model is instantiated with data from Uganda
and therefore is most applicable to countries with a similar HIV context. Second, there is relatively
little data on the natural course of depression in PLHIV and in lower-income countries [125]. We
made simplifying assumptions that match the existing evidence available from similar populations
and interventions. While additional granularity about depression could be added, such detail is
unlikely to impact our population-level outcomes. Third, we assumed that PLHIV on ART receive
no depression treatment under the status quo. To the extent that PLHIV already receive depression treatment, our analysis may overstate the intervention benefits. Fourth, we did not consider
side e↵ects of fluoxetine such as sexual dysfunction [51]. Our analysis may thus overestimate the
health benefits of ADT. However, given the low fluoxetine cost and the significant improvement
in QALYs among treated individuals, our conclusions regarding cost-e↵ectiveness are unlikely to
change. Fifth, our analysis assumes that ADT will improve ART adherence. Recent evidence
suggests that adding another pill to ART treatment could reduce adherence [68]. To the extent
that this occurs, our analysis may overestimate the health benefits of ADT. Lastly, we only examined the cost-e↵ectiveness of a pharmacological intervention for depression treatment. A promising
area for depression treatment is group or interpersonal therapy, which may impact the appeal of
pharmacological ADT [31, 68].
We conclude that ADT for treatment of depression among PLHIV in Uganda and similar countries in SSA is likely both highly e↵ective and cost-e↵ective when judged by common criteria
[84, 106]. Use of available HIV funding to help alleviate depression in PLHIV would be strongly
leveraged by the direct and indirect benefits from resulting improvements in the HIV care cascade,
will generate both individual and population benefits, and compares favorably to other investments
in HIV and other diseases.

831.34 M
831.38 M

Status Quo
ADT Provision
0.04 M

0.001

Incremental
LYs

Incremental
Costs ($)
QALYs
Individual Level
16.41
–
2153.28
16.43
0.02
2153.52
Population Level
805.67 M
–
105,687.20 M
806.45 M
0.79 M
105,699.36 M
QALYs

–
12.13 M

–
0.25

Incremental
Costs ($)

–
15.43

–
15.43

ICER
($/QALY gained)

ADT Provision

Strategy

Reduction in
Depression
Prevalence
15.981
(15.841, 16.111)

Reduction in
Pre-ART
Leakage
-0.510
(-0.756, 0.349)

3.722
(3.352, 4.092)

Reduction in
LTFU

Increase in
Adherence
to ART
1.014
(1.014, 1.015)

Reduction in
First Line
ART Failure
2.526
(2.276, 2.775)

Increase in
Virologic
Suppression
1.005
(1.004, 1.006)

Table 3.8: Base Case Results: Relative Percentage Changes (vs. Status Quo) in Care Cascade Parameters and 95% Uncertainty
Bounds

16.93
16.94

LYs

Status Quo
ADT Provision

Strategy

Table 3.7: Base Case Results: Costs, Life Years, Quality-Adjusted Life Years, and Incremental Cost-E↵ectiveness Ratio

CHAPTER 3. TREATING DEPRESSION IN PEOPLE WITH HIV
54

Chapter 4

Metamodeling for Policy
Simulations with Multivariate
Outcomes
4.1

Introduction

Simulation modeling serves an important role in informing disease-related policy, including performing cost-e↵ectiveness analyses (CEA). Increasingly complex simulation-based CEA models (in
terms of both structure and methodology) have been developed given a rise in computational power
and the increasing volume of available data. A consequence of model complexity is that generating simulation model outputs is often a time consuming process. For example, many runs may
be needed to reduce the Monte Carlo uncertainty from a complex stochastic model or to perform
required sensitivity analyses for a model with a large number of parameters. At the same time,
stakeholders need model-based tools to support decision-making at the local level, which might not
be adequately addressed in policy-related peer-reviewed articles given practical limitations associated with this format. Local contextualization of modeling studies may therefore be facilitated
by web-based applications that accommodate more flexible real-time model adaptations. Two key
factors for the successful deployment of such a platform are user-friendly interface and speed. In
addition, model interpretability is also important, especially for models used in public health, since
stakeholders are not comfortable with ‘black-box’ predictions.
The Second Panel on Cost-e↵ectiveness in Health and Medicine has stressed the importance
of efficient model emulators, or metamodels, as a means of reducing computational cost [98]. A
55

CHAPTER 4. METAMODELING FOR POLICY SIMULATIONS

56

metamodel is a statistical approximation of an originally constructed complex model. In addition to directly replacing a model to predict outputs, metamodels can also be used to improve
interpretability [73] and aid in the calibration process [143].
Metamodeling has a long history as a means of finding a surrogate of an original model [24]
and has been applied in a range of fields including public health, energy and the environment,
and queueing systems [37, 79, 104]. To build a good approximation of an original model, various
methods such as linear and polynomial regression, random forest models, and neural network models
have been used [104, 75, 132]. However, existing methods typically do not consider the particular
challenges that arise when multiple correlated output variables are relevant to a particular policy
analysis.
In this chapter, we focus on metamodeling for policy-related simulation models. We develop a
framework for metamodeling of simulation models that accommodates multivariate outcomes, and
we compare alternatives for the choice of base learning algorithm. As an example, we apply our
framework to develop a metamodel of an individual-based microsimulation model of alternative
testing and treatment strategies for hepatitis C virus (HCV) in correctional settings. We show how
metamodeling can provide accurate predictions of outputs and improve computational efficiency
and model interpretability.

4.2

Methods

In this section, we introduce a general framework for constructing metamodels, present details of
our method for handling multioutput prediction, describe di↵erent base models that can be used
to emulate the original model, and describe our model-independent variable selection scheme. We
also present evaluation metrics for the constructed metamodels.

4.2.1

Metamodeling Framework

A first step in metamodeling is to generate appropriate data for constructing the metamodels. We
first obtain n input variable sets. If all input variables are independent, random samples can be
drawn from each input variable; otherwise, the joint distribution of di↵erent input variables must
be considered when selecting input variable sets. We assume that distributions for input variable
values are available. Many modelers use a Bayesian approach to obtain the best input variable set
via model calibration, a process that yields a joint distribution of values for all parameters that
reflect their correlation structure [58]. The necessary number of input variable sets N depends
on the complexity of the model as reflected by factors such as the number of input variables, the
problem domain, and the quality of data samples. For each input variable set, we determine the

CHAPTER 4. METAMODELING FOR POLICY SIMULATIONS

57

associated outputs generated by the simulation model. Via metamodeling we then aim to build a
replacement model that links the input variable sets to the simulated outputs.
Depending on the data, preprocessing may be necessary before a model can be built. If the data
include categorical variables, they must be converted, either to multiple one-hot vectors (these are
vectors where all elements except one are set to zero) or to an ordinal variable depending on the
relationships between di↵erent categories. Such transformation is necessary since many machine
learning models do not inherently handle categorical variables.
Another important preprocessing step is standardization of input and output variables. This is
(j)

done using the formula zi

(j)

=

xi

si

xi

where xji is the j -th xi value in the array of n input variables,

and xi and si are the sample mean and standard deviation, respectively. There are many reasons for
performing such scaling, including improving the training process of models such as neural network
models via stochastic gradient descent and improving model stability [23]. Another important
reason for scaling is due to the multioutput structure. Standardization also allows combination
of loss functions that may have di↵erent scale. A disadvantage of standardization is that it can
complicate interpretation of the relationships between input and output variables.
The final step before building the metamodel is to randomly split the data into training and
testing data sets. The training phase is divided into two steps: a performance improvement phase
with hyperparameter tuning and variable selection, and actual training with selected variables and
specified model hyperparameters. We present details of the variable selection scheme below. For
the actual training phase, 10-fold cross-validation is utilized. Under this approach the training
data set is further split into 10 folds and, one at a time, each of the 10 folds is withheld while the
other 9 folds are used to build the model. Model performance is then evaluated with respect to the
withheld data in order to identify the best-performing model based on defined evaluation criteria.
Multi-output Regression
Most simulation models in public health generate multiple outputs of interest. Because metamodeling aims to build a replacement model to link the original simulation inputs and outputs, the
problem could be parameterized in a multioutput regression. There are two main approaches for
multioutput regression: problem transformation methods and algorithm adaptation methods. In
problem transformation methods, multioutput regression is transformed to separate single-output
predictions. Algorithm adaptation methods can be readily integrated with any single-output prediction model to predict all the outputs simultaneously. Algorithm adaptation methods such as
multi-stacking regression (MTS) and regression chains (RC) have been shown theoretically to perform better than problem transformation methods [25]. There are several regression chain variants.
Use of a regression chain with maximum correlation (RCMC), has been shown to outperform other

CHAPTER 4. METAMODELING FOR POLICY SIMULATIONS

58

methods over a range of di↵erent data sets in terms of accuracy and to require less computational
power than other methods such as ensemble regression chains [90]. We use RCMC and MTS in our
study.
We first describe multioutput regression and provide notation that will be used subsequently.
X denotes the input matrix which consists of d input variables (X1 , X2 , · · · , Xd ). Y denotes the
output matrix which consists of m output variables (Y1 , Y2 , · · · , Ym ). We have a training set of D

= { ( x1 , y1 ), . . . , (xn , yn )}, and our target is to learn a model H : X ! Y so that h(x) (x is a
random input vector) best approximates y (the corresponding output vector).

To build an MTS model, we follow a two-stage approach. In the training phase, we first
build m single-output prediction models. Then we stack input variables to m -1 output variables (X1 , X2 , · · · , Xd , Y1 , · · · , Yk 1 , Yk+1 , · · · , Ym ) to predict Yk . In the testing phase, we use

the first stage single-output prediction models to obtain a predicted Ŷ . We then use the predicted
Ŷ together with X to predict the final Ŷ .
To build an RCMC model, we calculate the correlation coefficients between output variables,
⇢ij , and calculate the average correlation coefficient ⇢i for each variable, averaged across all j
covariables. We rank the average correlations in descending order and use the order to determine
0

0

0

the sequence of prediction, represented by Y1 , Y2 · · · Ym . For the first prediction model, we build a
0

single-output prediction based on the inputs ( X1 , X2 , · · · , Xd ) and ( Y1 ). Subsequently, we use
0

0

0

inputs ( X1 , X2 , · · · , Xd ) and the previously used output variables ( Y1 , Y2 , · · · , Yk 1 ) together to
0

0

predict the new output variable ( Yk ). One caveat is that Y1 will not be available in the prediction
task after the model is constructed. Therefore, the prediction model will use
⇣ previously predicted
⌘
0

0

0

0

outputs. In other words, to predict (Ŷk , the values (X1 , X2 , · · · , Xd ) and Ŷ1 , Ŷ2 , · · · , Ŷk 1 are
used.

4.2.2

Base Learners
0

0

The regression chain introduced above adapts to the base learner hk : X ! Yk . A wide range

of choices exists for the base learner. We choose the following five widely used methods: linear
regression, elastic net with second-order terms, Gaussian process regression, random forest, and
neural network. For each method, we record the performance of the method in predicting individual
outcomes one at a time (hk : X ! Yk ) as well as the performance when using RCMC or MTS
together with the base learner. We note that neural network and random forest learners can
inherently handle multioutput regression since both can use total/average mean squared error across
all outputs to train the respective model.

CHAPTER 4. METAMODELING FOR POLICY SIMULATIONS

59

Linear Regression (LR)
Linear regression (LR) is a popular metamodeling method because of its fast implementation and
easy interpretability. The performance of LR can be comparable to or even better than more
sophisticated statistical models when data are sparse or scarce. However, in our metamodeling
setting, data sparsity is not a problem since we can use the original model to generate a sufficient
number of data points. We use LR as a baseline for performance evaluation.
Elastic Net with Second-Order Terms (EN)
To increase prediction power, second-order terms are added to the original input space together
with the elastic net model (EN) to capture non-linear relationships between inputs and outputs. We
choose elastic net regularization to overcome the problem of over-fitting and to allow for flexibility
between first-order (Lasso) and second-order (Ridge) regularizations since the relative performance
of Lasso and Ridge will depend on the distribution of true regression coefficients [145]. However,
use of elastic net also increases computational complexity. The objective of the elastic net is
min
w

1
kXW
2n

yk22 + ↵⇢kW k1 +

↵ (1 ⇢)
kW k22
2

(4.1)

where ↵ is the overall magnitude of regularization, ⇢ is the proportion of regularization from L1
norms which are sums of the magnitudes in vector space, 1- ⇢ is the L2 norms which are Euclidean
norms, n is the number of data points, X is feature vector, and y is corresponding output variable.
W is a weight vector to be determined through solving the optimization problem.
Gaussian Process Regression (GPR)
Gaussian process regression (GPR) is a Bayesian non-parametric kernel-based probabilistic model.
GPR assumes that the distribution of output data {f (x) , x 2 Rd } is a joint Gaussian
⇣
⌘ distribution
0

specified by mean function mean (x) and covariance/kernel function kernel x, x . In general,

similar input data will have similar output values. We use GPR with a Matern kernel due to its

flexibility in allowing for control of smoothness and because it can replicate di↵erent kernels by
changing model hyperparameters [1].
Random Forest (RF)
Random forest (RF) is an ensemble machine learning method that constructs multiple decision
trees and uses maximum voting or averages over all individual trees to obtain the outputs. RF
imposes no assumptions on the data and usually produces high prediction accuracy even without
hyperparameter tuning [29].

CHAPTER 4. METAMODELING FOR POLICY SIMULATIONS

60

Neural Network (NN)
Neural networks (NN) are a class of machine learning models and algorithms that use connected,
hierarchical functions [63]. We use multilayer perceptron, a technique that employs backpropagation
for training: a feedforward neural network model comprises an input layer, a changeable number of
middle layers, and an output layer. Each input/output variable is modeled by a neuron in the NN.
Each neuron value is determined by the all connected neurons from the last layer which follows as
P
f (x) = ( wi xi ). The activation function (·) is a predefined non-linear function to capture the
non-linearity in the data.

4.2.3

Performance Improvement Scheme

We now present two ways of improving model performance: model-free variable selection and
hyperparameter tuning for di↵erent base models. With MCMC, we have one regression for each
output variable but the input variable set is modified since we need to include the previous predicted
output variables. Variable selection works on the newly constructed input variable set, which
includes both the original input variables plus the new predicted output variables.
Variable Selection
To improve model performance, as well as interpretability, we implement a variable selection scheme
that is model-free and can be integrated into the training process. The objective is to search for
the subset of input variables that will result in the best model performance. However, because of
limitations on computational power and the typically large number of input features in simulation
models in public health, it is not feasible to perform an exhaustive search over entire subsets of
input variables.
We therefore use the following greedy search algorithm for variable selection:
• Step 1: Start with the full input variable set.
• Step 2: Remove the least important feature.
– Randomly permute each input variable.
– Compute the resulting accuracy reduction from each permuted input variable in the
validation data set.
– Remove the input variable with the smallest reduction in prediction accuracy.
• Step 3: Compute model performance by calculating the average of coefficient of determination
(average R 2 ) for the remaining subset of input variables. Go to Step 2 if the number of
variables is more than 1; otherwise, go to step 4.

CHAPTER 4. METAMODELING FOR POLICY SIMULATIONS

61

Table 4.1: Summary of explored hyperparameters (default parameters are bolded)
Model
Linear regression
Elastic net
Gaussian process regression
Random forest

Neural network

Explored Hyperparameters
None
1. ↵ = [0.001, 0.01, 0.1, 1]
2. ⇢ = [0.01, 0.1,0.5, 0.9, 0.99]
1. Kernel: [Radial Basis Function, Matérn with
p
1. # of variables to sample: [log2 D, D, D
2 , D]
2. # of trees: [ 100, 500, 1000]
1. # of hidden layers: [1,2]
2. # of neurons in hidden layer:
aaaa1 Layer: [D, 2D]
aaaa2 Layers: [(D, 16), (2D, 16), (D, 8), (2D, 8)]
3. Activation functions: ReLU, tanh, sigmoid

= [1/2, 3/2, 5/2]

• Step 4: Select the optimal subset of input variables based on model performance.
Hyperparameter Tuning
A hyperparameter is a parameter whose value is set before the learning process begins (e.g., the
number of trees considered in an RF algorithm). In hyperparameter tuning, hyperparameters
are varied to determine the best hyperparameters for a learning algorithm. Table 4.1 shows the
hyperparameters for each base model considered.

4.2.4

Evaluation Metrics

Model Accuracy
To evaluate the performance of each model on the test data set, we calculate the average coefficient
of determination (average R 2 ). This is given by the following formula:
2

average R =

m
X

1

j=1

(i)

where yj

Pn

(i)
fj (x(i) ))2
i=1 (yj
Pn
(i)
y j )2
i=1 (yj

(4.2)

is the i -th data point for j -th output variable, fj (·) is the trained metamodel for j -th

output variable, y j is the average over j -th output variable and x(i) is the i -th input variable.
Computational Efficiency
In addition to accuracy, we also assess computational efficiency, which we characterize by the CPU
time required for training and prediction as a function of the number of input variables and the
number of data points in the respective data set.

CHAPTER 4. METAMODELING FOR POLICY SIMULATIONS

62

Model Interpretability
Model interpretability is frequently an important goal of metamodeling, especially for models used
in public health. In order to make informed policy decisions, stakeholders want to understand the
relationship between inputs and outputs. Some models are intrinsically interpretable, but others
are too complex to understand and require post-hoc methods to improve interpretability.
To facilitate the interpretation of di↵erent models, we can use di↵erent model-agnostic tools,
which can be divided into global and local interpretation methods. Global interpretation methods
aim to illuminate the relationship between input and output variables. The variable importance
calculation determined from the variable selection scheme is a good way to assess the magnitude of
the impact of each input variable on the output variables.
To further understand the relationships between input and output variables, we can use the
partial dependence plot, which is similar to one-way or two-way sensitivity analysis in public health
simulation analysis. The partial function fˆxs (where xs is the input
⇣ variable
⌘ we are interested in)
Pn
(i)
(i)
1
ˆ
is estimated by calculating over the training data: fxs = n i=1 f xs , xc
where the terms xc
are all other input variables. Another way to achieve global interpretability is to generate a global

surrogate model, which is an interpretable model (like a decision tree or linear regression) that can
replicate the prediction of the original analytical model to the greatest extent [83].
Local model interpretation provides insights into an individual prediction of any black-box
model. Similar to a global surrogate model, a local surrogate model (LIME, or local interpretable
model-agnostic explanation) aims to replicate the prediction of the original analytical model at
individual data points by weighting permuted samples based on proximity to the data point of
interest.

4.2.5

Case Study: HCV Screening and Treatment

We illustrate our methods with a microsimulation model that assesses the clinical outcomes, coste↵ectiveness, and budgetary impact of various HCV screening and treatment strategies in US prisons
[9]. Our goal is to develop a simplified model that can be used as a tool by local and regional planners
to determine appropriate HCV testing and treatment strategies for their particular correctional
facilities.
For the metamodeling analysis, we considered five outcomes: the number of HCV cases identified, the number of HCV cases cured, screening cost, treatment cost, and total cost. The original
microsimulation analysis considered 15 di↵erent HCV screening and treatment strategies, in addition to the status quo of no testing or treatment. After discussions with stakeholders, we narrowed
down to six strategies evaluated using metamodeling, all compared to the status quo: risk-based
testing, treating only individuals whose HCV infection had progressed to fibrosis stage F3 or higher

CHAPTER 4. METAMODELING FOR POLICY SIMULATIONS

63

(F3+); risk-based testing, treating only individuals whose HCV infection had progressed to fibrosis
stage F2 or higher (F2+); risk-based testing, treating all patients with HCV (treat all); test all,
treat F3+; test all, treat F2+; test all, treat all. We considered two di↵erent time horizons (1 year
and 2 years). This yielded a total of 60 output variables (five outcomes for six strategies over two
time horizons).
We initially selected 37 input variables from the simulation model that stakeholders and the
modelers who developed the simulation model deemed the most important. In preliminary analysis,
we determined that some of these 37 variables had no impact on the 60 output variables, so we
shortened the list to 22 input variables (Table 4.2). We drew 2,000 independent samples from each
of the specified distributions for the 22 input variables and fed these 2,000 input variable sets into
the original simulation model. To minimize Monte Carlo noise, we ran the simulation model with
1,000,000 individuals for each input variable set. We randomly divided the 2,000 sets of input and
output variables into a training set with 1,600 sets and a test set with 400 data sets.
We implemented five single-prediction models corresponding to five base learners: LR, EN,
GPR, RF, and NN. We then implemented MTS and RCMC for each base learner. The final two
sets of models focused on improving the performance of MTS and RCMC models with variable
selection and hyperparameter tuning.

4.3

Results

The output variables from the simulation model were heavily correlated (average ⇢=0.58), suggesting a potential need for our methods.

4.3.1

Model Accuracy

Table 4.3 shows the average coefficient of determination average R2 for each model considered.
Looking at the single-output case – that is, without multioutput algorithm adaptation methods –
the worst-performing model was simple linear regression, with an R2 of 0.8812. Next best were EN
and RF, which both had an R2 of 0.9536 for the single-output case, followed by NN with an R2 of
0.9651, and GF with an R2 of 0.9865.
Inclusion of the multioutput algorithm adaptation methods increased R2 for all models, as
expected. RCMC and MTS increased R2 by 0.002 on average across all base learners, and variable
selection and hyperparameter tuning added 0.009 on average. The best performing model, GPR,
had an R2 of 0.9921 with RCMC, variable selection, and hyperparameter tuning – an extremely
high level of accuracy.

CHAPTER 4. METAMODELING FOR POLICY SIMULATIONS

64

Table 4.2: List of Input Variables for the Metamodels
Variable Name
sex male prev v2
age mon miu
age mon sd

Description
% of the cohort that is male
Mean age of cohort in months (baseline year = 2015)
Standard deviation of cohort age in months (baseline year = 2015)
Among those who are currently in prison or the prison system,
chronic hcv v2
estimated prevalence of chronic HCV infection (RNA+)
idu status none
% of individuals who have never been injection drug users
The percentage of the incarcerated population who report having
idu status former
a history of substance use disorder but no current active use
The percentage of the incarcerated population who report having
idu status current
a current, active substance use disorder (injection)
sentence dur mon miu
Mean sentence duration in months
sentence dur mon sd
Standard deviation of sentence duration in months
req sentdur eligible
Minimum sentence duration in months to be eligible for HCV treatment
Of those identified as having chronic HCV infection, the percentage
link care prison
who link to care in prison for HCV disease staging and treatment
Probability of linkage to HCV community care centers when released
prob link at release
from prison
Cost to the prison or prison system of an HCV antibody test; includes
cost test anti
the cost of the test reagents as well as any laboratory, sta↵, and overhead costs
Cost to the prison or prison system of an HCV RNA test; includes the
cost test rna
cost of the test reagents as well as any laboratory, sta↵, and overhead costs
incid hcv corr
Monthly incidence of HCV in prison for injection drug users
incid hcv comm
Monthly incidence of HCV in the community for injection drug users
Monthly cost of treating HCV with glecaprevir/pibrentasvir among
cost trt Glec pib
patients who have never been treated before and who do not have cirrhosis
Monthly cost of treating HCV with sofosbuvir/velpatasvir among
cost trt Sof vel
patients who have never been treated before and who do have cirrhosis
Monthly cost for treating HCV with sofosbuvir/velpatasvir/ voxilaprevir
among patients who have previously been treated but did not attain
cost trt Sof vel vox
HCV cure and who do not have cirrhosis
Monthly cost for treating HCV with sofosbuvir/velpatasvir/voxilaprevir/ribavarin
cost trt Sof vel vox rbv among patients who have previously been treated but did not attain
HCV cure and who do have cirrhosis
lab test
Type of fibrosis staging test (APRI or fibroscan)
test specif
Specificity of fibrosis staging test (given APRI or fibroscan)
HCV: hepatatis C, RNA: Ribonucleic acid, APRI:aspartate aminotransferase to platelet ratio

CHAPTER 4. METAMODELING FOR POLICY SIMULATIONS

65

Table 4.3: Summary of model performance (averageaR2 ) on test data set for hepatitis C virus
model
Model
LR
EN
GPR
RF
NN
Single Output
0.8812 0.9536 0.9865 0.9536 0.9651
RCMC
0.8814 0.9539 0.9875 0.9552 0.9672
RCMC + VS
0.8822 0.9541 0.9917 0.9598 0.9809
RCMC + VS + HT
0.9598 0.9921 0.9605 0.9833
MTS
0.8815 0.9541 0.9874 0.9556 0.9679
MTS + VS
0.8824 0.9564 0.9917 0.9584 0.9871
MTS + VS + HT
0.9592 0.9919 0.9608 0.9829
LR = linear regression, EN = elastic net, GPR = Gaussian process regression, RF = random
forest, NN = neural network, RCMC = regression chain with maximum correlation, VS =
variable selection, HT = hyperparameter tuning (LR does not have HT)

4.3.2

Computational Efficiency

Figure 4.1 shows the relationships between training and prediction time and the number of input
variables and the size of the training data set for each base model. Figures 4.1a and 4.1b show the
relationships between training time and the number of variables and the size of training dataset. In
Figure 4.1a, training the neural network is the most time-consuming task and requires a maximum
of 58.6 seconds to train one output variable when the number of input variables equals 80 whereas
linear regression only takes 0.009 seconds to train one output variable. The run time of NN is
positively correlated with the number of input variables and the size of the training data set because
of an increase in the number of edges from input neurons to hidden layers (in our model, the number
of hidden neurons is also positively correlated with the number of input variables) and the number
of data points to be used in the training. In Figure 4.1b, training GPR is the most time-consuming
task and requires a maximum of 15.9 seconds to complete the training of one output variable when
training data size equals 1600 whereas LR only takes 0.002 under the same condition. The training
time for GPR is greatly a↵ected by the number of input data points because the difficult operations
on large kernel matrices used to handle large training data size. The training time of RF is lower
than that of NN and GPR. EE and LR have the lowest average training times. Because we might
need to employ hundreds or even thousands training runs in training to cover di↵erent output
variables and employ di↵erent performance improvement schemes, the total amount of time spent
in training phase can be very lengthy for GPR and NN.
Figures 4.1c and 4.1d show the relationships between prediction run times and the number of
variables and the size of training dataset. For the prediction task, NN requires the most time of the
five methods we tested. As shown in Figure 4.1c, NN takes a maximum of 11.4 seconds to finish
the prediction task when the number of input variables equals 80 whereas LR only takes 0.0002

CHAPTER 4. METAMODELING FOR POLICY SIMULATIONS

66

(a) Training time with training data set size =
1600

(b) Training time with number of input variables =
22

(c) Prediction time with testing data set size
= 400

(d) Prediction time with number of input variables =
22 and testing data set size = 400

Figure 4.1: Average training and prediction times for the five base models
LR = linear regression, EN = elastic net, GPR = Gaussian process regression, RF = random
forest, NN = neural network
seconds. A similar observation can be made from Figure 4.1d: NN’s prediction time is a maximum
of 1.3 seconds whereas LR’s prediction time is on the order of 10 7 seconds. NN’s prediction time
increases with the number of input variables due to the increased neural net complexity and with
the size of the training set. The other four models have a very short prediction time for the testing
data set size of 400. In all cases, the prediction time was on the order of seconds. This is in contrast
to the original HCV microsimulation model where a single run required approximately 15 minutes
of computation time.

4.3.3

Model Interpretability

For model interpretability, LR and EN have clear advantages. LR is the most interpretable since
regression provides a closed-form solution with calculated coefficients that characterize the relationship between inputs and outputs. However, if the relationship between inputs and outputs is not
linear, making linear inferences based on coefficient values could be misleading. EN has slightly less
interpretability compared to LR because of the regularization terms: for instance, no closed-form
solutions on standard error estimates are provided. RF is an aggregation of single decision trees.
Even though the decision trees are intrinsically interpretable, the ensemble of multiple trees distorts

CHAPTER 4. METAMODELING FOR POLICY SIMULATIONS

67

Table 4.4: Variable importance for RF
Feature
Weight (mean ± std)
idu status current
0.8846 ± 0.0774
chronic hcv v2
0.2392 ± 0.0121
age mon miu
0.0205 ± 0.0015
RF = random forest; feature names are decoded in Table 4.2
Table 4.5: Variable importance for GPR
Feature
Weight (mean ± std)
idu status current
0.8592 ± 0.0600
chronic hcv v2
0.3865 ± 0.0553
idu status none
0.0323 ± 0.0007
idu status former
0.0303 ± 0.0024
age mon miu
0.0156 ± 0.0014
sentence dur mon miu
0.0103 ± 0.0030
sentence dur mon sd
0.0094 ± 0.0015
age mon sd
0.0033 ± 0.0005
test specif
0.0022 ± 0.0007
lab test
0.0021 ± 0.0003
sex male prev v2
0.0005 ± 0.0001
GPR = Gaussian process regression; feature names are decoded in Table 4.2
the original simple interpretable structure. GPR, our best performing model in terms of prediction,
is conceptually easy to understand and its probabilistic structure can provide uncertainty bounds
around predictions, but the inclusion of di↵erent kernels obscures the relationship between outputs
and inputs. Lastly, NN models are the least interpretable due to the addition of multiple layers
and di↵erent activation functions.
To facilitate the interpretation of the models, we examine the importance of variables, as calculated in the variable selection process, for predicting the number of HCV cases identified by
risk-based testing in one year. After the variable selection process, 3 variables were selected for
RF and 11 variables were selected for GPR. For both RF and GPR, the most important variable
in determining the number of HCV cases identified by risk-based testing in one year was the percentage of current injection drug users (IDUs) in the population (Tables 4.4 and 4.5). The second
most influential variable is the prevalence of chronic HCV in prison. All other variables had far less
importance. This is not surprising since risk-based testing strategies were defined in reference to
IDU status, and these are expected to lead to numbers of identified HCV cases that are proportional
to the number of people designated as the target population for testing.
To further understand the relationship between the input and output variables, we examined
the partial dependence plots for IDU prevalence and chronic HCV prevalence. Figure 4.2 shows

CHAPTER 4. METAMODELING FOR POLICY SIMULATIONS

68

(a) Partial dependence plots from RF

(b) Partial dependence plots from GPR

Figure 4.2: Partial dependence plots from RF (random forest) and GPR (Gaussian process regression) for predicting the number of hepatitis C virus (HCV) cases identified in one year by risk-based
testing.
The blue shaded region is the 95% confidence interval.
how the number of HCV cases identified in one year increases as chronic HCV and IDU prevalence
increase, for both RF (Figure 4.2a) and GPR (Figure 4.2b).
Figure 4.3 shows the weight for each variable in the local approximation (or LIME) model to
explain one test data point for RF (Figure 4.3a) and GPR (Figure 4.3b) in predicting the number
of HCV cases identified in one year. Figures 4.3a and 4.3b both show that each unit increase in
IDU prevalence had a larger impact on the output variable than a unit increase in chronic HCV
prevalence, with the e↵ect significantly larger for RF.

CHAPTER 4. METAMODELING FOR POLICY SIMULATIONS

69

(a) LIME model from RF

(b) LIME model from GPR

Figure 4.3: Prediction of the number of hepatitis c virus (HCV) cases identified in one year by
risk-based testing. LIME (local interpretable model agnostic) models from RF (random forest) and
GPR (Gaussian process regression) for one test data point when limiting the local linear regression
variables to variables found by variable selection.
The bar width is the weight of each variable in the local regression. The local regression has a bias
term. Variable definitions are decoded in Table 4.2

CHAPTER 4. METAMODELING FOR POLICY SIMULATIONS

4.4

70

Discussion

Prediction accuracy is one important metric for assessing a metamodel. For some simulation models,
including the example used in this chapter, there may be a clear relationship between input and
output variables that even simple learners like LR can approximate (R 2 = 0.8812 for our case
study). Even in these cases, our analysis shows how multioutput algorithm adaptation methods
can further improve model accuracy (up to R2 = 0.9921 for GPR in our case study). Achieving
such improvement is important in health policy settings since decisions informed by a metamodel
can have profound e↵ects on individual health conditions as well as overall population costs and
benefits.
Inclusion of the multioutput algorithm adaptation methods MTS and RCMC increased R 2 by
0.002 on average across the five base learners in our case study. Although this improvement is
not substantial for our analysis, RCMC and MTS have been shown to significantly improve model
accuracy in a variety of other applications [25]. One reason for the small increase in our analysis
might come from the fact that our data set of inputs and outputs, generated from a carefully
constructed simulation model, already exhibits a strong correlation between certain input and
output variables so the transformation of the input variables set by including output variables does
not provide sufficient additional information for improvement.
Variable selection and hyperparameter tuning added an additional 0.009 on average to R2 in
our case study. In the variable selection process, 11 of the 22 variables were not selected by any
model. Variable selection thus improved model accuracy while also shortening the list of variables
that end-users need to consider. Hyperparameter tuning can also improve model accuracy, but the
results will depend on the number of di↵erent hyperparameters searched, potentially requiring very
high computational power.
In addition to model accuracy, other factors such as model efficiency and interpretability are
also important in our setting. All of the metamodels had very short computation times, much
shorter than the computation time required for the original model, and thus any of them could be
instantiated as a tool for decision makers. LR has an advantage in its simplicity, versatility, and
interpretability. Another advantage of LR is that it is easy to obtain a prediction interval. This
is useful in public health where stakeholders and policy makers often want to know the range of
possible outcomes when assessing the impact of proposed strategies. For other types of models, we
have shown that various methods can be used to improve interpretability.
Our study has several limitations. We have illustrated our proposed metamodeling framework
for a specific simulation model. Application of our framework to other simulation models might
yield di↵erent findings. Nevertheless, we have shown that metamodels can run quickly with a high
degree of accuracy, and this is likely to be true for other metamodels of simulation models as well.

CHAPTER 4. METAMODELING FOR POLICY SIMULATIONS

71

Additionally, because of limited computational power, we limited our search space for hyperparameter tuning. However, considering the high values of R2 that were obtained, the potential for
additional improvement is very limited.
Metamodeling is an important tool to make results from complex models accessible to decision
makers. This study provides a framework for metamodeling in policy analyses with multivariate
outcomes. While the advantages and disadvantages of specific learning algorithms may vary across
di↵erent modeling applications, we expect that the general framework presented here will have
broad applicability to decision analytic models in health and medicine.

Chapter 5

Conclusions
This dissertation has shown how operations research and data analytics can inform healthcare decision making. Chapter 2 developed a new theoretical framework for sequential clinical decision
making. The QMDP model decodes the distribution of cumulative reward and provides an interpretable way to incorporate a decision-maker’s risk attitude. The theoretical framework can be
applied to solve various clinical decision making problems and is aligned with the goals of patientcentered care. Chapter 3 developed a model for health policy decision making. We used the
microsimulation model developed in that chapter to assess the e↵ectiveness and cost-e↵ectiveness
of providing antidepressants to HIV-infected individuals with depression. Chapter 4 focused on
model implementation. The metamodeling framework developed in that chapter can be used to
simplify policy simulations with multivariate outcomes, and was illustrated with its application to
a microsimulation model of HCV screening and treatment strategies in correctional facilities.
Numerous promising areas for further research remain.
First, as demonstrated by the HIV example in Chapter 2, the landscape of clinical decision
making is constantly changing as new requirements are imposed and new data become available,
which presents new opportunities and challenges for researchers. Traditional sequential decision
models such as MDP/QMDP require e↵ort to develop a concise and comprehensive representation of
a person’s health state that captures key aspects of the disease under consideration while also being
computationally tractable. How much of this information should be included in sequential decision
models? One can answer this question using a data-driven approach. The constructed model should
not only help select decision-related variables from a pool of available patient characteristics and
clinical variables but should also help infer the possible latent variables. This will not only improve
model performance but will also help provide insights into the relationships between variables.
Improved sequential medical decision models will be particularly useful for chronic conditions such as

72

CHAPTER 5. CONCLUSIONS

73

heart disease, diabetes and cancer, and infectious diseases such as HIV, where continuous monitoring
and treatment are needed.
Second, as shown in Chapter 3, traditional operations research methods such as simulation are
vital in many disease modeling problems, especially in the current environment of emerging global
disease outbreaks. An important area for further research is development of mathematical models
for di↵erent diseases and assessment of feasible and cost-e↵ective ways to control disease. Methodologically, there are many areas of interest in disease modeling. For example, structural sensitivity
analysis — that is, examining the form of the model that is used to develop recommendations — is
important since di↵erent models applied to the same disease context sometimes recommend di↵erent strategies. How to provide recommendations to policy makers when there is a lack of consensus
is a rich area for research.
Third, as introduced in Chapter 4, data analytics has become one of the most promising methodological areas for solving healthcare decision making problems. One of the most active areas is
“personalized medicine”. The term “personalized medicine” often refers to the use of genomic information to guide prognosis, diagnosis, and treatment decisions. However, patient heterogeneity
is not limited to genomic information; it also includes behavior, prior exposures, comorbidity, and
many other factors. Newly available healthcare data that captures such heterogeneities has created
opportunities for improving personalized medical decisions. Challenges arise throughout the entire
process: for example, data may be sparse and of low quality; learning models must employ appropriate causal reasoning; and clinical implementation of such models requires intense communication
and continued feedback. Developing methodologies and frameworks to help solve these challenges
is a key area for further research.

Appendix A

Supplement for Chapter 2
A.1

Relaxation of Assumption 1 (c)

Assumption 1 (c) was introduced for simplicity in our mathematical derivations. The assumption
requires that the reward function can be expressed as a function of St , at , and St+1 . Recall that the
input for the OPT problem (2.8) is
vt+1 (si , qi ) + rt (s, a, wt ) = vt+1 (si , qi ) + rt (s, a, lt (s, a, si )) .
When Assumption 1 (c) holds, the second part of this expression becomes deterministic with the
knowledge of St+1 (it is a function of St = s, at = a, and St+1 ). Therefore, from vt+1 (·, ·), we can
easily obtain the input for OPT by increasing it with the constant. However, when the assumption
does not hold, the reward rt (s, a, wt ) is a random variable and no longer a constant determined by
St , at and St+1 .
Theorem 1 and its proof tell us that this di↵erence does not matter as long as we can compute
the ⌧ -th quantile of the sum
vt+1 (si , qi ) + rt (s, a, wt )
for any ⌧ 2 [0, 1]. As in the proof of Theorem 1, we introduce a random variable Xsi such that
Q⌧ (Xsi ) = vt+1 (si , ⌧ )
for any ⌧ 2 [0, 1]. Let Wsi = rt (s, a, wt ) . It is easy to see that conditional on St+1 , the terms Xsi

and Wsi are independent. Then, what is left to come up with is an algorithm that takes as input
the quantile functions of two independent random variables and outputs the quantile function of

74

APPENDIX A. SUPPLEMENT FOR CHAPTER 2

75

their summations. For discrete random variables, this can be done efficiently. First, let the input
be two random variables X1 and X2 , which are represented by two sets of probability-value pairs
{(p1 , a1 ), ..., (pn , an )} and {(q1 , b1 ), ..., (qm , bm )}, i.e. P (X1 = ai ) = pi and P (X2 = bi ) = qi . The

idea is that when X1 and X2 are independent, we can easily compute the distribution of their sum
and therefore the quantile function. The procedure is summarized in Algorithm 3.
Algorithm 3 Algorithm for Computing Quantiles of the Sum of Random Variables
1: Input: Two random variables and their probability-value pairs:
2:
X1 : {(p1 , a1 ), ..., (pn , an )} and X2 : {(q1 , b1 ), ..., (qm , bm )}
3: Initialize Let D = {}.
4: for i = 1, ..., n do
5:
for j = 1, ..., m do

D = D [ {(pi qj , ai + bj )}
7: Merge pairs in D with the same value by:
8: Removing two pairs with same value and appending a new pair with the same value but with
the probability being the sum of the two removed probabilities.
9: Output: D, the set of probability-value pairs that represent the distribution of X1 + X2 .
6:

The complexity of Algorithm 3 is O(mn), where n and m are the number of breakpoints for
the quantile functions of X1 and X2 , respectively. In the context of QMDP, the complexity is
upper bounded by O(R2 T ). Therefore, the complexity bound for QMDP with an arbitrary reward
function will be O AST · max(R2 T, S) . Here T is the length of time horizon and A = |A| and
S = |S| are the sizes of the action and state spaces, respectively.

When Assumption 1 (c) is relaxed, we need to compute the sum of two random variables at every

time t for the input for OPT. This operation adds an order of O(R) to the complexity. However,
since the overall complexity is at most quadratic with respect to all the variables, the algorithm is
still efficient and scalable.

A.2

Proof of Lemmas and Theorems

A.2.1

Proof of Lemma 2

We first introduce two lemmas.
Lemma 3 Consider n binary random variables Yi 2 {0, 1} with
variables Xi . Then we have
P

n
X
i=1

Xi Y i

C

!

=

n
X
i=1

P (Yi = 1)P (Xi

Pn

i=1 Yi

C|Yi = 1),

= 1, and n random

APPENDIX A. SUPPLEMENT FOR CHAPTER 2

P

n
X

Xi Yi > C

n
X

Xi Yi  C

n
X

Xi Yi < C

i=1

P

i=1

P

i=1

!

=

!

=

!

=

76

n
X

P (Yi = 1)P (Xi > C|Yi = 1),

n
X

P (Yi = 1)P (Xi  C|Yi = 1),

n
X

P (Yi = 1)P (Xi < C|Yi = 1),

i=1

i=1

i=1

for any C 2 R.
[Proof of Lemma 3] We show that the first equation and the rest are similar.
P

n
X

Xi Yi

C

i=1

!

=
=
=

n
X
i=1
n
X
i=1
n
X

n
X

P

Yi = 1, Xi Yi

i=1

P (Yi = 1, Xi
P (Yi = 1)P (Xi

C

!

C)
C|Yi = 1)

i=1

Lemma 4 For a discrete random variable X and C 2 R, let
q = P (X < C).
Then
Qq+✏ (X)

C

for any ✏ > 0. Here Q⌧ (·) is the quantile function as in Definition 1.
[Proof of Lemma 4 ] For any C0 < C, P (X  C0 )  q < q + ✏. This is true by the definition of the
quantile.

With the above two lemmas, we now proceed to prove Lemma 2.
[Proof of Lemma 2] First, we show that
Q⌧

n
X
i=1

Xi Yi

!

max
q

min

i2{qi 6=1|i=1,2,...,n}

hi (qi )

(A.1)

APPENDIX A. SUPPLEMENT FOR CHAPTER 2

77

where hi (qi ) = Qqi (Xi ). Let
f (q) =

min

i2{qi 6=1|i=1,2,...,n}

hi (qi ).

To show (A.1), we only need to show for any feasible q = (q1 , ..., qn ) that
n
X

Q⌧

Xi Yi

i=1

!

f (q).

(A.2)

By saying q is feasible, we mean that q = (q1 , ..., qn ) is subject to
n
X
i=1

pi qi  ⌧,

qi 2 [0, 1], pi = P (Yi = 1).
We show (A.2) by contradiction. If
n
X

Q⌧

Xi Yi

i=1

!

< f (q),

then there exists ✏ > 0 such that
n
X

Q⌧

Xi Yi

i=1

!

 f (q)

✏.

!

⌧.

From the definition of quantiles, this implies
n
X

P

i=1

Xi Yi  f (q)

✏

However, from Lemma 3,
P

n
X
i=1

Xi Yi  f (q)

✏

!

=


n
X
i=1
n
X
i=1

pi P (Xi  f (q)

✏|Yi = 1)

pi P (Xi  hi (qi )

✏|Yi = 1) <

n
X
i=1

pi qi  ⌧.

Here the inequality in the middle of the second line is strict because of the definition of hi (qi ). So
the above leads to a contradiction, which means that inequality (A.2) is true.

APPENDIX A. SUPPLEMENT FOR CHAPTER 2

78

We now prove by construction that there exists q0 = (q10 , ..., qn0 ) such that
Q⌧

n
X

Xi Yi

i=1

!



min

i2{qi0 6=1|i=1,2,...,n}

Let
C = Q⌧

n
X

Xi Y i

i=1

!

hi (qi0 ).

(A.3)

.

From the definition of quantiles, we know that
P

n
X
i=1

and
P

n
X
i=1

Xi Y i  C

!

Xi Yi  C

✏

⌧
!

<⌧

(A.4)

for any ✏ > 0. We let q̄i = P (Xi < C|Yi = 1). We know that
n
X

pi q̄i =

i=1

n
X

pi P (Xi < C|Yi = 1) = P

i=1

n
X

Yi Xi < C

i=1

!

 ⌧.

Then, we prove that the inequality in the above equation is not binding, i.e.
n
X

pi q̄i < ⌧.

n
X

pi q̄i = ⌧,

i=1

If

i=1

it follows that
P

n
X
i=1

Yi Xi = C

!

=P

n
X
i=1

Yi Xi  C

!

P

n
X
i=1

Y i Xi < C

!

= 0.

Thus, P (Xi = C) = 0 for all i. Because the random variables Xi are discrete, there must exist
C0  C such that

P [Xi 2 (C0 , C)] = 0

APPENDIX A. SUPPLEMENT FOR CHAPTER 2

79

for all i. This implies
n
X

P

i=1

Yi Xi  C 0

!

=P
=P

n
X
i=1
n
X
i=1

Y i Xi  C
Yi Xi  C

!
!

n
X

P

i=1

Yi Xi 2 (C0 , C)

!

P

n
X
i=1

Y i Xi = C

!

= ⌧.

But this contradicts (A.4). Thus, we have
n
X

pi q̄i < ⌧.

i=1

With this strict inequality, let
⌧0 =

n
X

pi q̄i ,

i=1

✏=

⌧

n

⌧0

,

and
qi0 = min{q̄i + ✏, 1}.
From Lemma 4, we know that
hi (qi0 ) = Qqi0 (Xi )

C,

for qi0 6= 1. Therefore, (A.3) follows.

A.2.2

Proof of Theorems

Proof of Theorem 1
From the definition of vet (s, ⌧, a), we know that
vet (s, ⌧, a) =
=

max

vt⇡t:T (s, ⌧ )

max

X

{⇡t:T 2⇧|µt (s)=a}

{⇡t:T 2⇧|µt (s)=a}

Q⌧

s0 2S

0

1{St+1 = s |St = s, at = a}

"T 1
X
k=t

rk (Sk , ak , wk ) St = s, St+1 = s

0

#!

The cumulative reward part is
"T 1
X
k=t

#

rk (Sk , ak , wk ) St = s, St+1 = s0 , at = a = rt (St , a, wt )+

"T 1
X

k=t+1

#

rk (Sk , ak , wk ) St = s, St+1 = s0 , ⇡s0 .

APPENDIX A. SUPPLEMENT FOR CHAPTER 2

80

Here the first term rt (St , a, wt ) is deterministic with the knowledge of St , a,and St+1 , while the
second term is a random variable dependent on the state s0 . And we denote the follow-up policy
for time t + 1 to T with ⇡s0 . Let
⇡
Xs0s0 rt (St , a, wt ) +

"T 1
X

0

#

rk (Sk , ak , wk ) St = s, St+1 = s , ⇡s0 .

k=t+1

The subscripts indicate that the random variable is dependent on the state of St+1 and the policy
thereafter. Let the state space S = {s1 , ..., sn }. The definition of the value function vt+1 (s0 , ⌧ 0 ) tells
us that

Q⌧ 0 (Xs⇡it+1:T )  rt (St , at , lt (St , at , si )) + vt+1 (si , ⌧ 0 )
for any i = 1, ..., n and ⌧ 0 2 [0, 1]. The right-hand side of the above expression is a non-decreasing
function of ⌧ 0 2 [0, 1], and it can be treated as the inverse cumulative density function of a random
variable. Therefore we can introduce a random variable Zi such that

Q⌧ 0 (Zi |Yi = 1) = rt (St , at , lt (St , at , si )) + vt+1 (si , ⌧ 0 )
for any ⌧ 0 2 [0, 1]. We emphasize that the introduction of Zi is only for symbolic convenience in

matching the result in Lemma 2. By the definition of the OPT problem and Lemma 2, we know
that
OP T (s, ⌧, a, vt+1 (·, ·)) = Q⌧

n
X

Yi Zi

i=1

On the other hand,
vet (s, ⌧, a) =

max

(⇡s1 ,...,⇡sn )

⇡s

Q⌧

n
X

⇡s
Y i X si i

i=1

!

.

!

,

The relationship between Xsi i and Zi is that for any ⌧ 0 2 [0, 1],
⇡s

Q⌧ 0 (Xsi i )  Q⌧ 0 (Zi ),
and for each i and every ⌧ 0 there exists a policy ⇡s⇤i ,⌧ 0 that makes the equality hold.
First we show that
vet (s, ⌧, a)  OP T (s, ⌧, a, vt+1 (·, ·)) .

This only requires for any (⇡s1 , ..., ⇡sn ),
Q⌧

n
X
i=1

⇡s
Y i X si i

!

 OP T (s, ⌧, a, vt+1 (·, ·)) = Q⌧

n
X
i=1

Yi Zi

!

.

APPENDIX A. SUPPLEMENT FOR CHAPTER 2

81

⇡s

This is obviously true because of the relationship between Xsi i and Zi . Next, it is remaining to
show that there exists (˜
⇡s1 , ..., ⇡
˜sn ) such that
n
X

Q⌧

⇡
˜s
Y i X si i

i=1

!

n
X

Q⌧

Yi Zi

i=1

!

.

Let q⇤ = (q1⇤ , ..., qn⇤ ) be the optimal solution to the optimization problem OP T (s, ⌧, a, vt+1 (·, ·)) ,
and we take ⇡
˜si = ⇡s⇤i ,qi⇤ such that

⇡s⇤ ,q⇤

Qqi⇤ (Xsi i i ) = Qqi⇤ (Zi ).
Therefore,
Q⌧

n
X
i=1

⇡
˜s
Y i X si i

!

min

{qi⇤ 6=1|i=1,...,n}

Qqi⇤

✓

⇡s⇤ ,q⇤
X si i i

◆

=

min

{qi⇤ 6=1|i=1,...,n}

Qqi⇤ (Zi ) = Q⌧

n
X
i=1

Yi Zi

!

.

Here the first inequality is from Lemma 2 and the last equality is from the optimality of q. Thus,
we have
vet (s, ⌧, a)

OP T (s, ⌧, a, vt+1 (·, ·)) .

By combining the above two results, we finish the proof.
Proof of Theorem 2
This theorem follows directly from Theorem 1 using the fact that
vt (s, ⌧ ) = max vet (s, ⌧, a) = max OP T (s, ⌧, a, vt+1 (·, ·)).
a

a

Proof of Theorem 3

We prove by backward induction that the defined policy will optimize the value function vk (Sk , ⌧k ).
When k = T, the result is trivial. Assume the result is true for k = t + 1, i.e. for any state s0
0

0

s ,⌧
and quantile ⌧ 0 , there is a policy ⇡(t+1):T
such that under this policy, we can achieve a ⌧ 0 -quantile

reward vt+1 (s0 , ⌧ 0 ). Then for k = t, if we want to maximize the ⌧ -quantile reward with the state
St = s, then by solving the optimization problem OP T (s, a, ⌧, vt+1 (·, ·)) where a = arg max vt (s, ⌧ ),

we obtain the optimizer q⇤ = (q1⇤ , ..., qn⇤ ). From the last part of the proof of Theorem 1, we know
s ,q 0

i i
that by choosing the policy ⇡(t+1):T
, we can achieve the ⌧ -quantile reward vt (s, ⌧ ). Thus, we finish

the proof for the optimal policy by this induction argument.

APPENDIX A. SUPPLEMENT FOR CHAPTER 2

82

Proof for Theorem 4
We point out two facts. First, with the same condition on Xi , Yi as in Lemma 2,
CVaR⌧

n
X

Xi Yi

i=1

!

n
X

= Q⌧

Xi Yi

i=1

!

+

where pi = P (Yi = 1). Second, the quantile Q⌧ (

n
X

pi
1

⌧ i=1

Pn

"

E Xi

Q⌧

n
X

Xi Yi

i=1

!

#+

|Yi = 1

i=1 Xi Yi ) can be computed through the optimiza0
tion problem OPT. Then, the functions ut and ut represent the optimal CVaR cost-to-go function

and the corresponding quantile levels. With a similar argument as Theorem 2, we can show that
the dynamic programming procedure outputs the optimal value function by u0 (s, ⌧ ).
Proof of Theorem 5
Denote the state space as S and
gx (s, ⌧ ) = Q⌧

T
X1

r(Xt ) X0 = s

t=0

T
X1

gy (s, ⌧ ) = Q⌧

r(Yt ) Y0 = s

t=0

!

!

for ⌧ 2 [0, 1] and s 2 S. Given the condition (2.12), we have
gx (s, ⌧ )
From Lemma 2, we know that Q⌧

⇣P

T 1
t=0 r(Xt )

gy (s, ⌧ ).
⌘

and Q⌧

⇣P

T 1
t=0 r(Yt )

⌘

are specified by the same

optimization problem (2.10) but with di↵erent replacement of g(i, qi ) by gx (s, ⌧ ) and gy (s, ⌧ ). Consider the monotonicity of the problem OPT (2.10) in its input, we conclude
Q⌧

T
X1
t=0

r(Xt )

!

Q⌧

T
X1
t=0

!

r(Yt ) .

Proof of Theorem 6
To show the convergence of the value iteration algorithm for QMDP, we need to prove the convergence to the optimal as in the traditional MDP case [21]. The idea is to interpret the OP T
optimization procedure as a contraction mapping and to verify that the optimal value function is

APPENDIX A. SUPPLEMENT FOR CHAPTER 2

83

the fixed point of the mapping. For a value function v : S ⇥ [0, 1] ! R, let L be the operator
Lv = max OP T (s, ⌧, a, v(·, ·)).
a

Here Lv is a function of s and ⌧ , the inputs for OP T on the right-hand side. Then we have
v (k+1) = Lv (k) .
First, we show that the optimal value function v(s, ⌧ ) is the fixed point of the operator L, namely
v = Lv. By regarding

1
X

t

rt (St , at , wt )

t=1

as

T
X1

t

rt (St , at , wt )

t=k+1

in the proof of Theorem 2, we can utilize the same argument to prove that v(·, ·) is the fixed point.

Then what is left is to show that the operator L is a contraction mapping. We consider the 1-norm

as the norm in the space of functions, kf k1 = supx |f (x)|. We need to show, for any two value
functions v1 (·, ·) and v2 (·, ·), that

kLv1

Lv2 k1  kv1

v2 k1 ,

where the contraction rate is the discount factor . Indeed, it is sufficient to show that
|OP T (s, ⌧, a, v1 (·, ·))

OP T (s, ⌧, a, v2 (·, ·))|  kv1

for any (s, ⌧ ). Without loss of generality, we assume OP T (s, ⌧, a, v1 (·, ·))
⇤

Let q be the optimizer of OP T (s, ⌧, a, v1 (·, ·)), i.e.,
q⇤ = arg max
q

min

i2{qi 6=1|i=1,2,...,n}

v 2 k1 ,

(A.5)

OP T (s, ⌧, a, v2 (·, ·)).

[ v1 (si , qi ) + r (s, a, h(s, a, si ))] ,

Here q⇤ = (q1⇤ , ..., qn⇤ ). The second part of the reward is r(·) instead of rt (·) because in the infinitehorizon case the reward is stationary. Also, the reward part is the same for the two OP T s in (A.5).
Thus,
OP T (s, ⌧, a, v1 (·, ·)) =

min

i2{qi⇤ 6=1|i=1,2,...,n}

[ v1 (si , qi⇤ ) + r (s, a, h(s, a, si ))] ,

APPENDIX A. SUPPLEMENT FOR CHAPTER 2

84

and by definition,
OP T (s, ⌧, a, v2 (·, ·))

min

i2{qi⇤ 6=1|i=1,2,...,n}

[ v2 (si , qi⇤ ) + r (s, a, h(s, a, si ))] .

Combining these two results, we have
OP T (s, ⌧, a, v1 (·, ·))

OP T (s, ⌧, a, v2 (·, ·)) 

min

i2{qi⇤ 6=1|i=1,2,...,n}

min

[ v1 (si , qi⇤ ) + r (s, a, h(s, a, si ))]

i2{qi⇤ 6=1|i=1,2,...,n}

=

min

i2{qi⇤ 6=1|i=1,2,...,n}

 kv1

[ v2 (si , qi⇤ ) + r (s, a, h(s, a, si ))]

[ v1 (si , qi⇤ )]

min

i2{qi⇤ 6=1|i=1,2,...,n}

[ v2 (si , qi⇤ )]

v2 k1 .

Thus, for any values of (s, ⌧ ), we have
kLv1

Lv2 k1  kv1

v2 k1 .

Proof of Theorem 7
The proof of the optimal policy is exactly the same as in the finite-horizon case. The derivation of
the value function implies the existence of the policy that achieves the optimal value. By assuming
this existence for time t = 1, the optimal policy at t = 0 will be as stated in the theorem.

A.3

Synthetic Example: Computation Time for QMDP vs.
QBDP

Figure A.1 shows the CPU times for solving QMDP and QBDP for the synthetic example (QBDP
with one specific risk measure value ⌧ ). For a single value of ⌧ , QMDP is more time consuming
than QBDP. Figure A.2 compares the computation time of QMDP and QBDP solved for 100
di↵erent values of ⌧ with each policy simulated 10, 000 times to obtain the cumulative reward
function. We note that 10, 000 simulation trials might still be insufficient to obtain a desirable
confidence interval. The procedure is necessary when trading o↵ the risk and reward for QBDP
and other risk-sensitive MDP models. When the problem is less complex (shorter time horizon,
fewer states, smaller maximum reward), QBDP’s CPU time largely exceeds that of QMDP because
of the computational complexity from simulation. When the model time horizon becomes longer,
the computation time of QMDP exceeds that of QBDP. However, our experiment underestimates
QBDP’s computation time since we only obtained optimal policies for 100 di↵erent ⌧ values. As

APPENDIX A. SUPPLEMENT FOR CHAPTER 2

85

the model time horizon becomes larger, the cumulative reward function becomes very complex and
evaluation of a small number of ⌧ values will be insufficient.

Figure A.1: Synthetic example: CPU time of QMDP and QBDP (one run with ⌧ = 0.6).

Figure A.2: Synthetic example: CPU time of QMDP and QBDP (100 runs with di↵erent ⌧ values
and 10, 000 simulations of each policy).

A.4

HIV Treatment Example: Model Parameters

Table A.1: Model parameters for HIV treatment example
Model Parameters and Sources
Variable
Base Value
General population death rate
Varied by age
Mortality multiplier for cardiovascular disease
2.0
CD4 decrease every 6 months without ART
35.25
100 (dt  6)
50 (6 < dt  12)
40 (12 < dt  18)
CD4 increase every 6 months on ART by
40 (18 < dt  24)
treatment duration dt
25 (24 < dt  30)
20 (30 < dt  36)
20 (36 < dt  42)
0 (dt > 42)
0.1618 (ck  50)
0.0692 (50 < ck  100)
0.0549 (100 < ck  200)
6 month HIV death probability without ART by
0.0428 (200 < ck  300)
CD4 levels ck
0.0348 (300 < ck  400)
0.0295 (400 < ck  500)
0.0186 (ck > 500)
0.1356 (ck  50)
0.0472 (50 < ck  100)
0.0201 (100 < ck  200)
6 month HIV death probability with ART by
0.0103 (200 < ck  300)
CD4 levels ck
0.0076 (300 < ck  400)
0.0076 (400 < ck  500)
0.0045 (ck > 500)
0.82 (ck  50)
0.83 (50 < ck  100)
0.84 (100 < ck  200)
Utility for HIV-infected patients not on ART by
0.85 (200 < ck  300)
CD4 levels ck
0.86 (300 < ck  400)
0.87 (400 < ck  500)
0.88 (ck > 500)
0.72 (ck  50)
0.75 (50 < ck  100)
0.78 (100 < ck  200)
Utility for HIV-infected patients on ART by
0.81 (200 < ck  300)
CD4 levels ck
0.84 (300 < ck  400)
0.87 (400 < ck  500)
0.90 (ck > 500)

Source
[139]
[139]
[91]
[48]

[48]

[48]

[96]

[96]

APPENDIX A. SUPPLEMENT FOR CHAPTER 2

87

Figure A.3: HIV treatment example: Optimal actions for QBDP (for ⌧ = 0.20, 0.35, and 0.50) and
MDP.

A.5

HIV Treatment Example: Solution Using QBDP

Figure A.3 shows the optimal actions for the HIV treatment example obtained using MDP, and
QBDP with three values of ⌧ . The color of each point indicates the optimal action, to initiate ART
or delay, for di↵erent CD4 levels and ages. The optimal MDP action roughly matches the optimal
QMDP action with ⌧ = 0.5 (see Figure 2.10). This follows the intuition that the median roughly
equals the mean of a symmetric distribution. However, for QBDP, the optimal action roughly
matches the optimal MDP action when ⌧ = 0.35. The risk parameters in risk-sensitive MDP
models such as QBDP are less interpretable than the quantile parameter in QMDP: although the
parameter ⌧ conveys a meaning of the quantile of instant reward, the QBDP optimal value function
has nothing to do with the ⌧ -quantile or other distributional information of the true cumulative
reward under the QBDP optimal policy.
In Section 2.3.4, we noted that the optimal policy of QBDP or other nested-risk-measure models
does not consider past history and thus cannot consider non-Markovian policies. Our experimental
results reveal another potential shortcoming of the QBDP model. When computing the optimal
value function and optimal policy, the QBDP model summarizes the cost-to-go cumulative reward
into a deterministic value using backward dynamic programming. An improper choice of the risk
parameter in QBDP may cause a loss of information for the future cost-to-go reward and thus
cannot distinguish the outcomes under di↵erent actions. For example, di↵erent actions may result
in the same median but di↵erent 20% quantile values. In this case, if we choose ⌧ = 0.5 for QBDP,
the optimal policy is to always initiate treatment when the patient’s CD4 level is less than 500,
regardless of age. QBDP does not take into account the information from lower quantiles and cannot
distinguish di↵erent actions, leading to overly myopic decisions. In contrast, QMDP summarizes
the future cost-to-go value as a function of the quantile, and memorizes all the quantiles of the
cost-to-go value, and thus is more informative.

Bibliography
[1] A.B. Abdessalem, N. Dervilis, D.J. Wagg, and K. Worden. Automatic kernel selection for
Gaussian processes regression with approximate Bayesian computation and sequential Monte
Carlo. Front Built Environ, 3:52, 2017.
[2] Central Intelligence Agency. The World Factbook, 2018 (accessed February 3, 2019).
[3] D. Akena, J. Joska, E.A. Obuku, and D.J. Stein. Sensitivity and specificity of clinician
administered screening instruments in detecting depression among HIV-positive individuals
in Uganda. AIDS Care, 25(10):1245–1252, 2013.
[4] D. Akena, S. Musisi, J. Joska, and D.J. Stein.

The association between AIDS related

stigma and major depressive disorder among HIV-positive individuals in Uganda. PLoS One,
7(11):e48671, 2012.
[5] D.H. Akena, S. Musisi, and E. Kinyanda. A comparison of the clinical features of depression
in HIV-positive and HIV-negative patients in Uganda. Afr J Psychiatry, 13(1):43–51, 2010.
[6] S.S. Alistar, D.K. Owens, and M.L. Brandeau. E↵ectiveness and cost e↵ectiveness of oral
pre-exposure prophylaxis in a portfolio of prevention programs for injection drug users in
mixed HIV epidemics. PLoS One, 9(1):e86584, 2014.
[7] E. Altman. Constrained Markov Decision Processes. CRC Press, New York, 1999.
[8] A. Arlotto, N. Gans, and J.M. Steele. Markov decision problems where means bound variances. Oper Res, 62(4):864–875, 2014.
[9] S.A. Assoumou, A. Tasillo, C. Vellozzi, G. Eftekhari Yazdi, J. Wang, S. Nolen, L. Hagan,
W. Thompson, L.M. Randall, L. Strick, et al. Cost-e↵ectiveness and budgetary impact of
hepatitis C virus testing, treatment, and linkage to care in US prisons. Clin Infect Dis,
70(7):1388–1396, 2020.

88

BIBLIOGRAPHY

89

[10] P.C. Austin, J.V. Tu, P.A. Daly, and D.A. Alter. The use of quantile regression in health care
research: a case study examining gender di↵erences in the timeliness of thrombolytic therapy.
Stat Med, 24(5):791–816, 2005.
[11] M. Badri, S.D. Lawn, and R. Wood. Short-term risk of AIDS or death in people infected
with HIV-1 before antiretroviral therapy in South Africa: a longitudinal study. Lancet,
368(9543):1254–1259, 2006.
[12] D.R. Bangsberg, F.M. Hecht, E.D. Charlebois, A.R. Zolopa, M. Holodniy, L. Sheiner, J.D.
Bamberger, M.A. Chesney, and A. Moss. Adherence to protease inhibitors, HIV-1 viral load,
and development of drug resistance in an indigent population. AIDS, 14(4):357–366, 2000.
[13] The World Bank. World development indicators, 2016 (accessed February 9, 2019).
[14] The World Bank. GDP per capita (current US dollars) - sub-Saharan Africa, 2019 (accessed
December 9, 2019).
[15] N. Bauerle and J. Ott. Markov decision processes with average-value-at-risk criteria. Math
Methods Oper Res, 74(3):361–379, 2011.
[16] M.G. Bellemare, W. Dabney, and R. Munos. A distributional perspective on reinforcement
learning. In Proceedings of the 34th International Conference on Machine Learning-Volume
70, pages 449–458, 2017.
[17] E. Bendavid, S.D. Young, D.A. Katzenstein, A.M. Bayoumi, G.D. Sanders, and D.K. Owens.
Cost-e↵ectiveness of HIV monitoring strategies in resource-limited settings: a southern
African analysis. Arch Intern Med, 168(17):1910–1918, 2008.
[18] T.M. Berheto, D.B. Haile, and S. Mohammed. Predictors of loss to follow-up in patients
living with HIV/AIDS after initiation of antiretroviral therapy. N Am J Med Sci, 6(9):453–
459, 2014.
[19] J. Berkowitz and J. O’Brien. How accurate are value-at-risk models at commercial banks? J
Finance, 57(3):1093–1111, 2002.
[20] C. Bernard, F. Dabis, and N. de Rekeneire. Prevalence and factors associated with depression
in people living with HIV in sub-Saharan Africa: a systematic review and meta-analysis. PloS
One, 12(8):e0181960–e0181960, 2017.
[21] D.P. Bertsekas. Dynamic Programming and Optimal Control, volume 1. Athena Scientific,
Belmont, MA, 1995.

BIBLIOGRAPHY

90

[22] A. Beyerlein. Quantile regression – opportunities and challenges from a user’s perspective.
Am J Epidemiol, 180(3):330–331, 2014.
[23] C.M. Bishop et al. Neural Networks for Pattern Recognition. Oxford University Press, 1995.
[24] R.W. Blanning. The sources and uses of sensitivity information. INFORMS J Appl Anal,
4(4):32–38, 1974.
[25] H. Borchani, G. Varando, C. Bielza, and P. Larrañaga. A survey on multi-output regression.
Wires Data Min Knowl, 5(5):216–233, 2015.
[26] E. Breuer, L. Myer, H. Struthers, and J.A. Joska. HIV/AIDS and mental health research in
sub-Saharan Africa: a systematic review. Afr J AIDS Res, 10(2):101–122, 2011.
[27] J. Byakika-Tusiime, J. Crane, J.H. Oyugi, K. Ragland, A. Kawuma, P. Musoke, and D.R.
Bangsberg. Longitudinal antiretroviral adherence in HIV+ Ugandan parents and their children initiating HAART in the MTCT-Plus family treatment model: role of depression in
declining adherence over time. AIDS Behav, 13 Suppl 1:82–91, 2009.
[28] S. Carpin, Y.L. Chow, and M. Pavone. Risk aversion in finite Markov decision processes using
total cost criteria and average value at risk. In Proceedings of the 2016 IEEE International
Conference on Robotics and Automation, pages 335–342, 2016.
[29] R. Caruana and A. Niculescu-Mizil. An empirical comparison of supervised learning algorithms. In Proceedings of the 23rd International Conference on Machine Learning, pages
161–168, 2006.
[30] P. Cheridito and M. Stadje. Time-inconsistency of VaR and time-consistent alternatives. Fin
Res Lett, 6(1):40–46, 2009.
[31] D. Chibanda, T. Bowers, R. Verhey, S. Rusakaniko, M. Abas, H.A. Weiss, and R. Araya. The
Friendship Bench programme: a cluster randomised controlled trial of a brief psychological
intervention for common mental disorders delivered by lay health workers in Zimbabwe. Int
J Ment Health Syst, 9:21, 2015.
[32] D. Chisholm, K. Sanderson, J.L. Ayuso-Mateos, and S. Saxena. Reducing the global burden
of depression: population-level analysis of intervention cost-e↵ectiveness in 14 world regions.
Br J Psychiatry, 184:393–403, 2004.
[33] R. Cholera, B.W. Pence, B.N. Gaynes, J. Bassett, N. Qangule, A. Pettifor, C. Macphail, and
W.C. Miller. Depression and engagement in care among newly diagnosed HIV-infected adults
in Johannesburg, South Africa. AIDS Behav, 21(6):1632–1640, 2017.

BIBLIOGRAPHY

91

[34] Y. Chow. Risk-sensitive and data-driven sequential decision making. PhD thesis, Institute of
Computational and Mathematical Engineering, Stanford University. 2017.
[35] Y. Chow and M. Ghavamzadeh. Algorithms for CVaR optimization in MDPs. In Adv Neural
Inf Process Syst, pages 3509–3517, 2014.
[36] Y. Chow, A. Tamar, S. Mannor, and M. Pavone. Risk-sensitive and robust decision-making: a
CVaR optimization approach. In Advances in Neural Information Processing Systems, pages
1522–1530, 2015.
[37] A. Ciaranello, A.H. Sohn, I.J. Collins, C. Rothery, E. Abrams, B. Woods, P. Pei, M. Penazzato, and M. Mahy. Simulation modeling and metamodeling to inform national and international HIV policies for children and adolescents. J Acquir Immune Defic Syndr, 78 Suppl
1:S49–S57, 2018.
[38] J.A. Ciesla and J.E. Roberts. Meta-analysis of the relationship between HIV infection and
risk for depressive disorders. Am J Psychiatry, 158(5):725–730, 2001.
[39] A. Cipriani, X. Zhou, C. Del Giovane, S. E Hetrick, B. Qin, C. Whittington, et al. Comparative efficacy and tolerability of antidepressants for major depressive disorder in children and
adolescents: a network meta-analysis. Lancet, 388(10047):881–890, 2016.
[40] M. Dabney, W.and Rowland, M.G. Bellemare, and R. Munos. Distributional reinforcement
learning with quantile regression. In Thirty-Second AAAI Conference on Artificial Intelligence, pages 2892–2901, 2018.
[41] G. DeCandia, D. Hastorun, M. Jampani, G. Kakulapati, A. Lakshman, A. Pilchin, S. Sivasubramanian, P. Vosshall, and W. Vogels. Dynamo: Amazon’s highly available key-value
store. SIGOPS Oper Syst Rev, 41(6):205–220, 2007.
[42] E. Delage and S. Mannor. Percentile optimization for Markov decision processes with parameter uncertainty. Oper Res, 58(1):203–213, 2010.
[43] The Demographic and Health Surveys Program. Uganda demographic and health survey, 2011
(accessed February 9, 2019).
[44] The Demographic and Health Surveys Program. Uganda: HIV/AIDS sero-behavioural survey,
2005 (accessed February 3, 2019).
[45] D. Di Castro, A. Tamar, and S. Mannor. Policy gradients with variance related risk criteria.
arXiv preprint arXiv:1206.6404, 2012.

BIBLIOGRAPHY

92

[46] United Nations Population Division. World population prospects 2015, 2015 (accessed February 9, 2019).
[47] D. Duffie and J. Pan. An overview of value at risk. J. Deriv., 4(3):7–49, 1997.
[48] M. Egger, M. May, G. Chêne, A.N. Phillips, B. Ledergerber, F. Dabis, et al. Prognosis of
HIV-1-infected patients starting highly active antiretroviral therapy: a collaborative analysis
of prospective studies. Lancet, 360(9327):119–129, 2002.
[49] S. Ermon, C. Gomes, B. Selman, and A. Vladimirsky. Probabilistic planning with nonlinear utility functions and worst-case guarantees. In Proceedings of the 11th International
Conference on Autonomous Agents and Multiagent Systems-Volume 2, pages 965–972, 2012.
[50] Katabira E.T., Kamya M.R., Kalyesubula I., and Namale A. National antiretroviral treatment
and care guidelines for adults, adolescents, and children. In. Kampala, Uganda: Uganda
Ministry of Health, 2009.
[51] N.M. Ferguson, C.A. Donnelly, J. Hooper, A.C. Ghani, C. Fraser, L.M. Bartley, R.A. Rode,
P. Vernazza, D. Lapins, S.L. Mayer, and R.M. Anderson. Adherence to antiretroviral therapy
and its impact on clinical outcome in HIV-infected patients. J R Soc Interface, 2(4):349–363,
2005.
[52] A. Fernandez, J.A. Saameo, A. Pinto-Meza, J.V. Luciano, J. Autonell, D. Palao, L. SalvadorCarulla, J.G. Campayo, J.M. Haro, and A. Serrano. Burden of chronic physical conditions
and mental disorders in primary care. Br J Psychiatry, 196(4):302–309, 2010.
[53] J.A. Filar, D. Krass, and K.W. Ross. Percentile performance criteria for limiting average
Markov decision processes. IEEE Trans Autom Control, 40(1):2–10, 1995.
[54] President’s Emergency Plan for AIDS Relief. DREAMS and VMMC central data, 2015 (accessed February 14, 2016).
[55] Institute for Health Metrics and Evaluation. Assessing facility capacity, costs of care, and
patient perspectives, 2014 (accessed February 14, 2016).
[56] L. Fraenkel, S.T. Bogardus Jr., and D.R. Wittink. Risk-attitude and patient treatment
preferences. Lupus, 12(5):370–376, 2003.
[57] M.S. Freiberg and K. So-Armah. HIV and cardiovascular disease: we need a mechanism, and
we need a plan. J. Am. Heart Assoc., 5(3):e003411, 2016.

BIBLIOGRAPHY

93

[58] A. Gelman. A Bayesian formulation of exploratory data analysis and goodness-of-fit testing.
Int Stat Rev, 71(2):369–382, 2003.
[59] H. Gilbert, P. Weng, and Y. Xu. Optimizing quantiles in preference-based Markov decision
processes. ArXiv e-prints arXiv:1612.00094, 2016.
[60] J.S. Gonzalez, A.W. Batchelder, C. Psaros, and S.A. Safren. Depression and HIV/AIDS treatment nonadherence: a review and meta-analysis. J Acquir Immune Defic Syndr, 58(2):181–
187, 2011.
[61] A. Gupta, V. Dabla, B. Joshi, S. Chakraborty, J. Baishya, and A. Gupta. Challenges in
retention of patients in continuum of HIV care in Delhi— experience of a decade & way
ahead. World J AIDS, 04, 2014.
[62] World Health Organization. International drug price indicator guide, 2016 (accessed February
9, 2019).
[63] R. Hecht-Nielsen. Theory of the backpropagation neural network. In Neural Networks for
Perception, pages 65–93. Elsevier, 1992.
[64] S.D. Hollon, M.O. Stewart, and D. Strunk. Enduring e↵ects for cognitive behavior therapy
in the treatment of depression and anxiety. Annu Rev Psychol, 57:285–315, 2006.
[65] C.B. Holmes, R. Wood, M. Badri, S. Zilber, B. Wang, G. Maartens, H. Zheng, Z. Lu, K.A.
Freedberg, and E. Losina. CD4 decline and incidence of opportunistic infections in Cape
Town, South Africa: implications for prophylaxis and treatment. J Acquir Immune Defic
Syndr, 42(4):464–469, 2006.
[66] A.R. Honagodu, M. Krishna, R. Sundarachar, and P. Lepping. Group psychotherapies for
depression in persons with HIV: a systematic review. Indian J Psychiatry, 55(4):323–330,
2013.
[67] M.A. Horberg, M.J. Silverberg, L.B. Hurley, W.J. Towner, D.B. Klein, S. Berso↵-Matcha,
W.G. Weinberg, D. Antoniskis, M. Mogyoros, W.T. Dodge, R. Dobrinich, C P. Quesenberry,
and D.A. Kovach. E↵ects of depression and selective serotonin reuptake inhibitor use on
adherence to highly active antiretroviral therapy and on clinical outcomes in HIV-infected
patients. J Acquir Immune Defic Syndr, 47(3):384–390, 2008.
[68] M.C. Hosseinipour, G.P. Bisson, S. Miyahara, X. Sun, A. Moses, C. Riviere, et al. Empirical
tuberculosis therapy versus isoniazid in adult outpatients with advanced HIV initiating antiretroviral therapy (REMEMBER): a multicountry open-label randomised controlled trial.
Lancet, 387(10024):1198–1209, 2016.

BIBLIOGRAPHY

94

[69] R.A. Howard and J.E. Matheson. Risk-sensitive Markov decision processes. Manag Sci,
18(7):356–369, 1972.
[70] D.A. Iancu, M. Petrik, and D. Subramanian. Tight approximations of dynamic risk measures.
Math Oper Res, 40(3):655–682, 2015.
[71] Clinton Health Access Initiative. HIV/AIDS + TB, 2016 (accessed February 14, 2016).
[72] V. Jain, W. Chang, D.M. Byonanebye, A. Owaraganise, E. Twinomuhwezi, G. Amanyire,
D. Black, E. Marseille, M.R. Kamya, D.V. Havlir, and J.G. Kahn. Estimated costs for delivery
of HIV antiretroviral therapy to individuals with CD4+ T-Cell Counts > 350 cells/uL in rural
Uganda. PLoS One, 10(12):e0143433, 2015.
[73] H. Jalal, B. Dowd, F. Sainfort, and K.M. Kuntz. Linear regression metamodeling as a tool
to summarize and present simulation model results. Med Decis Making, 33(7):880–890, 2013.
[74] D.R. Jiang and W.B. Powell. Risk-averse approximate dynamic programming with quantilebased risk measures. Math Oper Res, 43(2):554–579, 2018.
[75] R. Jin, W. Chen, and T.W. Simpson. Comparative studies of metamodelling techniques under
multiple modelling criteria. Struct Multidisc Optim, 23(1):1–13, 2001.
[76] J.L. Juusola, M.L. Brandeau, D.K. Owens, and E. Bendavid. The cost-e↵ectiveness of preexposure prophylaxis for HIV prevention in the United States in men who have sex with men.
Ann Intern Med, 156(8):541–550, 2012.
[77] F.M. Kaharuza, R. Bunnell, S. Moss, D.W. Purcell, W. Bikaako-Kajura, N. Wamai, R. Downing, P. Solberg, A. Coutinho, and J. Mermin. Depression and CD4 cell count among persons
with HIV infection in Uganda. AIDS Behav, 10(4 Suppl):S105–111, 2006.
[78] J.G. Kahn, E. Marseille, D. Moore, R. Bunnell, W. Were, R. Degerman, J.W. Tappero,
P. Ekwaru, F. Kaharuza, and J. Mermin. CD4 cell count and viral load monitoring in patients
undergoing antiretroviral therapy in Uganda: cost-e↵ectiveness study. BMJ, 343:d6884, 2011.
[79] P. Kemper, D. Muller, and A. Thummler. Combining response surface methodology with
numerical models for optimization of class-based queueing systems. In 2005 International
Conference on Dependable Systems and Networks (DSN’05), pages 550–559, 2005.
[80] F. Kigozi, J. Ssebunnya, D. Kizza, S. Cooper, and S. Ndyanabangi. An overview of Uganda’s
mental health care system: results from an assessment using the world health organization’s
assessment instrument for mental health systems (WHO-AIMS). Int J Ment Health Syst,
4(1):1, 2010.

BIBLIOGRAPHY

95

[81] E. Kinyanda, S. Hoskins, J. Nakku, S. Nawaz, and V. Patel. Prevalence and risk factors
of major depressive disorder in HIV/AIDS as seen in semi-urban Entebbe district, Uganda.
BMC Psychiatry, 11:205, 2011.
[82] K. Kroenke, R.L. Spitzer, and J.B. Williams. The PHQ-9: validity of a brief depression
severity measure. J Gen Intern Med, 16(9):606–613, 2001.
[83] H. Lakkaraju, E. Kamar, R. Caruana, and J. Leskovec. Interpretable & explorable approximations of black box models. arXiv preprint arXiv:1707.01154, 2017.
[84] R. Laxminarayan, A.J. Mills, J.G. Breman, A.R. Measham, G. Alleyne, M. Claeson, P. Jha,
P. Musgrove, J. Chow, S. Shahid-Salles, and D.T. Jamison. Advancement of global health:
key messages from the Disease Control Priorities Project. Lancet, 367(9517):1193–1208, 2006.
[85] E.F. Long, M.L. Brandeau, and D.K. Owens. The cost-e↵ectiveness and population outcomes
of expanded HIV screening and antiretroviral treatment in the United States. Ann Intern
Med, 153(12):778–789, 2010.
[86] S. Mannor and J. Tsitsiklis. Mean-variance optimization in Markov decision processes. arXiv
preprint arXiv:1104.5601, 2011.
[87] J.E. Mason, B.T. Denton, N.D. Shah, and S.A. Smith. Optimizing the simultaneous management of blood pressure and cholesterol for type 2 diabetes patients. Eur J Oper Res,
233(3):727–738, 2014.
[88] N. McCreesh, I. Andrianakis, R.N. Nsubuga, M. Strong, I. Vernon, T.J. McKinley, J.E.
Oakley, M. Goldstein, R. Hayes, and R.G. White. Universal test, treat, and keep: improving
ART retention is key in cost-e↵ective HIV control in Uganda. BMC Infect Di, 17(1):322,
2017.
[89] A. Medina Lara, J. Kigozi, J. Amurwon, L. Muchabaiwa, B. Nyanzi Wakaholi, R.E. Mujica Mota, et al. Cost-e↵ectiveness analysis of clinically driven versus routine laboratory
monitoring of antiretroviral therapy in Uganda and Zimbabwe. PLoS One, 7(4):e33672, 2012.
[90] G. Melki, A. Cano, V. Kecman, and S. Ventura. Multi-target support vector regression via
correlation regressor chains. Info Sci, 415:53–69, 2017.
[91] J.W. Mellors, A. Munoz, J.V. Giorgi, B. Margolick, C.J. Tassoni, P. Gupta, L.A. Kingsley,
J.A. Todd, A.J. Saah, R. Detels, J.P. Phair, and C.R. Rinaldo Jr. Plasma viral load and
CD4+ lymphocytes as prognostic markers of HIV-1 infection. Ann Intern Med, 126(12):946–
954, 1997.

BIBLIOGRAPHY

96

[92] E.J. Mills, C. Bakanda, J. Birungi, K. Chan, N. Ford, C.L. Cooper, J.B. Nachega, M. Dybul,
and R.S. Hogg. Life expectancy of persons receiving combination antiretroviral therapy in
low-income countries: a cohort analysis from Uganda. Ann Intern Med, 155(4):209–216, 2011.
[93] E.J. Mills, J.B. Nachega, I. Buchan, J. Orbinski, A. Attaran, S. Singh, B. Rachlis, P. Wu,
C. Cooper, L. Thabane, K. Wilson, G.H. Guyatt, and D.R. Bangsberg. Adherence to antiretroviral therapy in sub-Saharan Africa and North America: a meta-analysis. JAMA,
296(6):679–690, 2006.
[94] E.M. Mulogo, V. Batwala, F. Nuwaha, A.S. Aden, and O.S. Baine. Cost e↵ectiveness of
facility and home based HIV voluntary counseling and testing strategies in rural Uganda. Afr
Health Sci, 13(2):423–429, 2013.
[95] E. Nakimuli-Mpungu, K. Wamala, J. Okello, S. Alderman, R. Odokonyero, R. Mojtabai, E.J.
Mills, S. Kanters, J.B. Nachega, and S. Musisi. Group support psychotherapy for depression treatment in people with HIV/AIDS in northern Uganda: a single-centre randomised
controlled trial. Lancet HIV, 2(5):e190–199, 2015.
[96] D.M. Negoescu, D.K. Owens, M.L. Brandeau, and E. Bendavid. Balancing immunological
benefits and cardiovascular risks of antiretroviral therapy: when is immediate treatment optimal? Clin Infect Dis, 55(10):1392–1399, 2012.
[97] A. Nemirovski and A. Shapiro. Convex approximations of chance constrained programs.
SIAM J Opt, 17(4):969–996, 2007.
[98] P.J. Neumann, D.D. Kim, T.A. Trikalinos, M.J. Sculpher, J.A. Salomon, L.A. Prosser, D.K.
Owens, D.O. Meltzer, K.M. Kuntz, M. Krahn, D. Feeny, A. Basu, L.B. Russell, J.E. Siegel,
T.G. Ganiats, and G.D. Sanders. Future directions for cost-e↵ectiveness analyses in health
and medicine. Med Decis Making, 38(7):767–777, 2018.
[99] V.K. Ngo, G. J. Wagner, N. Nakasujja, A. Dickens, F. Aunon, and S. Musisi. E↵ectiveness of
antidepressants and predictors of treatment response for depressed HIV patients in Uganda.
Int J STD AIDS, 26(14):998–1006, 2015.
[100] A. Nilim and L. El Ghaoui. Robust control of Markov decision processes with uncertain
transition matrices. Oper Res, 53(5):780–798, 2005.
[101] C. Obirikorang, P.K. Selleh, J.K. Abledu, and C.O. Fofie. Predictors of adherence to antiretroviral therapy among HIV/AIDS patients in the upper west region of Ghana. ISRN
AIDS, 2013:873939, 2013.

BIBLIOGRAPHY

97

[102] Joint United Nations Programme on HIV/AIDS. AIDSinfo, 2015 (accessed February 9, 2019).
[103] Joint United Nations Programme on HIV/AIDS. Ending AIDS. Progress toward the 90-90-90
targets, 2017 (accessed February 9, 2019).
[104] T. Orbenstergard, R.L. Jensen, and S.E. Maagaard. A comparison of six metamodeling
techniques applied to building performance simulations. Appl Energy, 211:89–103, 2018.
[105] World Health Organization. Health service delivery costs, 2008 (accessed February 14, 2016).
[106] World Health Organization. Threshold values for intervention cost-e↵ectiveness by region,
2009 (accessed February 3, 2019).
[107] World Health Organization. Guideline on when to start antiretroviral therapy and on preexposure prophylaxis for HIV, 2015 (accessed November 2, 2018).
[108] World Health Organization. Male circumcision for HIV prevention, 2017 (accessed November
2, 2018).
[109] World Health Organization. Health policy, 2019 (accessed June 22, 2019).
[110] C. Orrell, G. Harling, S.D. Lawn, R. Kaplan, M. McNally, L.G. Bekker, and R. Wood. Conservation of first-line antiretroviral treatment regimen where therapeutic options are limited.
Antivir Ther (Lond), 12(1):83–88, 2007.
[111] E. Ovuga, J. Boardman, and D. Wasserman. The prevalence of depression in two districts of
Uganda. Soc Psychiatry Psychiatr Epidemiol, 40(6):439–445, 2005.
[112] V. Patel, D. Chisholm, S. Rabe-Hesketh, F. Dias-Saxena, G. Andrew, and A. Mann. Efficacy
and cost-e↵ectiveness of drug and psychological treatments for common mental disorders in
general health care in Goa, India: a randomised, controlled trial. Lancet, 361(9351):33–39,
2003.
[113] G.C. Pflug and A. Pichler. Time-consistent decisions and temporal decomposition of coherent
risk functionals. Math Oper Res, 41(2):682–699, 2016.
[114] A.B. Piunovskiy. Dynamic programming in constrained Markov decision processes. Control
Cybern, 35(3):645–660, 2006.
[115] T.C. Quinn, M.J. Wawer, N. Sewankambo, D. Serwadda, C. Li, F. Wabwire-Mangen, M.O.
Meehan, T. Lutalo, and R.H. Gray. Viral load and heterosexual transmission of human
immunodeficiency virus type 1. Rakai Project Study Group. N Engl J Med, 342(13):921–929,
2000.

BIBLIOGRAPHY

98

[116] D.A. Revicki and M. Wood. Patient-assigned health state utilities for depression-related
outcomes: di↵erences by depression severity and antidepressant medications. J A↵ect Disord,
48(1):25–36, 1998.
[117] R.T. Rockafellar and S. Uryasev. Conditional value-at-risk for general loss distributions. J
Bank Finance, 26(7):1443–1471, 2002.
[118] A. Ruszczyński. Risk-averse dynamic programming for Markov decision processes. Math
Programming, 125(2):235–261, 2010.
[119] G.D. Sanders, P.J. Neumann, A. Basu, D.W. Brock, D. Feeny, M. Krahn, et al. Recommendations for conduct, methodological practices, and reporting of cost-e↵ectiveness analyses:
Second Panel on Cost-e↵ectiveness in Health and Medicine. JAMA, 316(10):1093–1103, 2016.
[120] L.I. Sennott. Average cost semi-Markov decision processes and the control of queueing systems. Probab Eng Inform Sci, 3(2):247—-272, 1989.
[121] A. Shapiro, W. Tekaya, J. Paulo da Costa, and M.P. Soares. Risk neutral and risk averse
stochastic dual dynamic programming method. Eur J Oper Res, 224(2):375–391, 2013.
[122] S.M. Shechter, M.D. Bailey, A.J. Schaefer, and M.S. Roberts. The optimal time to initiate
HIV therapy under ordered health states. Oper Res, 56(1):20–33, 2008.
[123] N.L. Sin and M.R. DiMatteo. Depression treatment enhances adherence to antiretroviral
therapy: a meta-analysis. Ann Behav Med, 47(3):259–269, 2014.
[124] R. Spirig. Support groups for people living with HIV/AIDS: a review of literature. J Assoc
Nurses AIDS Care, 9(4):43–55, 1998.
[125] B.T. Stegenga, M.H. Kamphuis, M. King, I. Nazareth, and M.I. Geerlings. The natural
course and outcome of major depressive disorder in primary care: the PREDICT-NL study.
Soc Psychiatry Psychiatr Epidemiol, 47(1):87–95, 2012.
[126] E. Tagar, M. Sundaram, K. Condli↵e, B. Matatiyo, F. Chimbwandira, B. Chilima, et al. Multicountry analysis of treatment costs for HIV/AIDS (MATCH): facility-level ART unit cost
analysis in Ethiopia, Malawi, Rwanda, South Africa and Zambia. PLoS One, 9(11):e108304,
2014.
[127] A. Tamar, D. Di Castro, and S. Mannor. Policy gradients with variance-related risk criteria.
In Proceedings of the 29th International Conference on Machine Learning, pages 1651–1658.
Omnipress, 2012.

BIBLIOGRAPHY

99

[128] F. Tanser, T. Bärnighausen, E. Grapsa, J. Zaidi, and M.L. Newell. High coverage of ART
associated with decline in risk of HIV acquisition in rural KwaZulu-Natal, South Africa.
Science, 339(6122):966–971, 2013.
[129] T.O. Tengs and T.H. Lin. A meta-analysis of utility estimates for HIV/AIDS. Med Decis
Making, 22(6):475–481, 2002.
[130] J.N. Tsitsiklis and B. van Roy. Optimal stopping of Markov processes: Hilbert space theory,
approximation algorithms, and an application to pricing high-dimensional financial derivatives. IEEE Trans Autom Control, 44(10):1840–1851, 1999.
[131] M. Ummels and C. Baier. Computing quantiles in Markov reward models. In International
Conference on Foundations of Software Science and Computational Structures, pages 353–368.
Springer, 2013.
[132] N. Villa-Vialaneix, M. Follador, M. Ratto, and A. Leip. A comparison of eight metamodeling
techniques for the simulation of N2O fluxes and N leaching from corn crops. Environ Modell
Softw, 34:51–66, 2012.
[133] G.J. Wagner, B. Ghosh-Dastidar, E. Robinson, V.K. Ngo, P. Glick, B. Mukasa, S. Musisi,
and D. Akena. E↵ects of depression alleviation on ART adherence and HIV clinic attendance
in Uganda, and the mediating roles of self-efficacy and motivation. AIDS Behav, 21(6):1655–
1664, 2017.
[134] G.J. Wagner, V. Ngo, P. Glick, E.A. Obuku, S. Musisi, and D. Akena. INtegration of DEPression Treatment into HIV Care in Uganda (INDEPTH-Uganda): study protocol for a
randomized controlled trial. Trials, 15:248, 2014.
[135] M. Wawer, R.H. Gray, N.K. Sewankambo, D. Serwadda, X. Li, O. Laeyendecker, N. Kiwanuka,
G. Kigozi, M. Kiddugavu, T. Lutalo, F. Nalugoda, F. Wabwire-Mangen, M.P. Meehan, and
T.C. Quinn. Rates of HIV-1 transmission per coital act, by stage of HIV-1 infection, in Rakai,
Uganda. J Infect Dis, 191(9):1403–1409, 2005.
[136] P.J. Weidle, N. Wamai, P. Solberg, C. Liechty, S. Sendagala, W. Were, J. Mermin, K. Buchacz,
P. Behumbiize, R.L. Ransom, and R. Bunnell. Adherence to antiretroviral therapy in a homebased AIDS care programme in rural Uganda. Lancet, 368(9547):1587–1594, 2006.
[137] H.A. Whiteford, M.G. Harris, G. McKeon, A. Baxter, C. Pennell, J.J. Barendregt, and
J. Wang. Estimating remission from untreated major depression: a systematic review and
meta-analysis. Psychol Med, 43(8):1569–1585, 2013.

BIBLIOGRAPHY

100

[138] W. Wiesemann, D. Kuhn, and B. Rustem. Robust Markov decision processes. Math Oper
Res, 38(1):153–183, 2013.
[139] World Health Organization. WHO Life Tables, 2016.
[140] World Health Organization. Global Health Observatory Data: HIV/AIDS, 2018.
[141] D. Yang, L. Zhao, Z. Lin, T. Qin, J. Bian, and T.Y. Liu. Fully parameterized quantile function for distributional reinforcement learning. In Advances in Neural Information Processing
Systems, pages 6190–6199, 2019.
[142] P. Yu, W.B. Haskell, and H. Xu. Dynamic programming for risk-aware sequential optimization. In 2017 IEEE 56th Annual Conference on Decision and Control (CDC), pages
4934–4939, 2017.
[143] J. Yuan, V. Nian, B. Su, and Q. Meng. A simultaneous calibration and parameter ranking
method for building energy models. Appl Energy, 206:657–666, 2017.
[144] Y. Zhang, L.M. Steimle, and B.T. Denton. Robust Markov decision processes for medical
treatment decisions. Working paper, 2015.
[145] H. Zou and T. Hastie. Regularization and variable selection via the elastic net. J R Stat Soc
Ser B Methodol, 67(2):301–320, 2005.

ProQuest Number: 28241805
INFORMATION TO ALL USERS
The quality and completeness of this reproduction is dependent on the quality
and completeness of the copy made available to ProQuest.

Distributed by ProQuest LLC ( 2021 ).
Copyright of the Dissertation is held by the Author unless otherwise noted.

This work may be used in accordance with the terms of the Creative Commons license
or other rights statement, as indicated in the copyright statement or in the metadata
associated with this work. Unless otherwise specified in the copyright statement
or the metadata, all rights are reserved by the copyright holder.

This work is protected against unauthorized copying under Title 17,
United States Code and other applicable copyright laws.

Microform Edition where available © ProQuest LLC. No reproduction or digitization
of the Microform Edition is authorized without permission of ProQuest LLC.

ProQuest LLC
789 East Eisenhower Parkway
P.O. Box 1346
Ann Arbor, MI 48106 - 1346 USA

